{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcdc DocETL: A System for Complex Document Processing","text":"<p>DocETL is a tool for creating and executing LLM-powered data processing pipelines. It offers a low-code, declarative YAML interface to define complex data operations on complex data.</p> <p>When to Use DocETL</p> <p>DocETL is the ideal choice when you're looking to maximize correctness and output quality for complex tasks over a collection of documents or unstructured datasets. You should consider using DocETL if:</p> <ul> <li>You have complex tasks that you want to represent via map-reduce (e.g., map over your documents, then group by the result of your map call &amp; reduce)</li> <li>You're unsure how to best write your pipeline or sequence of operations to maximize LLM accuracy</li> <li>You're working with long documents that don't fit into a single prompt or are too lengthy for effective LLM reasoning</li> <li>You have validation criteria and want tasks to automatically retry when the validation fails</li> </ul>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Rich Suite of Operators: Tailored for complex data processing, including specialized operators like \"resolve\" for entity resolution and \"gather\" for maintaining context when splitting documents.</li> <li>Low-Code Interface: Define your pipeline and prompts easily using YAML. You have 100% control over the prompts.</li> <li>Flexible Processing: Handle various document types and processing tasks across domains like law, medicine, and social sciences.</li> <li>Accuracy Optimization: Our optimizer leverages LLM agents to experiment with different logically-equivalent rewrites of your pipeline and automatically selects the most accurate version. This includes finding limits of how many documents to process in a single reduce operation before the accuracy plateaus.</li> </ul>"},{"location":"#getting-started","title":"\u26a1 Getting Started","text":"<p>To get started with DocETL:</p> <ol> <li>Install the package (see installation for detailed instructions)</li> <li>Define your pipeline in a YAML file. Want to use an LLM like ChatGPT or Claude to help you write your pipeline? See docetl.org/llms.txt for a big prompt you can copy paste into ChatGPT or Claude, before describing your task.</li> <li>Run your pipeline using the DocETL command-line interface</li> </ol> <p>Fastest Way: Claude Code</p> <p>Clone this repo and run <code>claude</code> to use the built-in DocETL skill. Just describe your data processing task and Claude will create and run the pipeline for you. See Quick Start (Claude Code) for details.</p>"},{"location":"#project-origin","title":"\ud83c\udfdb\ufe0f Project Origin","text":"<p>DocETL was created by members of the EPIC Data Lab and Data Systems and Foundations group at UC Berkeley. The EPIC (Effective Programming, Interaction, and Computation with Data) Lab focuses on developing low-code and no-code interfaces for data work, powered by next-generation predictive programming techniques. DocETL is one of the projects that emerged from our research efforts to streamline complex document processing tasks.</p> <p>For more information about the labs and other projects, visit the EPIC Lab webpage and the Data Systems and Foundations webpage.</p>"},{"location":"best-practices/","title":"Best Practices for DocETL","text":"<p>This guide outlines best practices for using DocETL effectively, focusing on the most important aspects of pipeline creation, execution, and optimization.</p> <p>Supported Models</p> <p>DocETL supports many models through LiteLLM:</p> <ul> <li>OpenAI models (e.g., GPT-4, GPT-3.5-turbo)</li> <li>Anthropic models (e.g., Claude 2, Claude Instant)</li> <li>Google VertexAI models (e.g., chat-bison, text-bison)</li> <li>Cohere models</li> <li>Replicate models</li> <li>Azure OpenAI models</li> <li>Hugging Face models</li> <li>AWS Bedrock models (e.g., Claude, AI21, Cohere)</li> <li>Gemini models (e.g., gemini-1.5-pro)</li> <li>Ollama models (e.g., llama2)</li> </ul> <p>For a complete and up-to-date list of supported models, please refer to the LiteLLM documentation. You can use the model name just like the litellm documentation (e.g., <code>openai/gpt-4o-mini</code> or <code>gemini/gemini-1.5-flash-002</code>).</p> <p>While DocETL supports various models, it has been primarily tested with OpenAI's language models. Using OpenAI is currently recommended for the best experience and most reliable results, especially for operations that depend on structured outputs. We have also tried gemini-1.5-flash-002 and found it to be pretty good for a much cheaper price.</p>"},{"location":"best-practices/#pipeline-design","title":"Pipeline Design","text":"<ol> <li>Start Simple: Begin with a basic pipeline and gradually add complexity as needed.</li> </ol> <p>Example: Start with a simple extraction operation before adding resolution and summarization.</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n</code></pre> <ol> <li>Modular Design: Break down complex tasks into smaller, manageable operations.</li> </ol> <p>Example: The medical transcripts pipeline in the tutorial demonstrates this by separating medication extraction, resolution, and summarization into distinct operations.</p> <ol> <li>Optimize Incrementally: Optimize one operation at a time to ensure stability and verify improvements.</li> </ol> <p>Example: After implementing the basic pipeline, you might optimize the <code>extract_medications</code> operation first:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    optimize: true\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n</code></pre>"},{"location":"best-practices/#schema-and-prompt-design","title":"Schema and Prompt Design","text":"<ol> <li>Configure System Prompts: Set up system prompts to provide context and establish the LLM's role for each operation. This helps generate more accurate and relevant responses.</li> </ol> <p>Example:    <pre><code>system_prompt:\n  dataset_description: a collection of transcripts of doctor visits\n  persona: a medical practitioner analyzing patient symptoms and reactions to medications\n</code></pre></p> <p>The system prompt will be used as a system prompt for all operations in the pipeline.</p> <ol> <li>Keep Schemas Simple: Use simple output schemas whenever possible. Complex nested structures can be difficult for LLMs to produce consistently.</li> </ol> <p>Good Example (Simple Schema):</p> <pre><code>output:\n  schema:\n    medication: list[str] # Note that this is different from the example in the tutorial.\n</code></pre> <p>Avoid (Complex Nested Structure):</p> <pre><code>output:\n  schema:\n    medications: \"list[{name: str, dosage: {amount: float, unit: str, frequency: str}}]\"\n</code></pre> <ol> <li>Clear and Concise Prompts: Write clear, concise prompts for LLM operations, providing relevant context from input data. Instruct quantities (e.g., 2-3 insights, one summary) to guide the LLM.</li> </ol> <p>Example: The <code>summarize_prescriptions</code> operation in the tutorial demonstrates a clear prompt with specific instructions:</p> <pre><code>prompt: |\n  Here are some transcripts of conversations between a doctor and a patient:\n\n  {% for value in inputs %}\n  Transcript {{ loop.index }}:\n  {{ value.src }}\n  {% endfor %}\n\n  For the medication {{ reduce_key }}, please provide the following information based on all the transcripts above:\n\n  1. Side Effects: Summarize all mentioned side effects of {{ reduce_key }}. List 2-3 main side effects.\n  2. Therapeutic Uses: Explain the medical conditions or symptoms for which {{ reduce_key }} was prescribed or recommended. Provide 1-2 primary uses.\n\n  Ensure your summary:\n  - Is based solely on information from the provided transcripts\n  - Focuses only on {{ reduce_key }}, not other medications\n  - Includes relevant details from all transcripts\n  - Is clear and concise\n  - Includes quotes from the transcripts\n</code></pre> <ol> <li>Take advantage of Jinja Templating: Use Jinja templating to dynamically generate prompts and provide context to the LLM. Feel free to use if statements, loops, and other Jinja features to customize prompts.</li> </ol> <p>Example: Using Jinja conditionals and loops in a prompt (note that age is a made-up field for this example):</p> <pre><code>prompt: |\n  Analyze the following medical transcript:\n  {{ input.src }}\n\n  {% if input.patient_age %}\n  Note that the patient is {{ input.patient_age }} years old.\n  {% endif %}\n\n  Please extract the following information:\n  {% for item in [\"medications\", \"symptoms\", \"diagnoses\"] %}\n  - List all {{ item }} mentioned in the transcript\n  {% endfor %}\n</code></pre> <ol> <li>Validate Outputs: Use the <code>validate</code> field to ensure the quality and correctness of processed data. This consists of Python statements that validate the output and optionally retry the LLM if one or more statements fail. To learn more about validation, see the validation documentation.</li> </ol> <p>Example: Adding validation to the <code>extract_medications</code> operation:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n    validate: |\n      len(output.medication) &gt; 0\n      all(isinstance(med, str) for med in output.medication)\n      all(len(med) &gt; 1 for med in output.medication)\n</code></pre>"},{"location":"best-practices/#handling-large-documents-and-entity-resolution","title":"Handling Large Documents and Entity Resolution","text":"<ol> <li> <p>Chunk Large Inputs: For documents exceeding token limits, consider using the optimizer to automatically chunk inputs.</p> </li> <li> <p>Use Resolve Operations: Implement resolve operations before reduce operations when dealing with similar entities. Take care to write the compare prompts well to guide the LLM--often the optimizer-synthesized prompts are too generic.</p> </li> </ol> <p>Example: A more specific <code>resolve_medications</code> operation:</p> <pre><code>- name: resolve_medications\n  type: resolve\n  blocking_keys:\n    - medication\n  blocking_threshold: 0.6162\n  comparison_prompt: |\n    Compare the following two medication entries:\n    Entry 1: {{ input1.medication }}\n    Entry 2: {{ input2.medication }}\n\n    Are these medications the same or closely related? Consider the following:\n    1. Are they different brand names for the same active ingredient?\n    2. Are they in the same drug class with similar effects?\n    3. Are they commonly used as alternatives for the same condition?\n\n    Respond with YES if they are the same or closely related, and NO if they are distinct medications.\n</code></pre>"},{"location":"best-practices/#optimization-and-execution","title":"Optimization and Execution","text":"<ol> <li>Use the Optimizer: Leverage DocETL's optimizer for complex pipelines or when dealing with large documents.</li> </ol> <p>Example: Run the optimizer on your pipeline:</p> <pre><code>docetl build pipeline.yaml\n</code></pre> <ol> <li>Leverage Caching: Take advantage of DocETL's caching mechanism to avoid redundant computations. DocETL caches by default.</li> </ol> <p>To clear the cache:</p> <pre><code>docetl clear-cache\n</code></pre> <ol> <li>Monitor Resource Usage: Keep an eye on API costs and processing time, especially when optimizing. Use <code>gpt-4o-mini</code> for optimization (the default is <code>gpt-4o</code>) to save costs. Learn more about how to do this in the optimizer docs.</li> </ol>"},{"location":"best-practices/#additional-notes","title":"Additional Notes","text":"<ul> <li>Sampling Operations: If you want to run an operation on a random sample of your data, you can set the <code>sample</code> parameter for that operation.</li> </ul> <p>Example:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    sample: 100\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n</code></pre> <ul> <li>Intermediate Output: If you provide an intermediate directory in your configuration, the outputs of each operation will be saved to this directory. This allows you to inspect the results of individual steps in the pipeline and can be useful for debugging or analyzing the pipeline's progress.</li> </ul> <p>Example:</p> <pre><code>pipeline:\n  output:\n    type: file\n    path: medication_summaries.json\n    intermediate_dir: intermediate_results\n</code></pre> <p>By following these comprehensive best practices and examples, you can create more efficient, reliable, and maintainable DocETL pipelines for your data processing tasks. Remember to iterate on your pipeline design, continuously refine your prompts, and leverage DocETL's optimization features to get the best results.</p>"},{"location":"installation/","title":"Installation","text":"<p>DocETL can be easily installed using pip, Python's package installer, or from source. Follow these steps to get DocETL up and running on your system:</p>"},{"location":"installation/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before installing DocETL, ensure you have Python 3.10 or later installed on your system. You can check your Python version by running:</p>"},{"location":"installation/#installation-via-pip","title":"\ud83d\udce6 Installation via pip","text":"<ol> <li>Install DocETL using pip:</li> </ol> <pre><code>pip install docetl\n</code></pre> <p>If you want to use the parsing tools, you need to install the <code>parsing</code> extra:</p> <pre><code>pip install docetl[parsing]\n</code></pre> <p>This command will install DocETL along with its dependencies as specified in the pyproject.toml file. To verify that DocETL has been installed correctly, you can run the following command in your terminal:</p> <pre><code>docetl version\n</code></pre>"},{"location":"installation/#installation-from-source","title":"\ud83d\udd27 Installation from Source","text":"<p>To install DocETL from source, follow these steps:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/ucbepic/docetl.git\ncd docetl\n</code></pre> <ol> <li>Install uv (if not already installed):</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ol> <li>Install the project dependencies and DocETL:</li> </ol> <pre><code>uv sync --all-extras\n</code></pre> <p>If you want to use only the parsing extra:</p> <pre><code>uv sync --extra parsing\n</code></pre> <ol> <li>Set up your OpenAI API key:</li> </ol> <p>Create a .env file in the project root and add your OpenAI API key:</p> <pre><code>OPENAI_API_KEY=your_api_key_here\n</code></pre> <p>Alternatively, you can set the OPENAI_API_KEY environment variable in your shell.</p> <ol> <li>Run the basic test suite to ensure everything is working (this costs less than $0.01 with OpenAI):</li> </ol> <pre><code>make tests-basic\n</code></pre>"},{"location":"installation/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":"<p>If you encounter any issues during installation, please ensure that:</p> <ul> <li>Your Python version is 3.10 or later</li> <li>You have the latest version of pip installed</li> <li>Your system meets all the requirements specified in the pyproject.toml file</li> </ul> <p>For further assistance, please refer to the project's GitHub repository or reach out on the Discord server.</p>"},{"location":"quickstart-claude-code/","title":"Quick Start with Claude Code","text":"<p>The fastest way to build DocETL pipelines is with Claude Code, Anthropic's agentic coding tool. DocETL includes a built-in Claude Code skill that helps you create, run, and debug pipelines interactively.</p>"},{"location":"quickstart-claude-code/#option-1-clone-the-repository-recommended","title":"Option 1: Clone the Repository (Recommended)","text":"<p>This gives you the full development environment with the skill already configured.</p> <ol> <li>Follow the Installation from Source instructions</li> <li>Run <code>claude</code> in the repository directory</li> </ol> <p>The skill is located at <code>.claude/skills/docetl/SKILL.md</code>.</p>"},{"location":"quickstart-claude-code/#option-2-install-via-pip","title":"Option 2: Install via pip","text":"<p>If you already have DocETL installed via pip, you can install the skill separately:</p> <pre><code>pip install docetl\ndocetl install-skill\n</code></pre> <p>This copies the skill to <code>~/.claude/skills/docetl/</code>. Then run <code>claude</code> in any directory.</p> <p>To uninstall: <code>docetl install-skill --uninstall</code></p>"},{"location":"quickstart-claude-code/#usage","title":"Usage","text":"<p>Simply describe what you want to do with your data. The skill activates automatically when you mention \"docetl\" or describe unstructured data processing tasks:</p> <pre><code>&gt; I have a folder of customer support tickets in JSON format.\n&gt; I want to extract the main issue, sentiment, and suggested resolution for each.\n</code></pre> <p>Claude will:</p> <ol> <li>Read your data to understand its structure</li> <li>Write a tailored pipeline with prompts specific to your documents</li> <li>Run the pipeline and show you the results</li> <li>Debug issues if any operations fail</li> </ol>"},{"location":"quickstart-claude-code/#alternative-manual-pipeline-authoring","title":"Alternative: Manual Pipeline Authoring","text":"<p>If you prefer not to use Claude Code, see the Quick Start Tutorial for writing pipelines by hand.</p>"},{"location":"retrievers/","title":"Retrievers","text":""},{"location":"retrievers/#retrievers-lancedb-oss","title":"Retrievers (LanceDB OSS)","text":"<p>Retrievers let you augment LLM operations with retrieved context from a LanceDB index built over a DocETL dataset. You define retrievers once at the top-level, then attach them to any LLM-powered operation using <code>retriever: &lt;name&gt;</code>. At runtime, DocETL performs full-text, vector, or hybrid search and injects the results into your prompt as <code>{{ retrieval_context }}</code>.</p> <p>LanceDB supports built-in full-text search, vector search, and hybrid with RRF reranking. See the official docs: LanceDB Hybrid Search docs.</p>"},{"location":"retrievers/#key-points","title":"Key points","text":"<ul> <li>Always OSS LanceDB (local <code>index_dir</code>).</li> <li>A retriever references a dataset from the pipeline config, or the output of a previous pipeline step.</li> <li>Operations do not override retriever settings. One source of truth = consistency.</li> <li><code>{{ retrieval_context }}</code> is available to your prompt; if not used, DocETL prepends a short \"extra context\" section automatically.</li> </ul>"},{"location":"retrievers/#configuration","title":"Configuration","text":"<p>Add a top-level <code>retrievers</code> section. Each retriever has:</p> <ul> <li><code>dataset</code>: dataset name to index (can be a dataset or output of a previous pipeline step)</li> <li><code>index_dir</code>: LanceDB path</li> <li><code>index_types</code>: which indexes to build: <code>fts</code>, <code>embedding</code>, or <code>hybrid</code> (both)</li> <li><code>fts.index_phrase</code>: Jinja template for indexing each row for full-text search</li> <li><code>fts.query_phrase</code>: Jinja template for building the FTS query at runtime</li> <li><code>embedding.model</code>: embedding model for vector index and queries</li> <li><code>embedding.index_phrase</code>: Jinja template for indexing each row for embeddings</li> <li><code>embedding.query_phrase</code>: Jinja template for building the embedding query</li> <li><code>query.mode</code>: <code>fts</code> | <code>embedding</code> | <code>hybrid</code> (defaults to <code>hybrid</code> when both indexes exist)</li> <li><code>query.top_k</code>: number of results to retrieve</li> </ul>"},{"location":"retrievers/#basic-example","title":"Basic example","text":"<pre><code>datasets:\n  transcripts:\n    type: file\n    path: workloads/medical/raw.json\n\ndefault_model: gpt-4o-mini\n\nretrievers:\n  medical_r:\n    type: lancedb\n    dataset: transcripts\n    index_dir: workloads/medical/lance_index\n    build_index: if_missing  # if_missing | always | never\n    index_types: [\"fts\", \"embedding\"]\n    fts:\n      index_phrase: \"{{ input.src }}\"\n      query_phrase: \"{{ input.src[:1000] }}\"\n    embedding:\n      model: openai/text-embedding-3-small\n      index_phrase: \"{{ input.src }}\"\n      query_phrase: \"{{ input.src[:1000] }}\"\n    query:\n      mode: hybrid\n      top_k: 8\n</code></pre>"},{"location":"retrievers/#multi-step-pipelines-with-retrieval","title":"Multi-step pipelines with retrieval","text":"<p>Most pipelines have a single step, but you can define multiple steps where the output of one step becomes the input (and retriever source) for the next. This is powerful for patterns like:</p> <ol> <li>Extract structured data from documents</li> <li>Build a retrieval index on that extracted data</li> <li>Use retrieval to find related items and process them</li> </ol>"},{"location":"retrievers/#example-extract-facts-then-find-conflicts","title":"Example: Extract facts, then find conflicts","text":"<pre><code>datasets:\n  articles:\n    type: file\n    path: workloads/wiki/articles.json\n\ndefault_model: gpt-4o-mini\n\n# Retriever indexes output of step 1 (extract_facts_step)\nretrievers:\n  facts_index:\n    type: lancedb\n    dataset: extract_facts_step  # References output of a pipeline step!\n    index_dir: workloads/wiki/facts_lance_index\n    build_index: if_missing\n    index_types: [\"fts\", \"embedding\"]\n    fts:\n      index_phrase: \"{{ input.fact }} from {{ input.title }}\"\n      query_phrase: \"{{ input.fact }}\"\n    embedding:\n      model: openai/text-embedding-3-small\n      index_phrase: \"{{ input.fact }}\"\n      query_phrase: \"{{ input.fact }}\"\n    query:\n      mode: hybrid\n      top_k: 5\n\noperations:\n  - name: extract_facts\n    type: map\n    prompt: |\n      Extract factual claims from this article.\n      Article: {{ input.title }}\n      Text: {{ input.text }}\n    output:\n      schema:\n        facts: list[string]\n\n  - name: unnest_facts\n    type: unnest\n    unnest_key: facts\n\n  - name: find_conflicts\n    type: map\n    retriever: facts_index  # Uses the retriever\n    prompt: |\n      Check if this fact conflicts with similar facts from other articles.\n\n      Current fact: {{ input.facts }} (from {{ input.title }})\n\n      Similar facts from other articles:\n      {{ retrieval_context }}\n\n      Return true only if there's a genuine contradiction.\n    output:\n      schema:\n        has_conflict: boolean\n\npipeline:\n  steps:\n    # Step 1: Extract and unnest facts\n    - name: extract_facts_step\n      input: articles\n      operations:\n        - extract_facts\n        - unnest_facts\n\n    # Step 2: Use retrieval to find conflicts\n    - name: find_conflicts_step\n      input: extract_facts_step  # Input is output of step 1\n      operations:\n        - find_conflicts\n\n  output:\n    type: file\n    path: workloads/wiki/conflicts.json\n    intermediate_dir: workloads/wiki/intermediates\n</code></pre> <p>In this example: - Step 1 (<code>extract_facts_step</code>) extracts facts from articles - The retriever (<code>facts_index</code>) indexes the output of step 1 - Step 2 (<code>find_conflicts_step</code>) processes each fact, using retrieval to find similar facts from other articles</p>"},{"location":"retrievers/#configuration-reference","title":"Configuration reference","text":""},{"location":"retrievers/#minimal-example","title":"Minimal example","text":"<p>Here's the simplest possible retriever config (FTS only):</p> <pre><code>retrievers:\n  my_search:                              # name can be anything you want\n    type: lancedb\n    dataset: my_dataset                   # must match a dataset name or pipeline step\n    index_dir: ./my_lance_index\n    index_types: [\"fts\"]\n    fts:\n      index_phrase: \"{{ input.text }}\"    # what to index from each row\n      query_phrase: \"{{ input.query }}\"   # what to search for at runtime\n</code></pre>"},{"location":"retrievers/#full-example-with-all-options","title":"Full example with all options","text":"<pre><code>retrievers:\n  my_search:\n    type: lancedb\n    dataset: my_dataset\n    index_dir: ./my_lance_index\n    build_index: if_missing               # optional, default: if_missing\n    index_types: [\"fts\", \"embedding\"]     # can be [\"fts\"], [\"embedding\"], or both\n    fts:\n      index_phrase: \"{{ input.text }}\"\n      query_phrase: \"{{ input.query }}\"\n    embedding:\n      model: openai/text-embedding-3-small\n      index_phrase: \"{{ input.text }}\"    # optional, falls back to fts.index_phrase\n      query_phrase: \"{{ input.query }}\"\n    query:                                # optional section\n      mode: hybrid                        # optional, auto-selects based on index_types\n      top_k: 10                           # optional, default: 5\n</code></pre>"},{"location":"retrievers/#required-fields","title":"Required fields","text":"Field Description <code>type</code> Must be <code>lancedb</code> <code>dataset</code> Name of a dataset or pipeline step to index <code>index_dir</code> Path where LanceDB stores the index (created if missing) <code>index_types</code> List of index types: <code>[\"fts\"]</code>, <code>[\"embedding\"]</code>, or <code>[\"fts\", \"embedding\"]</code>"},{"location":"retrievers/#optional-fields","title":"Optional fields","text":"Field Default Description <code>build_index</code> <code>if_missing</code> When to build: <code>if_missing</code>, <code>always</code>, or <code>never</code> <code>query.mode</code> auto <code>fts</code>, <code>embedding</code>, or <code>hybrid</code>. Auto-selects based on what indexes exist <code>query.top_k</code> 5 Number of results to return"},{"location":"retrievers/#the-fts-section","title":"The <code>fts</code> section","text":"<p>Required if <code>\"fts\"</code> is in <code>index_types</code>. Configures full-text search.</p> Field Required Description <code>index_phrase</code> yes Jinja template: what text to index from each dataset row <code>query_phrase</code> yes Jinja template: what text to search for at query time <p>Jinja variables available:</p> Template Variables When it runs <code>index_phrase</code> <code>input</code> = the dataset row Once per row when building the index <code>query_phrase</code> <code>input</code> = current item (map/filter/extract) At query time for each item processed <code>query_phrase</code> <code>reduce_key</code>, <code>inputs</code> (reduce operations) At query time for each group <p>Example - Medical knowledge base:</p> <pre><code>datasets:\n  drugs:\n    type: file\n    path: drugs.json  # [{\"name\": \"Aspirin\", \"uses\": \"pain, fever\"}, ...]\n\n  patient_notes:\n    type: file\n    path: notes.json  # [{\"symptoms\": \"headache and fever\"}, ...]\n\nretrievers:\n  drug_lookup:\n    type: lancedb\n    dataset: drugs                        # index the drugs dataset\n    index_dir: ./drug_index\n    index_types: [\"fts\"]\n    fts:\n      index_phrase: \"{{ input.name }}: {{ input.uses }}\"   # index: \"Aspirin: pain, fever\"\n      query_phrase: \"{{ input.symptoms }}\"                  # search with patient symptoms\n\noperations:\n  - name: find_treatment\n    type: map\n    retriever: drug_lookup                # attach the retriever\n    prompt: |\n      Patient symptoms: {{ input.symptoms }}\n\n      Relevant drugs from knowledge base:\n      {{ retrieval_context }}\n\n      Recommend a treatment.\n    output:\n      schema:\n        recommendation: string\n</code></pre> <p>When processing <code>{\"symptoms\": \"headache and fever\"}</code>:</p> <ol> <li><code>query_phrase</code> renders to <code>\"headache and fever\"</code></li> <li>FTS searches the index and finds <code>\"Aspirin: pain, fever\"</code> as a match</li> <li><code>{{ retrieval_context }}</code> in your prompt contains the matched results</li> </ol>"},{"location":"retrievers/#the-embedding-section","title":"The <code>embedding</code> section","text":"<p>Required if <code>\"embedding\"</code> is in <code>index_types</code>. Configures vector/semantic search.</p> Field Required Description <code>model</code> yes Embedding model, e.g. <code>openai/text-embedding-3-small</code> <code>index_phrase</code> no Jinja template for text to embed. Falls back to <code>fts.index_phrase</code> <code>query_phrase</code> yes Jinja template for query text to embed <p>Jinja variables: Same as FTS section.</p> <p>Example - Semantic search:</p> <pre><code>retrievers:\n  semantic_docs:\n    type: lancedb\n    dataset: documentation\n    index_dir: ./docs_index\n    index_types: [\"embedding\"]\n    embedding:\n      model: openai/text-embedding-3-small\n      index_phrase: \"{{ input.content }}\"\n      query_phrase: \"{{ input.question }}\"\n</code></pre>"},{"location":"retrievers/#the-query-section-optional","title":"The <code>query</code> section (optional)","text":"<p>Controls search behavior. You can omit this entire section.</p> Field Default Description <code>mode</code> auto <code>fts</code>, <code>embedding</code>, or <code>hybrid</code>. Auto-selects <code>hybrid</code> if both indexes exist <code>top_k</code> 5 Number of results to retrieve <p>Example - Override defaults:</p> <pre><code>retrievers:\n  my_search:\n    # ... other config ...\n    query:\n      mode: fts      # force FTS even if embedding index exists\n      top_k: 20      # return more results\n</code></pre>"},{"location":"retrievers/#using-a-retriever-in-operations","title":"Using a retriever in operations","text":"<p>Attach a retriever to any LLM operation (map, filter, reduce, extract) with <code>retriever: &lt;retriever_name&gt;</code>. The retrieved results are available as <code>{{ retrieval_context }}</code> in your prompt.</p> Parameter Type Default Description retriever string - Name of the retriever to use (must match a key in <code>retrievers</code>). save_retriever_output bool false If true, saves retrieved context to <code>_&lt;operation_name&gt;_retrieved_context</code> in output."},{"location":"retrievers/#map-example","title":"Map example","text":"<pre><code>- name: tag_visit\n  type: map\n  retriever: medical_r\n  save_retriever_output: true\n  output:\n    schema:\n      tag: string\n  prompt: |\n    Classify this medical visit. Related context:\n    {{ retrieval_context }}\n\n    Transcript: {{ input.src }}\n</code></pre>"},{"location":"retrievers/#filter-example","title":"Filter example","text":"<pre><code>- name: filter_relevant\n  type: filter\n  retriever: medical_r\n  prompt: |\n    Is this transcript relevant to medication counseling?\n    Context: {{ retrieval_context }}\n    Transcript: {{ input.src }}\n  output:\n    schema:\n      is_relevant: boolean\n</code></pre>"},{"location":"retrievers/#reduce-example","title":"Reduce example","text":"<p>When using reduce, the retrieval context is computed per group.</p> <pre><code>- name: summarize_by_medication\n  type: reduce\n  retriever: medical_r\n  reduce_key: medication\n  output:\n    schema:\n      summary: string\n  prompt: |\n    Summarize key points for medication '{{ reduce_key.medication }}'.\n    Related context: {{ retrieval_context }}\n\n    Inputs:\n    {% for item in inputs %}\n    - {{ item.src }}\n    {% endfor %}\n</code></pre>"},{"location":"retrievers/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No results: the retriever injects \"No extra context available.\" and continues.</li> <li>Index issues: set <code>build_index: always</code> to rebuild; ensure <code>index_dir</code> exists and is writable.</li> <li>Token limits: <code>retrieval_context</code> is truncated to ~1000 chars per retrieved doc.</li> </ul>"},{"location":"tutorial-pythonapi/","title":"Tutorial: Analyzing Medical Transcripts with DocETL Python API","text":"<p>This tutorial will guide you through the process of using DocETL's Python API to analyze medical transcripts and extract medication information. We'll create a pipeline that identifies medications, resolves similar names, and generates summaries of side effects and therapeutic uses.</p>"},{"location":"tutorial-pythonapi/#prerequisites-and-setup","title":"Prerequisites and Setup","text":"<p>For installation instructions, API key setup, and data preparation, please refer to the YAML-based tutorial. The prerequisite steps are identical for both the YAML and Python API approaches.</p> <p>Common Setup</p> <p>Both this Python API tutorial and the YAML-based tutorial share the same:</p> <ul> <li>Installation requirements</li> <li>API key configuration</li> <li>Data format and preparation steps</li> </ul> <p>Once you've completed those steps from the main tutorial, you can continue with this Python API implementation.</p>"},{"location":"tutorial-pythonapi/#creating-the-pipeline-with-python-api","title":"Creating the Pipeline with Python API","text":"<p>Now, let's create a DocETL pipeline using the Python API to analyze this data. We'll use a series of operations to extract and process the medication information:</p> <ol> <li>Medication Extraction: Analyze each transcript to identify and list all mentioned medications.</li> <li>Unnesting: The extracted medication list is flattened, such that each medication (and associated data) is a separate document.</li> <li>Medication Resolution: Similar medication names are resolved to standardize the entries. This step helps in consolidating different variations or brand names of the same medication.</li> <li>Summary Generation: For each unique medication, generate a summary of side effects and therapeutic uses based on information from all relevant transcripts.</li> </ol> <p>Create a Python script named <code>medical_analysis.py</code> with the following code:</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, UnnestOp, ResolveOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define the dataset - JSON file with medical transcripts\ndataset = Dataset(\n    type=\"file\",\n    path=\"medical_transcripts.json\"\n)\n\n# Define operations\noperations = [\n    # Extract medications from each transcript\n    MapOp(\n        name=\"extract_medications\",\n        type=\"map\",\n        prompt=\"\"\"\n        Analyze the following transcript of a conversation between a doctor and a patient:\n        {{ input.src }}\n        Extract and list all medications mentioned in the transcript.\n        If no medications are mentioned, return an empty list.\n        \"\"\",\n        output={\n            \"schema\": {\n                \"medication\": \"list[str]\"\n            }\n        }\n    ),\n    # Unnest to create separate items for each medication\n    UnnestOp(\n        name=\"unnest_medications\",\n        type=\"unnest\",\n        unnest_key=\"medication\"\n    ),\n    # Resolve similar medication names\n    ResolveOp(\n        name=\"resolve_medications\",\n        type=\"resolve\",\n        blocking_keys=[\"medication\"],\n        blocking_threshold=0.6162,\n        comparison_prompt=\"\"\"\n        Compare the following two medication entries:\n        Entry 1: {{ input1.medication }}\n        Entry 2: {{ input2.medication }}\n        Are these medications likely to be the same or closely related?\n        \"\"\",\n        embedding_model=\"text-embedding-3-small\",\n        output={\n            \"schema\": {\n                \"medication\": \"str\"\n            }\n        },\n        resolution_prompt=\"\"\"\n        Given the following matched medication entries:\n        {% for entry in inputs %}\n        Entry {{ loop.index }}: {{ entry.medication }}\n        {% endfor %}\n        Determine the best resolved medication name for this group of entries. The resolved\n        name should be a standardized, widely recognized medication name that best represents\n        all matched entries.\n        \"\"\"\n    ),\n    # Summarize side effects and uses for each medication\n    ReduceOp(\n        name=\"summarize_prescriptions\",\n        type=\"reduce\",\n        reduce_key=[\"medication\"],\n        prompt=\"\"\"\n        Here are some transcripts of conversations between a doctor and a patient:\n\n        {% for value in inputs %}\n        Transcript {{ loop.index }}:\n        {{ value.src }}\n        {% endfor %}\n\n        For the medication {{ reduce_key }}, please provide the following information based on all the transcripts above:\n\n        1. Side Effects: Summarize all mentioned side effects of {{ reduce_key }}.\n        2. Therapeutic Uses: Explain the medical conditions or symptoms for which {{ reduce_key }} was prescribed or recommended.\n\n        Ensure your summary:\n        - Is based solely on information from the provided transcripts\n        - Focuses only on {{ reduce_key }}, not other medications\n        - Includes relevant details from all transcripts\n        - Is clear and concise\n        - Includes quotes from the transcripts\n        \"\"\",\n        output={\n            \"schema\": {\n                \"side_effects\": \"str\",\n                \"uses\": \"str\"\n            }\n        }\n    )\n]\n\n# Define pipeline step\nstep = PipelineStep(\n    name=\"medical_info_extraction\",\n    input=\"transcripts\",\n    operations=[\n        \"extract_medications\",\n        \"unnest_medications\",\n        \"resolve_medications\",\n        \"summarize_prescriptions\"\n    ]\n)\n\n# Define output\noutput = PipelineOutput(\n    type=\"file\",\n    path=\"medication_summaries.json\",\n    intermediate_dir=\"intermediate_results\"\n)\n\n# Define system prompt (optional but recommended)\nsystem_prompt = {\n    \"dataset_description\": \"a collection of transcripts of doctor visits\",\n    \"persona\": \"a medical practitioner analyzing patient symptoms and reactions to medications\"\n}\n\n# Create the pipeline\npipeline = Pipeline(\n    name=\"medical_transcript_analysis\",\n    datasets={\"transcripts\": dataset},\n    operations=operations,\n    steps=[step],\n    output=output,\n    default_model=\"gpt-4o-mini\",\n    system_prompt=system_prompt\n)\n\n# Run the pipeline\ncost = pipeline.run()\nprint(f\"Pipeline execution completed. Total cost: ${cost:.2f}\")\n</code></pre>"},{"location":"tutorial-pythonapi/#running-the-pipeline","title":"Running the Pipeline","text":"<p>To execute the pipeline, run the Python script:</p> <pre><code>python medical_analysis.py\n</code></pre> <p>This will process the medical transcripts, extract medication information, resolve similar medication names, and generate summaries of side effects and therapeutic uses for each medication. The results will be saved in <code>medication_summaries.json</code>.</p> <p>Pipeline Performance</p> <p>When running this pipeline on a sample dataset, we observed the following performance metrics using <code>gpt-4o-mini</code>:</p> <ul> <li>Total cost: $0.10</li> <li>Total execution time: 49.13 seconds</li> </ul> <p>If you want to run it on a smaller sample, you can add a <code>sample</code> parameter to the <code>extract_medications</code> operation:</p> <pre><code>MapOp(\n    name=\"extract_medications\",\n    type=\"map\",\n    sample=10,  # Process only 10 random transcripts\n    # ... other parameters\n)\n</code></pre>"},{"location":"tutorial-pythonapi/#optimizing-the-pipeline","title":"Optimizing the Pipeline","text":"<p>If you want to optimize the pipeline configuration (such as finding the best blocking threshold for medication resolution):</p> <pre><code># Create the pipeline\npipeline = Pipeline(\n    # ... pipeline configuration as before\n)\n\n# Optimize the pipeline before running\noptimized_pipeline = pipeline.optimize()\n\n# Run the optimized pipeline\ncost = optimized_pipeline.run()\nprint(f\"Optimized pipeline execution completed. Total cost: ${cost:.2f}\")\n</code></pre>"},{"location":"tutorial-pythonapi/#further-questions","title":"Further Questions","text":"What if I want to focus on a specific type of medication or medical condition? <p>You can modify the prompts in the <code>extract_medications</code> and <code>summarize_prescriptions</code> operations to focus on specific types of medications or medical conditions. For example, you could update the <code>extract_medications</code> prompt to only list medications related to cardiovascular diseases.</p> How can I improve the accuracy of medication name resolution? <p>The <code>resolve_medications</code> operation uses a blocking threshold and comparison prompt to identify similar medication names. You can adjust the <code>blocking_threshold</code> parameter to control the sensitivity of the matching. Lower values will match more aggressively, while higher values require closer matches. You can also customize the comparison and resolution prompts.</p> Can I process other types of medical documents with this pipeline? <p>Yes, you can adapt this pipeline to process other types of medical documents by modifying the input data format and adjusting the prompts in each operation. For example, you could use it to analyze discharge summaries, clinical notes, or research papers by updating the extraction and summarization prompts accordingly.</p> How can I use the pandas integration? <p>DocETL provides a pandas integration for several operators (map, filter, merge, agg, split, gather, unnest). Here's an example of how to use it with the medication analysis:</p> <pre><code>import pandas as pd\nfrom docetl import SemanticAccessor\n\n# Load data as DataFrame\ndf = pd.read_json(\"medical_transcripts.json\")\n\n# Use semantic map operation to extract medications\nresult_df = df.semantic.map(\n    prompt=\"\"\"\n    Analyze the following transcript:\n    {{ input.src }}\n    Extract all medications mentioned.\n    \"\"\",\n    output_schema={\"medications\": \"list[str]\"}\n)\n\n# Continue processing with other pandas operations\n</code></pre> <p>Learn more about the pandas integration in the pandas documentation. </p>"},{"location":"tutorial-pythonapi/#using-a-code-operation-to-normalize-medication-names-python-api","title":"Using a Code Operation to Normalize Medication Names (Python API)","text":"<p>You can insert a code operation in the Python API pipeline to perform per-document transformations without calling an LLM. Code ops accept a Python function (it does not need to be named <code>transform</code>).</p> <pre><code>from docetl.api import CodeMapOp\n\n# Normalize medication names (lowercase/trim)\ndef normalize_medication(doc: dict) -&gt; dict:\n    med = doc.get(\"medication\", \"\")\n    return {\"medication_norm\": med.lower().strip() if isinstance(med, str) else med}\n\n# Add this op after unnesting and before resolve\noperations = [\n    # ... existing extract_medications (MapOp), unnest_medications (UnnestOp)\n    CodeMapOp(\n        name=\"normalize_medication\",\n        type=\"code_map\",\n        code=normalize_medication,\n    ),\n    # Optionally, update Resolve/Reduce to use \"medication_norm\" instead of \"medication\"\n]\n\n# And include it in the step order, e.g.\nstep = PipelineStep(\n    name=\"medical_info_extraction\",\n    input=\"transcripts\",\n    operations=[\n        \"extract_medications\",\n        \"unnest_medications\",\n        \"normalize_medication\",  # new code op here\n        \"resolve_medications\",\n        \"summarize_prescriptions\",\n    ],\n)\n</code></pre> <p>This keeps your deterministic preprocessing in Python while still leveraging DocETL for the LLM-powered stages. </p>"},{"location":"tutorial/","title":"Tutorial: Analyzing Medical Transcripts with DocETL","text":"<p>This tutorial will guide you through the process of using DocETL to analyze medical transcripts and extract medication information. We'll create a pipeline that identifies medications, resolves similar names, and generates summaries of side effects and therapeutic uses.</p>"},{"location":"tutorial/#installation","title":"Installation","text":"<p>First, let's install DocETL. Follow the instructions in the installation guide to set up DocETL on your system.</p>"},{"location":"tutorial/#setting-up-api-keys","title":"Setting up API Keys","text":"<p>DocETL uses LiteLLM under the hood, which supports various LLM providers. For this tutorial, we'll use OpenAI, as DocETL tests and existing pipelines are run with OpenAI.</p> <p>Setting up your API Key</p> <p>Set your OpenAI API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=your_api_key_here\n</code></pre> <p>Alternatively, you can create a <code>.env</code> file in your project directory and add the following line:</p> <pre><code>OPENAI_API_KEY=your_api_key_here\n</code></pre> <p>OpenAI Dependency</p> <p>DocETL has been primarily tested with OpenAI's language models and relies heavily on their structured output capabilities. While we aim to support other providers in the future, using OpenAI is currently recommended for the best experience and most reliable results.</p> <p>If you choose to use a different provider, be aware that you may encounter unexpected behavior or reduced functionality, especially with operations that depend on structured outputs. We use tool calling to extract structured outputs from the LLM's response, so make sure your provider supports tool calling.</p> <p>If using a Gemini model, you can use the <code>gemini</code> prefix for the model name. For example, <code>gemini/gemini-2.0-flash</code>. (This has worked pretty well for us so far, and is so cheap!)</p> <p>If using Ollama (e.g., llama 3.2), make sure your output schemas are not too complex, since these models are not as good as OpenAI for structured outputs! For example, use parallel map operations to reduce the number of output attributes per prompt.</p>"},{"location":"tutorial/#preparing-the-data","title":"Preparing the Data","text":"<p>Organize your medical transcript data in a JSON file as a list of objects. Each object should have a \"src\" key containing the transcript text. You can download the example dataset here.</p> <p>Sample Data Structure</p> <pre><code>[\n    {\n        \"src\": \"Doctor: Hello, Mrs. Johnson. How have you been feeling since starting the new medication, Lisinopril?\\nPatient: Well, doctor, I've noticed my blood pressure has improved, but I've been experiencing some dry cough...\",\n    },\n    {\n        \"src\": \"Doctor: Good morning, Mr. Smith. I see you're here for a follow-up on your Metformin prescription.\\nPatient: Yes, doctor. I've been taking it regularly, but I'm concerned about some side effects I've been experiencing...\",\n    }\n]\n</code></pre> <p>Save this file as <code>medical_transcripts.json</code> in your project directory.</p>"},{"location":"tutorial/#creating-the-pipeline","title":"Creating the Pipeline","text":"<p>Now, let's create a DocETL pipeline to analyze this data. We'll use a series of operations to extract and process the medication information:</p> <ol> <li>Medication Extraction: Analyze each transcript to identify and list all mentioned medications.</li> <li>Unnesting: The extracted medication list is flattened, such that each medication (and associated data) is a separate document. This operator is akin to the pandas <code>explode</code> operation.</li> <li>Medication Resolution: Similar medication names are resolved to standardize the entries. This step helps in consolidating different variations or brand names of the same medication. For example, step 1 might extract \"Ibuprofen\" and \"Motrin 800mg\" as separate medications, and step 3 might resolve them to a single \"Ibuprofen\" entry.</li> <li>Summary Generation: For each unique medication, generate a summary of side effects and therapeutic uses based on information from all relevant transcripts.</li> </ol> <p>Create a file named <code>pipeline.yaml</code> with the following structure:</p> <p>Pipeline Structure</p> <pre><code>datasets:\n  transcripts:\n    path: medical_transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\nsystem_prompt: # This is optional, but recommended for better performance. It is applied to all operations in the pipeline.\n  dataset_description: a collection of transcripts of doctor visits\n  persona: a medical practitioner analyzing patient symptoms and reactions to medications\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the following transcript of a conversation between a doctor and a patient:\n      {{ input.src }}\n      Extract and list all medications mentioned in the transcript.\n      If no medications are mentioned, return an empty list.\n\n  - name: unnest_medications\n    type: unnest\n    unnest_key: medication\n\n  - name: resolve_medications\n    type: resolve\n    blocking_keys:\n      - medication\n    blocking_threshold: 0.6162\n    comparison_prompt: |\n      Compare the following two medication entries:\n      Entry 1: {{ input1.medication }}\n      Entry 2: {{ input2.medication }}\n      Are these medications likely to be the same or closely related?\n    embedding_model: text-embedding-3-small\n    output:\n      schema:\n        medication: str\n    resolution_prompt: |\n      Given the following matched medication entries:\n      {% for entry in inputs %}\n      Entry {{ loop.index }}: {{ entry.medication }}\n      {% endfor %}\n      Determine the best resolved medication name for this group of entries. The resolved\n      name should be a standardized, widely recognized medication name that best represents\n      all matched entries.\n\n  - name: summarize_prescriptions\n    type: reduce\n    reduce_key:\n      - medication\n    output:\n      schema:\n        side_effects: str\n        uses: str\n    prompt: |\n      Here are some transcripts of conversations between a doctor and a patient:\n\n      {% for value in inputs %}\n      Transcript {{ loop.index }}:\n      {{ value.src }}\n      {% endfor %}\n\n      For the medication {{ reduce_key }}, please provide the following information based on all the transcripts above:\n\n      1. Side Effects: Summarize all mentioned side effects of {{ reduce_key }}.\n      2. Therapeutic Uses: Explain the medical conditions or symptoms for which {{ reduce_key }} was prescribed or recommended.\n\n      Ensure your summary:\n      - Is based solely on information from the provided transcripts\n      - Focuses only on {{ reduce_key }}, not other medications\n      - Includes relevant details from all transcripts\n      - Is clear and concise\n      - Includes quotes from the transcripts\n\npipeline:\n  steps:\n    - name: medical_info_extraction\n      input: transcripts\n      operations:\n        - extract_medications\n        - unnest_medications\n        - resolve_medications\n        - summarize_prescriptions\n  output:\n    type: file\n    path: medication_summaries.json\n    intermediate_dir: intermediate_results\n</code></pre>"},{"location":"tutorial/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Pipeline Performance</p> <p>When running this pipeline on a sample dataset, we observed the following performance metrics using <code>gpt-4o-mini</code> as defined in the pipeline:</p> <ul> <li>Total cost: $0.10</li> <li>Total execution time: 49.13 seconds</li> </ul> <p>If you want to run it on a smaller sample, set the <code>sample</code> parameter for the map operation. For example, <code>sample: 10</code> will run the pipeline on a random sample of 10 transcripts:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    sample: 10\n    ...\n</code></pre> <p>To execute the pipeline, run the following command in your terminal:</p> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>This will process the medical transcripts, extract medication information, resolve similar medication names, and generate summaries of side effects and therapeutic uses for each medication. The results will be saved in <code>medication_summaries.json</code>.</p>"},{"location":"tutorial/#further-questions","title":"Further Questions","text":"What if I want to focus on a specific type of medication or medical condition? <p>You can modify the prompts in the <code>extract_medications</code> and <code>summarize_prescriptions</code> operations to focus on specific types of medications or medical conditions. For example, you could update the <code>extract_medications</code> prompt to only list medications related to cardiovascular diseases.</p> How can I improve the accuracy of medication name resolution? <p>The <code>resolve_medications</code> operation uses a blocking threshold and comparison prompt to identify similar medication names. Learn more about how to configure this operation in the resolve operation documentation. To automatically find the optimal blocking threshold for your data, you can invoke the optimizer, as described in the optimization documentation.</p> Can I process other types of medical documents with this pipeline? <p>Yes, you can adapt this pipeline to process other types of medical documents by modifying the input data format and adjusting the prompts in each operation. For example, you could use it to analyze discharge summaries, clinical notes, or research papers by updating the extraction and summarization prompts accordingly.</p> How can I optimize the performance of this pipeline? <p>If you're unsure about the optimal configuration for your specific use case, you can use DocETL's optimizer, which can be invoked using <code>docetl build</code> instead of <code>docetl run</code>. Learn more about the optimizer in the optimization documentation.</p> How can I use the pandas integration? <p>DocETL provides a pandas integration for a few operators (map, filter, merge, agg). Learn more about the pandas integration in the pandas documentation.</p>"},{"location":"api-reference/cli/","title":"CLI Interface","text":""},{"location":"api-reference/cli/#docetl.cli.run","title":"<code>docetl.cli.run(yaml_file=typer.Argument(..., help='Path to the YAML file containing the pipeline configuration'), max_threads=typer.Option(None, help='Maximum number of threads to use for running operations'))</code>","text":"<p>Run the configuration specified in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>max_threads</code> <code>int | None</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef run(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    max_threads: int | None = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n):\n    \"\"\"\n    Run the configuration specified in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        max_threads (int | None): Maximum number of threads to use for running operations.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    runner = DSLRunner.from_yaml(str(yaml_file), max_threads=max_threads)\n    runner.load_run_save()\n</code></pre>"},{"location":"api-reference/cli/#docetl.cli.build","title":"<code>docetl.cli.build(yaml_file=typer.Argument(..., help='Path to the YAML file containing the pipeline configuration'), optimizer=typer.Option('moar', '--optimizer', '-o', help=\"Optimizer to use: 'moar' (default) or 'v1' (deprecated)\"), max_threads=typer.Option(None, help='Maximum number of threads to use for running operations'), resume=typer.Option(False, help='Resume optimization from a previous build that may have failed'), save_path=typer.Option(None, help='Path to save the optimized pipeline configuration'))</code>","text":"<p>Build and optimize the configuration specified in the YAML file. Any arguments passed here will override the values in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>optimizer</code> <code>str</code> <p>Optimizer to use - 'moar' or 'v1' (required).</p> <code>Option('moar', '--optimizer', '-o', help=\"Optimizer to use: 'moar' (default) or 'v1' (deprecated)\")</code> <code>max_threads</code> <code>int | None</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous run. Defaults to False.</p> <code>Option(False, help='Resume optimization from a previous build that may have failed')</code> <code>save_path</code> <code>Path</code> <p>Path to save the optimized pipeline configuration.</p> <code>Option(None, help='Path to save the optimized pipeline configuration')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef build(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    optimizer: str = typer.Option(\n        \"moar\",\n        \"--optimizer\",\n        \"-o\",\n        help=\"Optimizer to use: 'moar' (default) or 'v1' (deprecated)\",\n    ),\n    max_threads: int | None = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n    resume: bool = typer.Option(\n        False, help=\"Resume optimization from a previous build that may have failed\"\n    ),\n    save_path: Path = typer.Option(\n        None, help=\"Path to save the optimized pipeline configuration\"\n    ),\n):\n    \"\"\"\n    Build and optimize the configuration specified in the YAML file.\n    Any arguments passed here will override the values in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        optimizer (str): Optimizer to use - 'moar' or 'v1' (required).\n        max_threads (int | None): Maximum number of threads to use for running operations.\n        resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n        save_path (Path): Path to save the optimized pipeline configuration.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    # Validate optimizer choice\n    if optimizer not in [\"moar\", \"v1\"]:\n        typer.echo(\n            f\"Error: optimizer must be 'moar' or 'v1', got '{optimizer}'\", err=True\n        )\n        raise typer.Exit(1)\n\n    # Load YAML to check for optimizer_config\n    import yaml as yaml_lib\n\n    with open(yaml_file, \"r\") as f:\n        config = yaml_lib.safe_load(f)\n\n    if optimizer == \"moar\":\n        optimizer_config = config.get(\"optimizer_config\", {})\n        if not optimizer_config:\n            example_yaml = \"\"\"optimizer_config:\n  type: moar\n  save_dir: ./moar_results\n  available_models:\n    - gpt-5\n    - gpt-4o\n  evaluation_file: workloads/medical/evaluate_medications.py\n  metric_key: medication_extraction_score\n  max_iterations: 40\n  model: gpt-5\"\"\"\n\n            error_panel = Panel(\n                f\"[bold red]Error:[/bold red] optimizer_config section is required in YAML for MOAR optimizer.\\n\\n\"\n                f\"[bold]Example:[/bold]\\n\"\n                f\"[dim]{example_yaml}[/dim]\\n\\n\"\n                f\"[yellow]Note:[/yellow] dataset_name is inferred from the 'datasets' section. \"\n                f\"dataset_path can optionally be specified in optimizer_config, otherwise it's inferred from the 'datasets' section.\",\n                title=\"[bold red]Missing optimizer_config[/bold red]\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            raise typer.Exit(1)\n\n        if optimizer_config.get(\"type\") != \"moar\":\n            error_panel = Panel(\n                f\"[bold red]Error:[/bold red] optimizer_config.type must be 'moar', got '[yellow]{optimizer_config.get('type')}[/yellow]'\",\n                title=\"[bold red]Invalid optimizer type[/bold red]\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            raise typer.Exit(1)\n\n        # Validate required fields in optimizer_config\n        required_fields = {\n            \"save_dir\": \"Output directory for MOAR results\",\n            \"available_models\": \"List of model names to use\",\n            \"evaluation_file\": \"Path to evaluation function file\",\n            \"metric_key\": \"Key to extract from evaluation results\",\n            \"max_iterations\": \"Number of MOARSearch iterations\",\n            \"model\": \"LLM model name for directive instantiation\",\n        }\n\n        missing_fields = [\n            field for field in required_fields if not optimizer_config.get(field)\n        ]\n        if missing_fields:\n            # Create a table for required fields\n            fields_table = Table(\n                show_header=True, header_style=\"bold cyan\", box=None, padding=(0, 2)\n            )\n            fields_table.add_column(\"Field\", style=\"yellow\")\n            fields_table.add_column(\"Description\", style=\"dim\")\n\n            for field, desc in required_fields.items():\n                style = \"bold red\" if field in missing_fields else \"dim\"\n                fields_table.add_row(f\"[{style}]{field}[/{style}]\", desc)\n\n            # Create example YAML\n            example_yaml = \"\"\"optimizer_config:\n  type: moar\n  save_dir: ./moar_results\n  available_models:\n    - gpt-5\n    - gpt-4o\n  evaluation_file: workloads/medical/evaluate_medications.py\n  metric_key: medication_extraction_score\n  max_iterations: 40\n  model: gpt-5\"\"\"\n\n            missing_list = \", \".join(\n                [f\"[bold red]{f}[/bold red]\" for f in missing_fields]\n            )\n\n            # Build error content with table rendered separately\n            from rich.console import Group\n\n            error_group = Group(\n                f\"[bold red]Missing required fields:[/bold red] {missing_list}\\n\",\n                \"[bold]Required fields:[/bold]\",\n                fields_table,\n                f\"\\n[bold]Example:[/bold]\\n[dim]{example_yaml}[/dim]\\n\",\n                \"[yellow]Note:[/yellow] dataset_name is inferred from the 'datasets' section. \"\n                \"dataset_path can optionally be specified in optimizer_config, otherwise it's inferred from the 'datasets' section.\",\n            )\n\n            error_panel = Panel(\n                error_group,\n                title=\"[bold red]Missing Required Fields[/bold red]\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            raise typer.Exit(1)\n\n        # Run MOAR optimization\n        from docetl.moar.cli_helpers import run_moar_optimization\n\n        try:\n            results = run_moar_optimization(\n                yaml_path=str(yaml_file),\n                optimizer_config=optimizer_config,\n            )\n            typer.echo(\"\\n\u2705 MOAR optimization completed successfully!\")\n            typer.echo(f\"   Results saved to: {optimizer_config.get('save_dir')}\")\n            if results.get(\"evaluation_file\"):\n                typer.echo(f\"   Evaluation: {results['evaluation_file']}\")\n        except Exception as e:\n            typer.echo(f\"Error running MOAR optimization: {e}\", err=True)\n            raise typer.Exit(1)\n\n    else:  # v1 optimizer (deprecated)\n        console.print(\n            Panel(\n                \"[bold yellow]Warning:[/bold yellow] The V1 optimizer is deprecated. \"\n                \"Please use MOAR optimizer instead: [bold]docetl build pipeline.yaml --optimizer moar[/bold]\",\n                title=\"[bold yellow]Deprecated Optimizer[/bold yellow]\",\n                border_style=\"yellow\",\n            )\n        )\n        runner = DSLRunner.from_yaml(str(yaml_file), max_threads=max_threads)\n        runner.optimize(\n            save=True,\n            return_pipeline=False,\n            resume=resume,\n            save_path=save_path,\n        )\n</code></pre>"},{"location":"api-reference/cli/#docetl.cli.clear_cache","title":"<code>docetl.cli.clear_cache()</code>","text":"<p>Clear the LLM cache stored on disk.</p> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef clear_cache():\n    \"\"\"\n    Clear the LLM cache stored on disk.\n    \"\"\"\n    cc()\n</code></pre>"},{"location":"api-reference/docetl/","title":"docetl Core","text":""},{"location":"api-reference/docetl/#docetl.DSLRunner","title":"<code>docetl.DSLRunner</code>","text":"<p>               Bases: <code>ConfigWrapper</code></p> <p>DSLRunner orchestrates pipeline execution by building and traversing a DAG of OpContainers. The runner uses a two-phase approach:</p> <ol> <li>Build Phase:</li> <li>Parses YAML config into a DAG of OpContainers</li> <li>Each operation becomes a node connected to its dependencies</li> <li>Special handling for equijoins which have two parent nodes</li> <li> <p>Validates operation syntax and schema compatibility</p> </li> <li> <p>Execution Phase:</p> </li> <li>Starts from the final operation and pulls data through the DAG</li> <li>Handles caching/checkpointing of intermediate results</li> <li>Tracks costs and execution metrics</li> <li>Manages dataset loading and result persistence</li> </ol> <p>The separation between build and execution phases allows for: - Pipeline validation before any execution - Cost estimation and optimization - Partial pipeline execution for testing</p> Source code in <code>docetl/runner.py</code> <pre><code>class DSLRunner(ConfigWrapper):\n    \"\"\"\n    DSLRunner orchestrates pipeline execution by building and traversing a DAG of OpContainers.\n    The runner uses a two-phase approach:\n\n    1. Build Phase:\n       - Parses YAML config into a DAG of OpContainers\n       - Each operation becomes a node connected to its dependencies\n       - Special handling for equijoins which have two parent nodes\n       - Validates operation syntax and schema compatibility\n\n    2. Execution Phase:\n       - Starts from the final operation and pulls data through the DAG\n       - Handles caching/checkpointing of intermediate results\n       - Tracks costs and execution metrics\n       - Manages dataset loading and result persistence\n\n    The separation between build and execution phases allows for:\n    - Pipeline validation before any execution\n    - Cost estimation and optimization\n    - Partial pipeline execution for testing\n    \"\"\"\n\n    @classproperty\n    def schema(cls):\n        # Accessing the schema loads all operations, so only do this\n        # when we actually need it...\n        # Yes, this means DSLRunner.schema isn't really accessible to\n        # static type checkers. But it /is/ available for dynamic\n        # checking, and for generating json schema.\n\n        OpType = functools.reduce(\n            lambda a, b: a | b, [op.schema for op in get_operations().values()]\n        )\n        # More pythonic implementation of the above, but only works in python 3.11:\n        # OpType = Union[*[op.schema for op in get_operations().values()]]\n\n        class Pipeline(BaseModel):\n            config: dict[str, Any] | None\n            parsing_tools: list[schemas.ParsingTool] | None\n            datasets: dict[str, schemas.Dataset]\n            retrievers: dict[str, Any] | None\n            operations: list[OpType]\n            pipeline: schemas.PipelineSpec\n\n        return Pipeline\n\n    @classproperty\n    def json_schema(cls):\n        return cls.schema.model_json_schema()\n\n    def __init__(self, config: dict, max_threads: int | None = None, **kwargs):\n        \"\"\"\n        Initialize the DSLRunner with a YAML configuration file.\n\n        Args:\n            max_threads (int, optional): Maximum number of threads to use. Defaults to None.\n        \"\"\"\n        super().__init__(\n            config,\n            base_name=kwargs.pop(\"base_name\", None),\n            yaml_file_suffix=kwargs.pop(\"yaml_file_suffix\", None),\n            max_threads=max_threads,\n            **kwargs,\n        )\n        self.total_cost = 0\n        self._initialize_state()\n        self._setup_parsing_tools()\n        self._setup_retrievers()\n        self._build_operation_graph(config)\n        self._compute_operation_hashes()\n\n        # Run initial validation\n        self._from_df_accessors = kwargs.get(\"from_df_accessors\", False)\n        if not self._from_df_accessors:\n            self.syntax_check()\n\n    def _initialize_state(self) -&gt; None:\n        \"\"\"Initialize basic runner state and datasets\"\"\"\n        self.datasets = {}\n        self.intermediate_dir = (\n            self.config.get(\"pipeline\", {}).get(\"output\", {}).get(\"intermediate_dir\")\n        )\n\n    def _setup_parsing_tools(self) -&gt; None:\n        \"\"\"Set up parsing tools from configuration\"\"\"\n        self.parsing_tool_map = create_parsing_tool_map(\n            self.config.get(\"parsing_tools\", None)\n        )\n\n    def _setup_retrievers(self) -&gt; None:\n        \"\"\"Instantiate retrievers from configuration (lazy index creation).\"\"\"\n        from docetl.retrievers.lancedb import LanceDBRetriever\n\n        self.retrievers: dict[str, Any] = {}\n        retrievers_cfg = self.config.get(\"retrievers\", {}) or {}\n        for name, rconf in retrievers_cfg.items():\n            if not isinstance(rconf, dict):\n                raise ValueError(f\"Invalid retriever '{name}' configuration\")\n            if rconf.get(\"type\") != \"lancedb\":\n                raise ValueError(\n                    f\"Unsupported retriever type '{rconf.get('type')}' for '{name}'. Only 'lancedb' is supported.\"\n                )\n            required = [\"dataset\", \"index_dir\", \"index_types\"]\n            for key in required:\n                if key not in rconf:\n                    raise ValueError(\n                        f\"Retriever '{name}' missing required key '{key}'.\"\n                    )\n            # Defaults\n            rconf.setdefault(\"query\", {\"top_k\": 5})\n            rconf.setdefault(\"build_index\", \"if_missing\")\n\n            self.retrievers[name] = LanceDBRetriever(self, name, rconf)\n\n    def _build_operation_graph(self, config: dict) -&gt; None:\n        \"\"\"Build the DAG of operations from configuration\"\"\"\n        self.config = config\n        self.op_container_map = {}\n        self.last_op_container = None\n\n        for step in self.config[\"pipeline\"][\"steps\"]:\n            self._validate_step(step)\n\n            if step.get(\"input\"):\n                self._add_scan_operation(step)\n            else:\n                self._add_equijoin_operation(step)\n\n            self._add_step_operations(step)\n            self._add_step_boundary(step)\n\n    def _validate_step(self, step: dict) -&gt; None:\n        \"\"\"Validate step configuration\"\"\"\n        assert \"name\" in step.keys(), f\"Step {step} does not have a name\"\n        assert \"operations\" in step.keys(), f\"Step {step} does not have `operations`\"\n\n    def _add_scan_operation(self, step: dict) -&gt; None:\n        \"\"\"Add a scan operation for input datasets\"\"\"\n        scan_op_container = OpContainer(\n            f\"{step['name']}/scan_{step['input']}\",\n            self,\n            {\n                \"type\": \"scan\",\n                \"dataset_name\": step[\"input\"],\n                \"name\": f\"scan_{step['input']}\",\n            },\n        )\n        self.op_container_map[f\"{step['name']}/scan_{step['input']}\"] = (\n            scan_op_container\n        )\n        if self.last_op_container:\n            scan_op_container.add_child(self.last_op_container)\n        self.last_op_container = scan_op_container\n\n    def _add_equijoin_operation(self, step: dict) -&gt; None:\n        \"\"\"Add an equijoin operation with its scan operations\"\"\"\n        equijoin_operation_name = list(step[\"operations\"][0].keys())[0]\n        left_dataset_name = list(step[\"operations\"][0].values())[0][\"left\"]\n        right_dataset_name = list(step[\"operations\"][0].values())[0][\"right\"]\n\n        left_scan_op_container = OpContainer(\n            f\"{step['name']}/scan_{left_dataset_name}\",\n            self,\n            {\n                \"type\": \"scan\",\n                \"dataset_name\": left_dataset_name,\n                \"name\": f\"scan_{left_dataset_name}\",\n            },\n        )\n        if self.last_op_container:\n            left_scan_op_container.add_child(self.last_op_container)\n        right_scan_op_container = OpContainer(\n            f\"{step['name']}/scan_{right_dataset_name}\",\n            self,\n            {\n                \"type\": \"scan\",\n                \"dataset_name\": right_dataset_name,\n                \"name\": f\"scan_{right_dataset_name}\",\n            },\n        )\n        if self.last_op_container:\n            right_scan_op_container.add_child(self.last_op_container)\n        equijoin_op_container = OpContainer(\n            f\"{step['name']}/{equijoin_operation_name}\",\n            self,\n            self.find_operation(equijoin_operation_name),\n            left_name=left_dataset_name,\n            right_name=right_dataset_name,\n        )\n\n        equijoin_op_container.add_child(left_scan_op_container)\n        equijoin_op_container.add_child(right_scan_op_container)\n\n        self.last_op_container = equijoin_op_container\n        self.op_container_map[f\"{step['name']}/{equijoin_operation_name}\"] = (\n            equijoin_op_container\n        )\n        self.op_container_map[f\"{step['name']}/scan_{left_dataset_name}\"] = (\n            left_scan_op_container\n        )\n        self.op_container_map[f\"{step['name']}/scan_{right_dataset_name}\"] = (\n            right_scan_op_container\n        )\n\n    def _add_step_operations(self, step: dict) -&gt; None:\n        \"\"\"Add operations for a step\"\"\"\n        op_start_idx = 1 if not step.get(\"input\") else 0\n\n        for operation_name in step[\"operations\"][op_start_idx:]:\n            if not isinstance(operation_name, str):\n                raise ValueError(\n                    f\"Operation {operation_name} in step {step['name']} should be a string. \"\n                    \"If you intend for it to be an equijoin, don't specify an input in the step.\"\n                )\n\n            op_container = OpContainer(\n                f\"{step['name']}/{operation_name}\",\n                self,\n                self.find_operation(operation_name),\n            )\n            op_container.add_child(self.last_op_container)\n            self.last_op_container = op_container\n            self.op_container_map[f\"{step['name']}/{operation_name}\"] = op_container\n\n    def _add_step_boundary(self, step: dict) -&gt; None:\n        \"\"\"Add a step boundary node\"\"\"\n        step_boundary = StepBoundary(\n            f\"{step['name']}/boundary\",\n            self,\n            {\"type\": \"step_boundary\", \"name\": f\"{step['name']}/boundary\"},\n        )\n        step_boundary.add_child(self.last_op_container)\n        self.op_container_map[f\"{step['name']}/boundary\"] = step_boundary\n        self.last_op_container = step_boundary\n\n    def _compute_operation_hashes(self) -&gt; None:\n        \"\"\"Compute hashes for operations to enable caching\"\"\"\n        op_map = {op[\"name\"]: op for op in self.config[\"operations\"]}\n        self.step_op_hashes = defaultdict(dict)\n\n        for step in self.config[\"pipeline\"][\"steps\"]:\n            for idx, op in enumerate(step[\"operations\"]):\n                op_name = op if isinstance(op, str) else list(op.keys())[0]\n\n                all_ops_until_and_including_current = (\n                    [op_map[prev_op] for prev_op in step[\"operations\"][:idx]]\n                    + [op_map[op_name]]\n                    + [self.config.get(\"system_prompt\", {})]\n                )\n\n                for op in all_ops_until_and_including_current:\n                    if \"model\" not in op:\n                        op[\"model\"] = self.default_model\n\n                all_ops_str = json.dumps(all_ops_until_and_including_current)\n                self.step_op_hashes[step[\"name\"]][op_name] = hashlib.sha256(\n                    all_ops_str.encode()\n                ).hexdigest()\n\n    def get_output_path(self, require=False):\n        output_path = self.config.get(\"pipeline\", {}).get(\"output\", {}).get(\"path\")\n        if output_path:\n            if not (\n                output_path.lower().endswith(\".json\")\n                or output_path.lower().endswith(\".csv\")\n            ):\n                raise ValueError(\n                    f\"Output path '{output_path}' is not a JSON or CSV file. Please provide a path ending with '.json' or '.csv'.\"\n                )\n        elif require:\n            raise ValueError(\n                \"No output path specified in the configuration. Please provide an output path ending with '.json' or '.csv' in the configuration to use the save() method.\"\n            )\n\n        return output_path\n\n    def syntax_check(self):\n        \"\"\"\n        Perform a syntax check on all operations defined in the configuration.\n        \"\"\"\n        self.console.log(\"[yellow]Checking operations...[/yellow]\")\n\n        # Just validate that it's a json file if specified\n        self.get_output_path()\n        current = self.last_op_container\n\n        try:\n            # Walk the last op container to check syntax\n            op_containers = []\n            if self.last_op_container:\n                op_containers = [self.last_op_container]\n\n            while op_containers:\n                current = op_containers.pop(0)\n                syntax_result = current.syntax_check()\n                self.console.log(syntax_result, end=\"\")\n                # Add all children to the queue\n                op_containers.extend(current.children)\n        except Exception as e:\n            raise ValueError(\n                f\"Syntax check failed for operation '{current.name}': {str(e)}\"\n            )\n\n        self.console.log(\"[green]\u2713 All operations passed syntax check[/green]\")\n\n    def print_query_plan(self, show_boundaries=False):\n        \"\"\"\n        Print a visual representation of the entire query plan using indentation and arrows.\n        Operations are color-coded by step to show the pipeline structure while maintaining\n        dependencies between steps.\n        \"\"\"\n        if not self.last_op_container:\n            self.console.log(\"\\n[bold]Pipeline Steps:[/bold]\")\n            self.console.log(\n                Panel(\"No operations in pipeline\", title=\"Query Plan\", width=100)\n            )\n            self.console.log()\n            return\n\n        def _print_op(\n            op: OpContainer, indent: int = 0, step_colors: dict[str, str] | None = None\n        ) -&gt; str:\n            # Handle boundary operations based on show_boundaries flag\n            if isinstance(op, StepBoundary):\n                if show_boundaries:\n                    output = []\n                    indent_str = \"  \" * indent\n                    step_name = op.name.split(\"/\")[0]\n                    color = step_colors.get(step_name, \"white\")\n                    output.append(\n                        f\"{indent_str}[{color}][bold]{op.name}[/bold][/{color}]\"\n                    )\n                    output.append(f\"{indent_str}Type: step_boundary\")\n                    if op.children:\n                        output.append(f\"{indent_str}[yellow]\u25bc[/yellow]\")\n                        for child in op.children:\n                            output.append(_print_op(child, indent + 1, step_colors))\n                    return \"\\n\".join(output)\n                elif op.children:\n                    return _print_op(op.children[0], indent, step_colors)\n                return \"\"\n\n            # Build the string for the current operation with indentation\n            indent_str = \"  \" * indent\n            output = []\n\n            # Color code the operation name based on its step\n            step_name = op.name.split(\"/\")[0]\n            color = step_colors.get(step_name, \"white\")\n            output.append(f\"{indent_str}[{color}][bold]{op.name}[/bold][/{color}]\")\n            output.append(f\"{indent_str}Type: {op.config['type']}\")\n\n            # Add schema if available\n            if \"output\" in op.config and \"schema\" in op.config[\"output\"]:\n                output.append(f\"{indent_str}Output Schema:\")\n                for field, field_type in op.config[\"output\"][\"schema\"].items():\n                    escaped_type = escape(str(field_type))\n                    output.append(\n                        f\"{indent_str}  {field}: [bright_white]{escaped_type}[/bright_white]\"\n                    )\n\n            # Add children\n            if op.children:\n                if op.is_equijoin:\n                    output.append(f\"{indent_str}[yellow]\u25bc LEFT[/yellow]\")\n                    output.append(_print_op(op.children[0], indent + 1, step_colors))\n                    output.append(f\"{indent_str}[yellow]\u25bc RIGHT[/yellow]\")\n                    output.append(_print_op(op.children[1], indent + 1, step_colors))\n                else:\n                    output.append(f\"{indent_str}[yellow]\u25bc[/yellow]\")\n                    for child in op.children:\n                        output.append(_print_op(child, indent + 1, step_colors))\n\n            return \"\\n\".join(output)\n\n        # Get all step boundaries and extract unique step names\n        step_boundaries = [\n            op\n            for name, op in self.op_container_map.items()\n            if isinstance(op, StepBoundary)\n        ]\n        step_boundaries.sort(key=lambda x: x.name)\n\n        # Create a color map for steps - using distinct colors\n        colors = [\"cyan\", \"magenta\", \"green\", \"yellow\", \"blue\", \"red\"]\n        step_names = [b.name.split(\"/\")[0] for b in step_boundaries]\n        step_colors = {\n            name: colors[i % len(colors)] for i, name in enumerate(step_names)\n        }\n\n        # Print the legend\n        self.console.log(\"\\n[bold]Pipeline Steps:[/bold]\")\n        for step_name, color in step_colors.items():\n            self.console.log(f\"[{color}]\u25a0[/{color}] {step_name}\")\n\n        # Print the full query plan starting from the last step boundary\n        query_plan = _print_op(self.last_op_container, step_colors=step_colors)\n        self.console.log(Panel(query_plan, title=\"Query Plan\", width=100))\n        self.console.log()\n\n    def find_operation(self, op_name: str) -&gt; dict:\n        for operation_config in self.config[\"operations\"]:\n            if operation_config[\"name\"] == op_name:\n                return operation_config\n        raise ValueError(f\"Operation '{op_name}' not found in configuration.\")\n\n    def load_run_save(self) -&gt; float:\n        \"\"\"\n        Execute the entire pipeline defined in the configuration.\n        \"\"\"\n        output_path = self.get_output_path(require=True)\n\n        # Print the query plan\n        self.print_query_plan()\n\n        start_time = time.time()\n\n        if self.last_op_container:\n            self.load()\n            self.console.rule(\"[bold]Pipeline Execution[/bold]\")\n            output, _, _ = self.last_op_container.next()\n            self.save(output)\n\n        execution_time = time.time() - start_time\n\n        # Print execution summary\n        summary = (\n            f\"Cost: [green]${self.total_cost:.2f}[/green]\\n\"\n            f\"Time: {execution_time:.2f}s\\n\"\n            + (\n                f\"Cache: [dim]{self.intermediate_dir}[/dim]\\n\"\n                if self.intermediate_dir\n                else \"\"\n            )\n            + f\"Output: [dim]{output_path}[/dim]\"\n        )\n        self.console.log(Panel(summary, title=\"Execution Summary\"))\n\n        return self.total_cost\n\n    def load(self) -&gt; None:\n        \"\"\"\n        Load all datasets defined in the configuration.\n        \"\"\"\n        datasets = {}\n        self.console.rule(\"[bold]Loading Datasets[/bold]\")\n\n        for name, dataset_config in self.config[\"datasets\"].items():\n            if dataset_config[\"type\"] == \"file\":\n                datasets[name] = Dataset(\n                    self,\n                    \"file\",\n                    dataset_config[\"path\"],\n                    source=\"local\",\n                    parsing=dataset_config.get(\"parsing\", []),\n                    user_defined_parsing_tool_map=self.parsing_tool_map,\n                )\n                self.console.log(\n                    f\"[green]\u2713[/green] Loaded dataset '{name}' from {dataset_config['path']}\"\n                )\n            elif dataset_config[\"type\"] == \"memory\":\n                datasets[name] = Dataset(\n                    self,\n                    \"memory\",\n                    dataset_config[\"path\"],\n                    source=\"local\",\n                    parsing=dataset_config.get(\"parsing\", []),\n                    user_defined_parsing_tool_map=self.parsing_tool_map,\n                )\n                self.console.log(\n                    f\"[green]\u2713[/green] Loaded dataset '{name}' from in-memory data\"\n                )\n            else:\n                raise ValueError(f\"Unsupported dataset type: {dataset_config['type']}\")\n\n        self.datasets = {\n            name: (\n                dataset\n                if isinstance(dataset, Dataset)\n                else Dataset(self, \"memory\", dataset)\n            )\n            for name, dataset in datasets.items()\n        }\n        self.console.log()\n\n    def save(self, data: list[dict]) -&gt; None:\n        \"\"\"\n        Save the final output of the pipeline.\n        \"\"\"\n        self.get_output_path(require=True)\n\n        output_config = self.config[\"pipeline\"][\"output\"]\n        if output_config[\"type\"] == \"file\":\n            # Create the directory if it doesn't exist\n            if os.path.dirname(output_config[\"path\"]):\n                os.makedirs(os.path.dirname(output_config[\"path\"]), exist_ok=True)\n            if output_config[\"path\"].lower().endswith(\".json\"):\n                with open(output_config[\"path\"], \"w\") as file:\n                    json.dump(data, file, indent=2)\n            else:  # CSV\n                import csv\n\n                with open(output_config[\"path\"], \"w\", newline=\"\") as file:\n                    writer = csv.DictWriter(file, fieldnames=data[0].keys())\n                    limited_data = [\n                        {k: d.get(k, None) for k in data[0].keys()} for d in data\n                    ]\n                    writer.writeheader()\n                    writer.writerows(limited_data)\n            self.console.log(\n                f\"[green]\u2713[/green] Saved to [dim]{output_config['path']}[/dim]\\n\"\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported output type: {output_config['type']}. Supported types: file\"\n            )\n\n    def _load_from_checkpoint_if_exists(\n        self, step_name: str, operation_name: str\n    ) -&gt; list[dict] | None:\n        if self.intermediate_dir is None or self.config.get(\"bypass_cache\", False):\n            return None\n\n        intermediate_config_path = os.path.join(\n            self.intermediate_dir, \".docetl_intermediate_config.json\"\n        )\n\n        if not os.path.exists(intermediate_config_path):\n            return None\n\n        # Make sure the step and op name is in the checkpoint config path\n        if (\n            step_name not in self.step_op_hashes\n            or operation_name not in self.step_op_hashes[step_name]\n        ):\n            return None\n\n        # See if the checkpoint config is the same as the current step op hash\n        with open(intermediate_config_path, \"r\") as f:\n            intermediate_config = json.load(f)\n\n        if (\n            intermediate_config.get(step_name, {}).get(operation_name, \"\")\n            != self.step_op_hashes[step_name][operation_name]\n        ):\n            return None\n\n        checkpoint_path = os.path.join(\n            self.intermediate_dir, step_name, f\"{operation_name}.json\"\n        )\n        # check if checkpoint exists\n        if os.path.exists(checkpoint_path):\n            if f\"{step_name}_{operation_name}\" not in self.datasets:\n                self.datasets[f\"{step_name}_{operation_name}\"] = Dataset(\n                    self, \"file\", checkpoint_path, \"local\"\n                )\n\n                self.console.log(\n                    f\"[green]\u2713[/green] [italic]Loaded checkpoint for operation '{operation_name}' in step '{step_name}' from {checkpoint_path}[/italic]\"\n                )\n\n                return self.datasets[f\"{step_name}_{operation_name}\"].load()\n        return None\n\n    def clear_intermediate(self) -&gt; None:\n        \"\"\"\n        Clear the intermediate directory.\n        \"\"\"\n        # Remove the intermediate directory\n        if self.intermediate_dir:\n            shutil.rmtree(self.intermediate_dir)\n            return\n\n        raise ValueError(\"Intermediate directory not set. Cannot clear intermediate.\")\n\n    def _save_checkpoint(\n        self, step_name: str, operation_name: str, data: list[dict]\n    ) -&gt; None:\n        \"\"\"\n        Save a checkpoint of the current data after an operation.\n\n        This method creates a JSON file containing the current state of the data\n        after an operation has been executed. The checkpoint is saved in a directory\n        structure that reflects the step and operation names.\n\n        Args:\n            step_name (str): The name of the current step in the pipeline.\n            operation_name (str): The name of the operation that was just executed.\n            data (list[dict]): The current state of the data to be checkpointed.\n\n        Note:\n            The checkpoint is saved only if a checkpoint directory has been specified\n            when initializing the DSLRunner.\n        \"\"\"\n        checkpoint_path = os.path.join(\n            self.intermediate_dir, step_name, f\"{operation_name}.json\"\n        )\n        if os.path.dirname(checkpoint_path):\n            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n        with open(checkpoint_path, \"w\") as f:\n            json.dump(data, f)\n\n        # Update the intermediate config file with the hash for this step/operation\n        # so that future runs can validate and reuse this checkpoint.\n        if self.intermediate_dir:\n            intermediate_config_path = os.path.join(\n                self.intermediate_dir, \".docetl_intermediate_config.json\"\n            )\n\n            # Initialize or load existing intermediate configuration\n            if os.path.exists(intermediate_config_path):\n                try:\n                    with open(intermediate_config_path, \"r\") as cfg_file:\n                        intermediate_config: dict[str, dict[str, str]] = json.load(\n                            cfg_file\n                        )\n                except json.JSONDecodeError:\n                    # If the file is corrupted, start fresh to avoid crashes\n                    intermediate_config = {}\n            else:\n                intermediate_config = {}\n\n            # Ensure nested dict structure exists\n            step_dict = intermediate_config.setdefault(step_name, {})\n\n            # Write (or overwrite) the hash for the current operation\n            step_dict[operation_name] = self.step_op_hashes[step_name][operation_name]\n\n            # Persist the updated configuration\n            with open(intermediate_config_path, \"w\") as cfg_file:\n                json.dump(intermediate_config, cfg_file, indent=2)\n\n        self.console.log(\n            f\"[green]\u2713 [italic]Intermediate saved for operation '{operation_name}' in step '{step_name}' at {checkpoint_path}[/italic][/green]\"\n        )\n\n    def should_optimize(\n        self, step_name: str, op_name: str, **kwargs\n    ) -&gt; tuple[str, float, list[dict[str, Any]], list[dict[str, Any]]]:\n        self.load()\n\n        # Augment the kwargs with the runner's config if not already provided\n        kwargs[\"litellm_kwargs\"] = self.config.get(\"optimizer_config\", {}).get(\n            \"litellm_kwargs\", {}\n        )\n        kwargs[\"rewrite_agent_model\"] = self.config.get(\"optimizer_config\", {}).get(\n            \"rewrite_agent_model\", \"gpt-5.1\"\n        )\n        kwargs[\"judge_agent_model\"] = self.config.get(\"optimizer_config\", {}).get(\n            \"judge_agent_model\", \"gpt-4o-mini\"\n        )\n\n        builder = Optimizer(self, **kwargs)\n        self.optimizer = builder\n        result = builder.should_optimize(step_name, op_name)\n        return result\n\n    def optimize(\n        self,\n        save: bool = False,\n        return_pipeline: bool = True,\n        **kwargs,\n    ) -&gt; tuple[dict | \"DSLRunner\", float]:\n\n        if not self.last_op_container:\n            raise ValueError(\"No operations in pipeline. Cannot optimize.\")\n\n        self.load()\n\n        # Augment the kwargs with the runner's config if not already provided\n        kwargs[\"litellm_kwargs\"] = self.config.get(\"optimizer_config\", {}).get(\n            \"litellm_kwargs\", {}\n        )\n        kwargs[\"rewrite_agent_model\"] = self.config.get(\"optimizer_config\", {}).get(\n            \"rewrite_agent_model\", \"gpt-5.1\"\n        )\n        kwargs[\"judge_agent_model\"] = self.config.get(\"optimizer_config\", {}).get(\n            \"judge_agent_model\", \"gpt-4o-mini\"\n        )\n\n        save_path = kwargs.get(\"save_path\", None)\n        # Pop the save_path from kwargs\n        kwargs.pop(\"save_path\", None)\n\n        builder = Optimizer(\n            self,\n            **kwargs,\n        )\n        self.optimizer = builder\n        llm_api_cost = builder.optimize()\n        operations_cost = self.total_cost\n        self.total_cost += llm_api_cost\n\n        # Log the cost of optimization\n        self.console.log(\n            f\"[green italic]\ud83d\udcb0 Total cost: ${self.total_cost:.4f}[/green italic]\"\n        )\n        self.console.log(\n            f\"[green italic]  \u251c\u2500 Operation execution cost: ${operations_cost:.4f}[/green italic]\"\n        )\n        self.console.log(\n            f\"[green italic]  \u2514\u2500 Optimization cost: ${llm_api_cost:.4f}[/green italic]\"\n        )\n\n        if save:\n            # If output path is provided, save the optimized config to that path\n            if kwargs.get(\"save_path\"):\n                save_path = kwargs[\"save_path\"]\n                if not os.path.isabs(save_path):\n                    save_path = os.path.join(os.getcwd(), save_path)\n                builder.save_optimized_config(save_path)\n                self.optimized_config_path = save_path\n            else:\n                builder.save_optimized_config(f\"{self.base_name}_opt.yaml\")\n                self.optimized_config_path = f\"{self.base_name}_opt.yaml\"\n\n        if return_pipeline:\n            return (\n                DSLRunner(builder.clean_optimized_config(), self.max_threads),\n                self.total_cost,\n            )\n\n        return builder.clean_optimized_config(), self.total_cost\n\n    def _run_operation(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]] | dict[str, Any],\n        return_instance: bool = False,\n        is_build: bool = False,\n    ) -&gt; list[dict[str, Any]] | tuple[list[dict[str, Any]], BaseOperation, float]:\n        \"\"\"\n        Run a single operation based on its configuration.\n\n        This method creates an instance of the appropriate operation class and executes it.\n        It also updates the total operation cost.\n\n        Args:\n            op_config (dict[str, Any]): The configuration of the operation to run.\n            input_data (list[dict[str, Any]]): The input data for the operation.\n            return_instance (bool, optional): If True, return the operation instance along with the output data.\n\n        Returns:\n            list[dict[str, Any]] | tuple[list[dict[str, Any]], BaseOperation, float]:\n            If return_instance is False, returns the output data.\n            If return_instance is True, returns a tuple of the output data, the operation instance, and the cost.\n        \"\"\"\n        operation_class = get_operation(op_config[\"type\"])\n\n        oc_kwargs = {\n            \"runner\": self,\n            \"config\": op_config,\n            \"default_model\": self.config[\"default_model\"],\n            \"max_threads\": self.max_threads,\n            \"console\": self.console,\n            \"status\": self.status,\n        }\n        operation_instance = operation_class(**oc_kwargs)\n        if op_config[\"type\"] == \"equijoin\":\n            output_data, cost = operation_instance.execute(\n                input_data[\"left_data\"], input_data[\"right_data\"]\n            )\n        elif op_config[\"type\"] == \"filter\":\n            output_data, cost = operation_instance.execute(input_data, is_build)\n        else:\n            output_data, cost = operation_instance.execute(input_data)\n\n        self.total_cost += cost\n\n        if return_instance:\n            return output_data, operation_instance\n        else:\n            return output_data\n\n    def _flush_partial_results(\n        self, operation_name: str, batch_index: int, data: list[dict]\n    ) -&gt; None:\n        \"\"\"\n        Save partial (batch-level) results from an operation to a directory named\n        '&lt;operation_name&gt;_batches' inside the intermediate directory.\n\n        Args:\n            operation_name (str): The name of the operation, e.g. 'extract_medications'.\n            batch_index (int): Zero-based index of the batch.\n            data (list[dict]): Batch results to write to disk.\n        \"\"\"\n        if not self.intermediate_dir:\n            return\n\n        op_batches_dir = os.path.join(\n            self.intermediate_dir, f\"{operation_name}_batches\"\n        )\n        os.makedirs(op_batches_dir, exist_ok=True)\n\n        # File name: 'batch_0.json', 'batch_1.json', etc.\n        checkpoint_path = os.path.join(op_batches_dir, f\"batch_{batch_index}.json\")\n\n        with open(checkpoint_path, \"w\") as f:\n            json.dump(data, f)\n\n        self.console.log(\n            f\"[green]\u2713[/green] [italic]Partial checkpoint saved for '{operation_name}', \"\n            f\"batch {batch_index} at '{checkpoint_path}'[/italic]\"\n        )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.__init__","title":"<code>__init__(config, max_threads=None, **kwargs)</code>","text":"<p>Initialize the DSLRunner with a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>max_threads</code> <code>int</code> <p>Maximum number of threads to use. Defaults to None.</p> <code>None</code> Source code in <code>docetl/runner.py</code> <pre><code>def __init__(self, config: dict, max_threads: int | None = None, **kwargs):\n    \"\"\"\n    Initialize the DSLRunner with a YAML configuration file.\n\n    Args:\n        max_threads (int, optional): Maximum number of threads to use. Defaults to None.\n    \"\"\"\n    super().__init__(\n        config,\n        base_name=kwargs.pop(\"base_name\", None),\n        yaml_file_suffix=kwargs.pop(\"yaml_file_suffix\", None),\n        max_threads=max_threads,\n        **kwargs,\n    )\n    self.total_cost = 0\n    self._initialize_state()\n    self._setup_parsing_tools()\n    self._setup_retrievers()\n    self._build_operation_graph(config)\n    self._compute_operation_hashes()\n\n    # Run initial validation\n    self._from_df_accessors = kwargs.get(\"from_df_accessors\", False)\n    if not self._from_df_accessors:\n        self.syntax_check()\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.clear_intermediate","title":"<code>clear_intermediate()</code>","text":"<p>Clear the intermediate directory.</p> Source code in <code>docetl/runner.py</code> <pre><code>def clear_intermediate(self) -&gt; None:\n    \"\"\"\n    Clear the intermediate directory.\n    \"\"\"\n    # Remove the intermediate directory\n    if self.intermediate_dir:\n        shutil.rmtree(self.intermediate_dir)\n        return\n\n    raise ValueError(\"Intermediate directory not set. Cannot clear intermediate.\")\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.load","title":"<code>load()</code>","text":"<p>Load all datasets defined in the configuration.</p> Source code in <code>docetl/runner.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"\n    Load all datasets defined in the configuration.\n    \"\"\"\n    datasets = {}\n    self.console.rule(\"[bold]Loading Datasets[/bold]\")\n\n    for name, dataset_config in self.config[\"datasets\"].items():\n        if dataset_config[\"type\"] == \"file\":\n            datasets[name] = Dataset(\n                self,\n                \"file\",\n                dataset_config[\"path\"],\n                source=\"local\",\n                parsing=dataset_config.get(\"parsing\", []),\n                user_defined_parsing_tool_map=self.parsing_tool_map,\n            )\n            self.console.log(\n                f\"[green]\u2713[/green] Loaded dataset '{name}' from {dataset_config['path']}\"\n            )\n        elif dataset_config[\"type\"] == \"memory\":\n            datasets[name] = Dataset(\n                self,\n                \"memory\",\n                dataset_config[\"path\"],\n                source=\"local\",\n                parsing=dataset_config.get(\"parsing\", []),\n                user_defined_parsing_tool_map=self.parsing_tool_map,\n            )\n            self.console.log(\n                f\"[green]\u2713[/green] Loaded dataset '{name}' from in-memory data\"\n            )\n        else:\n            raise ValueError(f\"Unsupported dataset type: {dataset_config['type']}\")\n\n    self.datasets = {\n        name: (\n            dataset\n            if isinstance(dataset, Dataset)\n            else Dataset(self, \"memory\", dataset)\n        )\n        for name, dataset in datasets.items()\n    }\n    self.console.log()\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.load_run_save","title":"<code>load_run_save()</code>","text":"<p>Execute the entire pipeline defined in the configuration.</p> Source code in <code>docetl/runner.py</code> <pre><code>def load_run_save(self) -&gt; float:\n    \"\"\"\n    Execute the entire pipeline defined in the configuration.\n    \"\"\"\n    output_path = self.get_output_path(require=True)\n\n    # Print the query plan\n    self.print_query_plan()\n\n    start_time = time.time()\n\n    if self.last_op_container:\n        self.load()\n        self.console.rule(\"[bold]Pipeline Execution[/bold]\")\n        output, _, _ = self.last_op_container.next()\n        self.save(output)\n\n    execution_time = time.time() - start_time\n\n    # Print execution summary\n    summary = (\n        f\"Cost: [green]${self.total_cost:.2f}[/green]\\n\"\n        f\"Time: {execution_time:.2f}s\\n\"\n        + (\n            f\"Cache: [dim]{self.intermediate_dir}[/dim]\\n\"\n            if self.intermediate_dir\n            else \"\"\n        )\n        + f\"Output: [dim]{output_path}[/dim]\"\n    )\n    self.console.log(Panel(summary, title=\"Execution Summary\"))\n\n    return self.total_cost\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.print_query_plan","title":"<code>print_query_plan(show_boundaries=False)</code>","text":"<p>Print a visual representation of the entire query plan using indentation and arrows. Operations are color-coded by step to show the pipeline structure while maintaining dependencies between steps.</p> Source code in <code>docetl/runner.py</code> <pre><code>def print_query_plan(self, show_boundaries=False):\n    \"\"\"\n    Print a visual representation of the entire query plan using indentation and arrows.\n    Operations are color-coded by step to show the pipeline structure while maintaining\n    dependencies between steps.\n    \"\"\"\n    if not self.last_op_container:\n        self.console.log(\"\\n[bold]Pipeline Steps:[/bold]\")\n        self.console.log(\n            Panel(\"No operations in pipeline\", title=\"Query Plan\", width=100)\n        )\n        self.console.log()\n        return\n\n    def _print_op(\n        op: OpContainer, indent: int = 0, step_colors: dict[str, str] | None = None\n    ) -&gt; str:\n        # Handle boundary operations based on show_boundaries flag\n        if isinstance(op, StepBoundary):\n            if show_boundaries:\n                output = []\n                indent_str = \"  \" * indent\n                step_name = op.name.split(\"/\")[0]\n                color = step_colors.get(step_name, \"white\")\n                output.append(\n                    f\"{indent_str}[{color}][bold]{op.name}[/bold][/{color}]\"\n                )\n                output.append(f\"{indent_str}Type: step_boundary\")\n                if op.children:\n                    output.append(f\"{indent_str}[yellow]\u25bc[/yellow]\")\n                    for child in op.children:\n                        output.append(_print_op(child, indent + 1, step_colors))\n                return \"\\n\".join(output)\n            elif op.children:\n                return _print_op(op.children[0], indent, step_colors)\n            return \"\"\n\n        # Build the string for the current operation with indentation\n        indent_str = \"  \" * indent\n        output = []\n\n        # Color code the operation name based on its step\n        step_name = op.name.split(\"/\")[0]\n        color = step_colors.get(step_name, \"white\")\n        output.append(f\"{indent_str}[{color}][bold]{op.name}[/bold][/{color}]\")\n        output.append(f\"{indent_str}Type: {op.config['type']}\")\n\n        # Add schema if available\n        if \"output\" in op.config and \"schema\" in op.config[\"output\"]:\n            output.append(f\"{indent_str}Output Schema:\")\n            for field, field_type in op.config[\"output\"][\"schema\"].items():\n                escaped_type = escape(str(field_type))\n                output.append(\n                    f\"{indent_str}  {field}: [bright_white]{escaped_type}[/bright_white]\"\n                )\n\n        # Add children\n        if op.children:\n            if op.is_equijoin:\n                output.append(f\"{indent_str}[yellow]\u25bc LEFT[/yellow]\")\n                output.append(_print_op(op.children[0], indent + 1, step_colors))\n                output.append(f\"{indent_str}[yellow]\u25bc RIGHT[/yellow]\")\n                output.append(_print_op(op.children[1], indent + 1, step_colors))\n            else:\n                output.append(f\"{indent_str}[yellow]\u25bc[/yellow]\")\n                for child in op.children:\n                    output.append(_print_op(child, indent + 1, step_colors))\n\n        return \"\\n\".join(output)\n\n    # Get all step boundaries and extract unique step names\n    step_boundaries = [\n        op\n        for name, op in self.op_container_map.items()\n        if isinstance(op, StepBoundary)\n    ]\n    step_boundaries.sort(key=lambda x: x.name)\n\n    # Create a color map for steps - using distinct colors\n    colors = [\"cyan\", \"magenta\", \"green\", \"yellow\", \"blue\", \"red\"]\n    step_names = [b.name.split(\"/\")[0] for b in step_boundaries]\n    step_colors = {\n        name: colors[i % len(colors)] for i, name in enumerate(step_names)\n    }\n\n    # Print the legend\n    self.console.log(\"\\n[bold]Pipeline Steps:[/bold]\")\n    for step_name, color in step_colors.items():\n        self.console.log(f\"[{color}]\u25a0[/{color}] {step_name}\")\n\n    # Print the full query plan starting from the last step boundary\n    query_plan = _print_op(self.last_op_container, step_colors=step_colors)\n    self.console.log(Panel(query_plan, title=\"Query Plan\", width=100))\n    self.console.log()\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.save","title":"<code>save(data)</code>","text":"<p>Save the final output of the pipeline.</p> Source code in <code>docetl/runner.py</code> <pre><code>def save(self, data: list[dict]) -&gt; None:\n    \"\"\"\n    Save the final output of the pipeline.\n    \"\"\"\n    self.get_output_path(require=True)\n\n    output_config = self.config[\"pipeline\"][\"output\"]\n    if output_config[\"type\"] == \"file\":\n        # Create the directory if it doesn't exist\n        if os.path.dirname(output_config[\"path\"]):\n            os.makedirs(os.path.dirname(output_config[\"path\"]), exist_ok=True)\n        if output_config[\"path\"].lower().endswith(\".json\"):\n            with open(output_config[\"path\"], \"w\") as file:\n                json.dump(data, file, indent=2)\n        else:  # CSV\n            import csv\n\n            with open(output_config[\"path\"], \"w\", newline=\"\") as file:\n                writer = csv.DictWriter(file, fieldnames=data[0].keys())\n                limited_data = [\n                    {k: d.get(k, None) for k in data[0].keys()} for d in data\n                ]\n                writer.writeheader()\n                writer.writerows(limited_data)\n        self.console.log(\n            f\"[green]\u2713[/green] Saved to [dim]{output_config['path']}[/dim]\\n\"\n        )\n    else:\n        raise ValueError(\n            f\"Unsupported output type: {output_config['type']}. Supported types: file\"\n        )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Perform a syntax check on all operations defined in the configuration.</p> Source code in <code>docetl/runner.py</code> <pre><code>def syntax_check(self):\n    \"\"\"\n    Perform a syntax check on all operations defined in the configuration.\n    \"\"\"\n    self.console.log(\"[yellow]Checking operations...[/yellow]\")\n\n    # Just validate that it's a json file if specified\n    self.get_output_path()\n    current = self.last_op_container\n\n    try:\n        # Walk the last op container to check syntax\n        op_containers = []\n        if self.last_op_container:\n            op_containers = [self.last_op_container]\n\n        while op_containers:\n            current = op_containers.pop(0)\n            syntax_result = current.syntax_check()\n            self.console.log(syntax_result, end=\"\")\n            # Add all children to the queue\n            op_containers.extend(current.children)\n    except Exception as e:\n        raise ValueError(\n            f\"Syntax check failed for operation '{current.name}': {str(e)}\"\n        )\n\n    self.console.log(\"[green]\u2713 All operations passed syntax check[/green]\")\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer","title":"<code>docetl.Optimizer</code>","text":"<p>Orchestrates the optimization of a DocETL pipeline by analyzing and potentially rewriting operations marked for optimization. Works with the runner's pull-based execution model to maintain lazy evaluation while improving pipeline efficiency.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>class Optimizer:\n    \"\"\"\n    Orchestrates the optimization of a DocETL pipeline by analyzing and potentially rewriting\n    operations marked for optimization. Works with the runner's pull-based execution model\n    to maintain lazy evaluation while improving pipeline efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        runner: \"DSLRunner\",\n        rewrite_agent_model: str = \"gpt-5.1\",\n        judge_agent_model: str = \"gpt-4o-mini\",\n        litellm_kwargs: dict[str, Any] = {},\n        resume: bool = False,\n        timeout: int = 60,\n    ):\n        \"\"\"\n        Initialize the optimizer with a runner instance and configuration.\n        Sets up optimization parameters, caching, and cost tracking.\n\n        Args:\n            yaml_file (str): Path to the YAML configuration file.\n            model (str): The name of the language model to use. Defaults to \"gpt-5.1\".\n            resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n            timeout (int): Timeout in seconds for operations. Defaults to 60.\n\n        Attributes:\n            config (Dict): Stores the loaded configuration from the YAML file.\n            console (Console): Rich console for formatted output.\n            max_threads (int): Maximum number of threads for parallel processing.\n            base_name (str): Base name used for file paths.\n            yaml_file_suffix (str): Suffix for YAML configuration files.\n            runner (DSLRunner): The DSL runner instance.\n            status: Status tracking for the runner.\n            optimized_config (Dict): A copy of the original config to be optimized.\n            llm_client (LLMClient): Client for interacting with the language model.\n            timeout (int): Timeout for operations in seconds.\n            resume (bool): Whether to resume from previous optimization.\n            captured_output (CapturedOutput): Captures output during optimization.\n            sample_cache (Dict): Maps operation names to tuples of (output_data, sample_size).\n            optimized_ops_path (str): Path to store optimized operations.\n            sample_size_map (Dict): Maps operation types to sample sizes.\n\n        The method also calls print_optimizer_config() to display the initial configuration.\n        \"\"\"\n        self.config = runner.config\n        self.console = runner.console\n        self.max_threads = runner.max_threads\n\n        self.base_name = runner.base_name\n        self.yaml_file_suffix = runner.yaml_file_suffix\n        self.runner = runner\n        self.status = runner.status\n\n        self.optimized_config = copy.deepcopy(self.config)\n\n        # Get the rate limits from the optimizer config\n        rate_limits = self.config.get(\"optimizer_config\", {}).get(\"rate_limits\", {})\n\n        self.llm_client = LLMClient(\n            runner,\n            rewrite_agent_model,\n            judge_agent_model,\n            rate_limits,\n            **litellm_kwargs,\n        )\n        self.timeout = timeout\n        self.resume = resume\n        self.captured_output = CapturedOutput()\n\n        # Add sample cache for build operations\n        self.sample_cache = {}  # Maps operation names to (output_data, sample_size)\n\n        home_dir = os.environ.get(\"DOCETL_HOME_DIR\", os.path.expanduser(\"~\"))\n        cache_dir = os.path.join(home_dir, f\".docetl/cache/{runner.yaml_file_suffix}\")\n        os.makedirs(cache_dir, exist_ok=True)\n\n        # Hash the config to create a unique identifier\n        config_hash = hashlib.sha256(str(self.config).encode()).hexdigest()\n        self.optimized_ops_path = f\"{cache_dir}/{config_hash}.yaml\"\n\n        # Update sample size map\n        self.sample_size_map = SAMPLE_SIZE_MAP\n        if self.config.get(\"optimizer_config\", {}).get(\"sample_sizes\", {}):\n            self.sample_size_map.update(self.config[\"optimizer_config\"][\"sample_sizes\"])\n\n        if not self.runner._from_df_accessors:\n            self.print_optimizer_config()\n\n    def print_optimizer_config(self):\n        \"\"\"\n        Print the current configuration of the optimizer.\n\n        This method uses the Rich console to display a formatted output of the optimizer's\n        configuration. It includes details such as the YAML file path, sample sizes for\n        different operation types, maximum number of threads, the language model being used,\n        and the timeout setting.\n\n        The output is color-coded and formatted for easy readability, with a header and\n        separator lines to clearly delineate the configuration information.\n        \"\"\"\n        self.console.log(\n            Panel.fit(\n                \"[bold cyan]Optimizer Configuration[/bold cyan]\\n\"\n                f\"[yellow]Sample Size:[/yellow] {self.sample_size_map}\\n\"\n                f\"[yellow]Max Threads:[/yellow] {self.max_threads}\\n\"\n                f\"[yellow]Rewrite Agent Model:[/yellow] {self.llm_client.rewrite_agent_model}\\n\"\n                f\"[yellow]Judge Agent Model:[/yellow] {self.llm_client.judge_agent_model}\\n\"\n                f\"[yellow]Rate Limits:[/yellow] {self.config.get('optimizer_config', {}).get('rate_limits', {})}\\n\",\n                title=\"Optimizer Configuration\",\n            )\n        )\n\n    def _insert_empty_resolve_operations(self):\n        \"\"\"\n        Determines whether to insert resolve operations in the pipeline.\n\n        For each reduce operation in the tree, checks if it has any map operation as a descendant\n        without a resolve operation in between. If found, inserts an empty resolve operation\n        right after the reduce operation.\n\n        The method modifies the operation container tree in-place.\n\n        Returns:\n            None\n        \"\"\"\n        if not self.runner.last_op_container:\n            return\n\n        def find_map_without_resolve(container, visited=None):\n            \"\"\"Helper to find first map descendant without a resolve operation in between.\"\"\"\n            if visited is None:\n                visited = set()\n\n            if container.name in visited:\n                return None\n            visited.add(container.name)\n\n            if not container.children:\n                return None\n\n            for child in container.children:\n                if child.config[\"type\"] == \"map\":\n                    return child\n                if child.config[\"type\"] == \"resolve\":\n                    continue\n                map_desc = find_map_without_resolve(child, visited)\n                if map_desc:\n                    return map_desc\n            return None\n\n        # Walk down the operation container tree\n        containers_to_check = [self.runner.last_op_container]\n        while containers_to_check:\n            current = containers_to_check.pop(0)\n\n            # Skip if this is a boundary or has no children\n            if isinstance(current, StepBoundary) or not current.children:\n                containers_to_check.extend(current.children)\n                continue\n\n            # Get the step name from the container's name\n            step_name = current.name.split(\"/\")[0]\n\n            # Check if current container is a reduce operation\n            if current.config[\"type\"] == \"reduce\" and current.config.get(\n                \"synthesize_resolve\", True\n            ):\n                reduce_key = current.config.get(\"reduce_key\", \"_all\")\n                if isinstance(reduce_key, str):\n                    reduce_key = [reduce_key]\n\n                if \"_all\" not in reduce_key:\n                    # Find map descendant without resolve\n                    map_desc = find_map_without_resolve(current)\n                    if map_desc:\n                        # Synthesize an empty resolver\n                        self.console.log(\n                            \"[yellow]Synthesizing empty resolver operation:[/yellow]\"\n                        )\n                        self.console.log(\n                            f\"  \u2022 [cyan]Reduce operation:[/cyan] [bold]{current.name}[/bold]\"\n                        )\n                        self.console.log(\n                            f\"  \u2022 [cyan]Step:[/cyan] [bold]{step_name}[/bold]\"\n                        )\n\n                        # Create new resolve operation config\n                        new_resolve_name = (\n                            f\"synthesized_resolve_{len(self.config['operations'])}\"\n                        )\n                        new_resolve_config = {\n                            \"name\": new_resolve_name,\n                            \"type\": \"resolve\",\n                            \"empty\": True,\n                            \"optimize\": True,\n                            \"embedding_model\": \"text-embedding-3-small\",\n                            \"resolution_model\": self.config.get(\n                                \"default_model\", \"gpt-4o-mini\"\n                            ),\n                            \"comparison_model\": self.config.get(\n                                \"default_model\", \"gpt-4o-mini\"\n                            ),\n                            \"_intermediates\": {\n                                \"map_prompt\": map_desc.config.get(\"prompt\"),\n                                \"reduce_key\": reduce_key,\n                            },\n                        }\n\n                        # Add to operations list\n                        self.config[\"operations\"].append(new_resolve_config)\n\n                        # Create new resolve container\n                        new_resolve_container = OpContainer(\n                            f\"{step_name}/{new_resolve_name}\",\n                            self.runner,\n                            new_resolve_config,\n                        )\n\n                        # Insert the new container between reduce and its children\n                        new_resolve_container.children = current.children\n                        for child in new_resolve_container.children:\n                            child.parent = new_resolve_container\n                        current.children = [new_resolve_container]\n                        new_resolve_container.parent = current\n\n                        # Add to container map\n                        self.runner.op_container_map[\n                            f\"{step_name}/{new_resolve_name}\"\n                        ] = new_resolve_container\n\n                        # Add children to the queue\n                        containers_to_check.extend(new_resolve_container.children)\n\n    def _add_map_prompts_to_reduce_operations(self):\n        \"\"\"\n        Add relevant map prompts to reduce operations based on their reduce keys.\n\n        This method walks the operation container tree to find map operations and their\n        output schemas, then associates those with reduce operations that use those keys.\n        When a reduce operation is found, it looks through its descendants to find the\n        relevant map operations and adds their prompts.\n\n        The method modifies the operation container tree in-place.\n        \"\"\"\n        if not self.runner.last_op_container:\n            return\n\n        def find_map_prompts_for_keys(container, keys, visited=None):\n            \"\"\"Helper to find map prompts for given keys in the container's descendants.\"\"\"\n            if visited is None:\n                visited = set()\n\n            if container.name in visited:\n                return []\n            visited.add(container.name)\n\n            prompts = []\n            if container.config[\"type\"] == \"map\":\n                output_schema = container.config.get(\"output\", {}).get(\"schema\", {})\n                if any(key in output_schema for key in keys):\n                    prompts.append(container.config.get(\"prompt\", \"\"))\n\n            for child in container.children:\n                prompts.extend(find_map_prompts_for_keys(child, keys, visited))\n\n            return prompts\n\n        # Walk down the operation container tree\n        containers_to_check = [self.runner.last_op_container]\n        while containers_to_check:\n            current = containers_to_check.pop(0)\n\n            # Skip if this is a boundary or has no children\n            if isinstance(current, StepBoundary) or not current.children:\n                containers_to_check.extend(current.children)\n                continue\n\n            # If this is a reduce operation, find relevant map prompts\n            if current.config[\"type\"] == \"reduce\":\n                reduce_keys = current.config.get(\"reduce_key\", [])\n                if isinstance(reduce_keys, str):\n                    reduce_keys = [reduce_keys]\n\n                # Find map prompts in descendants\n                relevant_prompts = find_map_prompts_for_keys(current, reduce_keys)\n\n                if relevant_prompts:\n                    current.config[\"_intermediates\"] = current.config.get(\n                        \"_intermediates\", {}\n                    )\n                    current.config[\"_intermediates\"][\"last_map_prompt\"] = (\n                        relevant_prompts[-1]\n                    )\n\n            # Add children to the queue\n            containers_to_check.extend(current.children)\n\n    def should_optimize(\n        self, step_name: str, op_name: str\n    ) -&gt; tuple[str, list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"\n        Analyzes whether an operation should be optimized by running it on a sample of input data\n        and evaluating potential optimizations. Returns the optimization suggestion and relevant data.\n        \"\"\"\n        self.console.rule(\"[bold cyan]Beginning Pipeline Assessment[/bold cyan]\")\n\n        self._insert_empty_resolve_operations()\n\n        node_of_interest = self.runner.op_container_map[f\"{step_name}/{op_name}\"]\n\n        # Run the node_of_interest's children\n        input_data = []\n        for child in node_of_interest.children:\n            input_data.append(\n                child.next(\n                    is_build=True,\n                    sample_size_needed=SAMPLE_SIZE_MAP.get(child.config[\"type\"]),\n                )[0]\n            )\n\n        # Set the step\n        self.captured_output.set_step(step_name)\n\n        # Determine whether we should optimize the node_of_interest\n        if (\n            node_of_interest.config.get(\"type\") == \"map\"\n            or node_of_interest.config.get(\"type\") == \"filter\"\n        ):\n            # Create instance of map optimizer\n            map_optimizer = MapOptimizer(\n                self.runner,\n                self.runner._run_operation,\n                is_filter=node_of_interest.config.get(\"type\") == \"filter\",\n            )\n            should_optimize_output, input_data, output_data = (\n                map_optimizer.should_optimize(node_of_interest.config, input_data[0])\n            )\n        elif node_of_interest.config.get(\"type\") == \"reduce\":\n            reduce_optimizer = ReduceOptimizer(\n                self.runner,\n                self.runner._run_operation,\n            )\n            should_optimize_output, input_data, output_data = (\n                reduce_optimizer.should_optimize(node_of_interest.config, input_data[0])\n            )\n        elif node_of_interest.config.get(\"type\") == \"resolve\":\n            resolve_optimizer = JoinOptimizer(\n                self.runner,\n                node_of_interest.config,\n                target_recall=self.config.get(\"optimizer_config\", {})\n                .get(\"resolve\", {})\n                .get(\"target_recall\", 0.95),\n            )\n            _, should_optimize_output = resolve_optimizer.should_optimize(input_data[0])\n\n            # if should_optimize_output is empty, then we should move to the reduce operation\n            if should_optimize_output == \"\":\n                return \"\", [], [], 0.0\n        else:\n            return \"\", [], [], 0.0\n\n        # Return the string and operation cost\n        return (\n            should_optimize_output,\n            input_data,\n            output_data,\n            self.runner.total_cost + self.llm_client.total_cost,\n        )\n\n    def optimize(self) -&gt; float:\n        \"\"\"\n        Optimizes the entire pipeline by walking the operation DAG and applying\n        operation-specific optimizers where marked. Returns the total optimization cost.\n        \"\"\"\n        self.console.rule(\"[bold cyan]Beginning Pipeline Rewrites[/bold cyan]\")\n\n        # If self.resume is True and there's a checkpoint, load it\n        if self.resume:\n            if os.path.exists(self.optimized_ops_path):\n                # Load the yaml and change the runner with it\n                with open(self.optimized_ops_path, \"r\") as f:\n                    partial_optimized_config = yaml.safe_load(f)\n                    self.console.log(\n                        \"[yellow]Loading partially optimized pipeline from checkpoint...[/yellow]\"\n                    )\n                    self.runner._build_operation_graph(partial_optimized_config)\n            else:\n                self.console.log(\n                    \"[yellow]No checkpoint found, starting optimization from scratch...[/yellow]\"\n                )\n\n        else:\n            self._insert_empty_resolve_operations()\n\n        # Start with the last operation container and visit each child\n        self.runner.last_op_container.optimize()\n\n        flush_cache(self.console)\n\n        # Print the query plan\n        self.console.rule(\"[bold cyan]Optimized Query Plan[/bold cyan]\")\n        self.runner.print_query_plan()\n\n        return self.llm_client.total_cost\n\n    def _optimize_equijoin(\n        self,\n        op_config: dict[str, Any],\n        left_name: str,\n        right_name: str,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        run_operation: Callable[\n            [dict[str, Any], list[dict[str, Any]]], list[dict[str, Any]]\n        ],\n    ) -&gt; tuple[list[dict[str, Any]], dict[str, list[dict[str, Any]]], str, str]:\n        \"\"\"\n        Optimizes an equijoin operation by analyzing join conditions and potentially inserting\n        map operations to improve join efficiency. Returns the optimized configuration and updated data.\n        \"\"\"\n        max_iterations = 2\n        new_left_name = left_name\n        new_right_name = right_name\n        new_steps = []\n        for _ in range(max_iterations):\n            join_optimizer = JoinOptimizer(\n                self.runner,\n                op_config,\n                target_recall=self.runner.config.get(\"optimizer_config\", {})\n                .get(\"equijoin\", {})\n                .get(\"target_recall\", 0.95),\n                estimated_selectivity=self.runner.config.get(\"optimizer_config\", {})\n                .get(\"equijoin\", {})\n                .get(\"estimated_selectivity\", None),\n            )\n            optimized_config, cost, agent_results = join_optimizer.optimize_equijoin(\n                left_data, right_data\n            )\n            self.runner.total_cost += cost\n            # Update the operation config with the optimized values\n            op_config.update(optimized_config)\n\n            if not agent_results.get(\"optimize_map\", False):\n                break  # Exit the loop if no more map optimizations are necessary\n\n            # Update the status to indicate we're optimizing a map operation\n            output_key = agent_results[\"output_key\"]\n            if self.runner.status:\n                self.runner.status.update(\n                    f\"Optimizing map operation for {output_key} extraction to help with the equijoin\"\n                )\n            map_prompt = agent_results[\"map_prompt\"]\n            dataset_to_transform = (\n                left_data\n                if agent_results[\"dataset_to_transform\"] == \"left\"\n                else right_data\n            )\n\n            # Create a new step for the map operation\n            map_operation = {\n                \"name\": f\"synthesized_{output_key}_extraction\",\n                \"type\": \"map\",\n                \"prompt\": map_prompt,\n                \"model\": self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                \"output\": {\"schema\": {output_key: \"string\"}},\n                \"optimize\": False,\n            }\n\n            # Optimize the map operation\n            if map_operation[\"optimize\"]:\n                dataset_to_transform_sample = (\n                    random.sample(dataset_to_transform, self.sample_size_map.get(\"map\"))\n                    if self.config.get(\"optimizer_config\", {}).get(\n                        \"random_sample\", False\n                    )\n                    else dataset_to_transform[: self.sample_size_map.get(\"map\")]\n                )\n                optimized_map_operations = self._optimize_map(\n                    map_operation, dataset_to_transform_sample\n                )\n            else:\n                optimized_map_operations = [map_operation]\n\n            new_step = {\n                \"name\": f\"synthesized_{output_key}_extraction\",\n                \"input\": (\n                    left_name\n                    if agent_results[\"dataset_to_transform\"] == \"left\"\n                    else right_name\n                ),\n                \"operations\": [mo[\"name\"] for mo in optimized_map_operations],\n            }\n            if agent_results[\"dataset_to_transform\"] == \"left\":\n                new_left_name = new_step[\"name\"]\n            else:\n                new_right_name = new_step[\"name\"]\n\n            new_steps.append((new_step[\"name\"], new_step, optimized_map_operations))\n\n            # Now run the optimized map operation on the entire dataset_to_transform\n            for op in optimized_map_operations:\n                dataset_to_transform = run_operation(op, dataset_to_transform)\n\n            # Update the appropriate dataset for the next iteration\n            if agent_results[\"dataset_to_transform\"] == \"left\":\n                left_data = dataset_to_transform\n            else:\n                right_data = dataset_to_transform\n\n            if self.runner.status:\n                self.runner.status.update(\n                    f\"Optimizing equijoin operation with {output_key} extraction\"\n                )\n\n        return op_config, new_steps, new_left_name, new_right_name\n\n    def checkpoint_optimized_ops(self) -&gt; None:\n        \"\"\"\n        Generates the clean config and saves it to the self.optimized_ops_path\n        This is used to resume optimization from a previous run\n        \"\"\"\n        clean_config = self.clean_optimized_config()\n        with open(self.optimized_ops_path, \"w\") as f:\n            yaml.safe_dump(clean_config, f, default_flow_style=False, width=80)\n\n    # Recursively resolve all anchors and aliases\n    @staticmethod\n    def resolve_anchors(data):\n        \"\"\"\n        Recursively resolve all anchors and aliases in a nested data structure.\n\n        This static method traverses through dictionaries and lists, resolving any YAML anchors and aliases.\n\n        Args:\n            data: The data structure to resolve. Can be a dictionary, list, or any other type.\n\n        Returns:\n            The resolved data structure with all anchors and aliases replaced by their actual values.\n        \"\"\"\n        if isinstance(data, dict):\n            return {k: Optimizer.resolve_anchors(v) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [Optimizer.resolve_anchors(item) for item in data]\n        else:\n            return data\n\n    def clean_optimized_config(self) -&gt; dict:\n        \"\"\"\n        Creates a clean YAML configuration from the optimized operation containers,\n        removing internal fields and organizing operations into proper pipeline steps.\n        \"\"\"\n        if not self.runner.last_op_container:\n            return self.config\n\n        # Create a clean copy of the config\n        datasets = {}\n        for dataset_name, dataset_config in self.config.get(\"datasets\", {}).items():\n            if dataset_config[\"type\"] == \"memory\":\n                dataset_config_copy = copy.deepcopy(dataset_config)\n                dataset_config_copy[\"path\"] = \"in-memory data\"\n                datasets[dataset_name] = dataset_config_copy\n            else:\n                datasets[dataset_name] = dataset_config\n\n        clean_config = {\n            \"datasets\": datasets,\n            \"operations\": [],\n            \"pipeline\": self.runner.config.get(\n                \"pipeline\", {}\n            ).copy(),  # Copy entire pipeline config\n        }\n\n        # Reset steps to regenerate\n        clean_config[\"pipeline\"][\"steps\"] = []\n\n        # Keep track of operations we've seen to avoid duplicates\n        seen_operations = set()\n\n        def clean_operation(op_container: OpContainer) -&gt; dict:\n            \"\"\"Remove internal fields from operation config\"\"\"\n            op_config = op_container.config\n            clean_op = copy.deepcopy(op_config)\n\n            clean_op.pop(\"_intermediates\", None)\n\n            # If op has already been optimized, remove the recursively_optimize and optimize fields\n            if op_container.is_optimized:\n                for field in [\"recursively_optimize\", \"optimize\"]:\n                    clean_op.pop(field, None)\n\n            return clean_op\n\n        def process_container(container, current_step=None):\n            \"\"\"Process an operation container and its dependencies\"\"\"\n            # Skip step boundaries\n            if isinstance(container, StepBoundary):\n                if container.children:\n                    return process_container(container.children[0], current_step)\n                return None, None\n\n            # Get step name from container name\n            step_name = container.name.split(\"/\")[0]\n\n            # If this is a new step, create it\n            if not current_step or current_step[\"name\"] != step_name:\n                current_step = {\"name\": step_name, \"operations\": []}\n                clean_config[\"pipeline\"][\"steps\"].insert(0, current_step)\n\n            # Skip scan operations but process their dependencies\n            if container.config[\"type\"] == \"scan\":\n                if container.children:\n                    return process_container(container.children[0], current_step)\n                return None, current_step\n\n            # Handle equijoin operations\n            if container.is_equijoin:\n                # Add operation to list if not seen\n                if container.name not in seen_operations:\n                    op_config = clean_operation(container)\n                    clean_config[\"operations\"].append(op_config)\n                    seen_operations.add(container.name)\n\n                # Add to step operations with left and right inputs\n                current_step[\"operations\"].insert(\n                    0,\n                    {\n                        container.config[\"name\"]: {\n                            \"left\": container.kwargs[\"left_name\"],\n                            \"right\": container.kwargs[\"right_name\"],\n                        }\n                    },\n                )\n\n                # Process both children\n                if container.children:\n                    process_container(container.children[0], current_step)\n                    process_container(container.children[1], current_step)\n            else:\n                # Add operation to list if not seen\n                if container.name not in seen_operations:\n                    op_config = clean_operation(container)\n                    clean_config[\"operations\"].append(op_config)\n                    seen_operations.add(container.name)\n\n                # Add to step operations\n                current_step[\"operations\"].insert(0, container.config[\"name\"])\n\n                # Process children\n                if container.children:\n                    for child in container.children:\n                        process_container(child, current_step)\n\n            return container, current_step\n\n        # Start processing from the last container\n        process_container(self.runner.last_op_container)\n\n        # Add inputs to steps based on their first operation\n        for step in clean_config[\"pipeline\"][\"steps\"]:\n            first_op = step[\"operations\"][0]\n            if isinstance(first_op, dict):  # This is an equijoin\n                continue  # Equijoin steps don't need an input field\n            elif len(step[\"operations\"]) &gt; 0:\n                # Find the first non-scan operation's input by looking at its dependencies\n                op_container = self.runner.op_container_map.get(\n                    f\"{step['name']}/{first_op}\"\n                )\n                if op_container and op_container.children:\n                    child = op_container.children[0]\n                    while (\n                        child\n                        and child.config[\"type\"] == \"step_boundary\"\n                        and child.children\n                    ):\n                        child = child.children[0]\n                    if child and child.config[\"type\"] == \"scan\":\n                        step[\"input\"] = child.config[\"dataset_name\"]\n\n        # Preserve all other config key-value pairs from original config\n        for key, value in self.config.items():\n            if key not in [\"datasets\", \"operations\", \"pipeline\"]:\n                clean_config[key] = value\n\n        return clean_config\n\n    def save_optimized_config(self, optimized_config_path: str):\n        \"\"\"\n        Saves the optimized configuration to a YAML file after resolving all references\n        and cleaning up internal optimization artifacts.\n        \"\"\"\n        resolved_config = self.clean_optimized_config()\n\n        with open(optimized_config_path, \"w\") as f:\n            yaml.safe_dump(resolved_config, f, default_flow_style=False, width=80)\n            self.console.log(\n                f\"[green italic]\ud83d\udcbe Optimized config saved to {optimized_config_path}[/green italic]\"\n            )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.__init__","title":"<code>__init__(runner, rewrite_agent_model='gpt-5.1', judge_agent_model='gpt-4o-mini', litellm_kwargs={}, resume=False, timeout=60)</code>","text":"<p>Initialize the optimizer with a runner instance and configuration. Sets up optimization parameters, caching, and cost tracking.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <code>model</code> <code>str</code> <p>The name of the language model to use. Defaults to \"gpt-5.1\".</p> required <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous run. Defaults to False.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds for operations. Defaults to 60.</p> <code>60</code> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Stores the loaded configuration from the YAML file.</p> <code>console</code> <code>Console</code> <p>Rich console for formatted output.</p> <code>max_threads</code> <code>int</code> <p>Maximum number of threads for parallel processing.</p> <code>base_name</code> <code>str</code> <p>Base name used for file paths.</p> <code>yaml_file_suffix</code> <code>str</code> <p>Suffix for YAML configuration files.</p> <code>runner</code> <code>DSLRunner</code> <p>The DSL runner instance.</p> <code>status</code> <code>DSLRunner</code> <p>Status tracking for the runner.</p> <code>optimized_config</code> <code>Dict</code> <p>A copy of the original config to be optimized.</p> <code>llm_client</code> <code>LLMClient</code> <p>Client for interacting with the language model.</p> <code>timeout</code> <code>int</code> <p>Timeout for operations in seconds.</p> <code>resume</code> <code>bool</code> <p>Whether to resume from previous optimization.</p> <code>captured_output</code> <code>CapturedOutput</code> <p>Captures output during optimization.</p> <code>sample_cache</code> <code>Dict</code> <p>Maps operation names to tuples of (output_data, sample_size).</p> <code>optimized_ops_path</code> <code>str</code> <p>Path to store optimized operations.</p> <code>sample_size_map</code> <code>Dict</code> <p>Maps operation types to sample sizes.</p> <p>The method also calls print_optimizer_config() to display the initial configuration.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def __init__(\n    self,\n    runner: \"DSLRunner\",\n    rewrite_agent_model: str = \"gpt-5.1\",\n    judge_agent_model: str = \"gpt-4o-mini\",\n    litellm_kwargs: dict[str, Any] = {},\n    resume: bool = False,\n    timeout: int = 60,\n):\n    \"\"\"\n    Initialize the optimizer with a runner instance and configuration.\n    Sets up optimization parameters, caching, and cost tracking.\n\n    Args:\n        yaml_file (str): Path to the YAML configuration file.\n        model (str): The name of the language model to use. Defaults to \"gpt-5.1\".\n        resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n        timeout (int): Timeout in seconds for operations. Defaults to 60.\n\n    Attributes:\n        config (Dict): Stores the loaded configuration from the YAML file.\n        console (Console): Rich console for formatted output.\n        max_threads (int): Maximum number of threads for parallel processing.\n        base_name (str): Base name used for file paths.\n        yaml_file_suffix (str): Suffix for YAML configuration files.\n        runner (DSLRunner): The DSL runner instance.\n        status: Status tracking for the runner.\n        optimized_config (Dict): A copy of the original config to be optimized.\n        llm_client (LLMClient): Client for interacting with the language model.\n        timeout (int): Timeout for operations in seconds.\n        resume (bool): Whether to resume from previous optimization.\n        captured_output (CapturedOutput): Captures output during optimization.\n        sample_cache (Dict): Maps operation names to tuples of (output_data, sample_size).\n        optimized_ops_path (str): Path to store optimized operations.\n        sample_size_map (Dict): Maps operation types to sample sizes.\n\n    The method also calls print_optimizer_config() to display the initial configuration.\n    \"\"\"\n    self.config = runner.config\n    self.console = runner.console\n    self.max_threads = runner.max_threads\n\n    self.base_name = runner.base_name\n    self.yaml_file_suffix = runner.yaml_file_suffix\n    self.runner = runner\n    self.status = runner.status\n\n    self.optimized_config = copy.deepcopy(self.config)\n\n    # Get the rate limits from the optimizer config\n    rate_limits = self.config.get(\"optimizer_config\", {}).get(\"rate_limits\", {})\n\n    self.llm_client = LLMClient(\n        runner,\n        rewrite_agent_model,\n        judge_agent_model,\n        rate_limits,\n        **litellm_kwargs,\n    )\n    self.timeout = timeout\n    self.resume = resume\n    self.captured_output = CapturedOutput()\n\n    # Add sample cache for build operations\n    self.sample_cache = {}  # Maps operation names to (output_data, sample_size)\n\n    home_dir = os.environ.get(\"DOCETL_HOME_DIR\", os.path.expanduser(\"~\"))\n    cache_dir = os.path.join(home_dir, f\".docetl/cache/{runner.yaml_file_suffix}\")\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Hash the config to create a unique identifier\n    config_hash = hashlib.sha256(str(self.config).encode()).hexdigest()\n    self.optimized_ops_path = f\"{cache_dir}/{config_hash}.yaml\"\n\n    # Update sample size map\n    self.sample_size_map = SAMPLE_SIZE_MAP\n    if self.config.get(\"optimizer_config\", {}).get(\"sample_sizes\", {}):\n        self.sample_size_map.update(self.config[\"optimizer_config\"][\"sample_sizes\"])\n\n    if not self.runner._from_df_accessors:\n        self.print_optimizer_config()\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.checkpoint_optimized_ops","title":"<code>checkpoint_optimized_ops()</code>","text":"<p>Generates the clean config and saves it to the self.optimized_ops_path This is used to resume optimization from a previous run</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def checkpoint_optimized_ops(self) -&gt; None:\n    \"\"\"\n    Generates the clean config and saves it to the self.optimized_ops_path\n    This is used to resume optimization from a previous run\n    \"\"\"\n    clean_config = self.clean_optimized_config()\n    with open(self.optimized_ops_path, \"w\") as f:\n        yaml.safe_dump(clean_config, f, default_flow_style=False, width=80)\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.clean_optimized_config","title":"<code>clean_optimized_config()</code>","text":"<p>Creates a clean YAML configuration from the optimized operation containers, removing internal fields and organizing operations into proper pipeline steps.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def clean_optimized_config(self) -&gt; dict:\n    \"\"\"\n    Creates a clean YAML configuration from the optimized operation containers,\n    removing internal fields and organizing operations into proper pipeline steps.\n    \"\"\"\n    if not self.runner.last_op_container:\n        return self.config\n\n    # Create a clean copy of the config\n    datasets = {}\n    for dataset_name, dataset_config in self.config.get(\"datasets\", {}).items():\n        if dataset_config[\"type\"] == \"memory\":\n            dataset_config_copy = copy.deepcopy(dataset_config)\n            dataset_config_copy[\"path\"] = \"in-memory data\"\n            datasets[dataset_name] = dataset_config_copy\n        else:\n            datasets[dataset_name] = dataset_config\n\n    clean_config = {\n        \"datasets\": datasets,\n        \"operations\": [],\n        \"pipeline\": self.runner.config.get(\n            \"pipeline\", {}\n        ).copy(),  # Copy entire pipeline config\n    }\n\n    # Reset steps to regenerate\n    clean_config[\"pipeline\"][\"steps\"] = []\n\n    # Keep track of operations we've seen to avoid duplicates\n    seen_operations = set()\n\n    def clean_operation(op_container: OpContainer) -&gt; dict:\n        \"\"\"Remove internal fields from operation config\"\"\"\n        op_config = op_container.config\n        clean_op = copy.deepcopy(op_config)\n\n        clean_op.pop(\"_intermediates\", None)\n\n        # If op has already been optimized, remove the recursively_optimize and optimize fields\n        if op_container.is_optimized:\n            for field in [\"recursively_optimize\", \"optimize\"]:\n                clean_op.pop(field, None)\n\n        return clean_op\n\n    def process_container(container, current_step=None):\n        \"\"\"Process an operation container and its dependencies\"\"\"\n        # Skip step boundaries\n        if isinstance(container, StepBoundary):\n            if container.children:\n                return process_container(container.children[0], current_step)\n            return None, None\n\n        # Get step name from container name\n        step_name = container.name.split(\"/\")[0]\n\n        # If this is a new step, create it\n        if not current_step or current_step[\"name\"] != step_name:\n            current_step = {\"name\": step_name, \"operations\": []}\n            clean_config[\"pipeline\"][\"steps\"].insert(0, current_step)\n\n        # Skip scan operations but process their dependencies\n        if container.config[\"type\"] == \"scan\":\n            if container.children:\n                return process_container(container.children[0], current_step)\n            return None, current_step\n\n        # Handle equijoin operations\n        if container.is_equijoin:\n            # Add operation to list if not seen\n            if container.name not in seen_operations:\n                op_config = clean_operation(container)\n                clean_config[\"operations\"].append(op_config)\n                seen_operations.add(container.name)\n\n            # Add to step operations with left and right inputs\n            current_step[\"operations\"].insert(\n                0,\n                {\n                    container.config[\"name\"]: {\n                        \"left\": container.kwargs[\"left_name\"],\n                        \"right\": container.kwargs[\"right_name\"],\n                    }\n                },\n            )\n\n            # Process both children\n            if container.children:\n                process_container(container.children[0], current_step)\n                process_container(container.children[1], current_step)\n        else:\n            # Add operation to list if not seen\n            if container.name not in seen_operations:\n                op_config = clean_operation(container)\n                clean_config[\"operations\"].append(op_config)\n                seen_operations.add(container.name)\n\n            # Add to step operations\n            current_step[\"operations\"].insert(0, container.config[\"name\"])\n\n            # Process children\n            if container.children:\n                for child in container.children:\n                    process_container(child, current_step)\n\n        return container, current_step\n\n    # Start processing from the last container\n    process_container(self.runner.last_op_container)\n\n    # Add inputs to steps based on their first operation\n    for step in clean_config[\"pipeline\"][\"steps\"]:\n        first_op = step[\"operations\"][0]\n        if isinstance(first_op, dict):  # This is an equijoin\n            continue  # Equijoin steps don't need an input field\n        elif len(step[\"operations\"]) &gt; 0:\n            # Find the first non-scan operation's input by looking at its dependencies\n            op_container = self.runner.op_container_map.get(\n                f\"{step['name']}/{first_op}\"\n            )\n            if op_container and op_container.children:\n                child = op_container.children[0]\n                while (\n                    child\n                    and child.config[\"type\"] == \"step_boundary\"\n                    and child.children\n                ):\n                    child = child.children[0]\n                if child and child.config[\"type\"] == \"scan\":\n                    step[\"input\"] = child.config[\"dataset_name\"]\n\n    # Preserve all other config key-value pairs from original config\n    for key, value in self.config.items():\n        if key not in [\"datasets\", \"operations\", \"pipeline\"]:\n            clean_config[key] = value\n\n    return clean_config\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.optimize","title":"<code>optimize()</code>","text":"<p>Optimizes the entire pipeline by walking the operation DAG and applying operation-specific optimizers where marked. Returns the total optimization cost.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def optimize(self) -&gt; float:\n    \"\"\"\n    Optimizes the entire pipeline by walking the operation DAG and applying\n    operation-specific optimizers where marked. Returns the total optimization cost.\n    \"\"\"\n    self.console.rule(\"[bold cyan]Beginning Pipeline Rewrites[/bold cyan]\")\n\n    # If self.resume is True and there's a checkpoint, load it\n    if self.resume:\n        if os.path.exists(self.optimized_ops_path):\n            # Load the yaml and change the runner with it\n            with open(self.optimized_ops_path, \"r\") as f:\n                partial_optimized_config = yaml.safe_load(f)\n                self.console.log(\n                    \"[yellow]Loading partially optimized pipeline from checkpoint...[/yellow]\"\n                )\n                self.runner._build_operation_graph(partial_optimized_config)\n        else:\n            self.console.log(\n                \"[yellow]No checkpoint found, starting optimization from scratch...[/yellow]\"\n            )\n\n    else:\n        self._insert_empty_resolve_operations()\n\n    # Start with the last operation container and visit each child\n    self.runner.last_op_container.optimize()\n\n    flush_cache(self.console)\n\n    # Print the query plan\n    self.console.rule(\"[bold cyan]Optimized Query Plan[/bold cyan]\")\n    self.runner.print_query_plan()\n\n    return self.llm_client.total_cost\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.print_optimizer_config","title":"<code>print_optimizer_config()</code>","text":"<p>Print the current configuration of the optimizer.</p> <p>This method uses the Rich console to display a formatted output of the optimizer's configuration. It includes details such as the YAML file path, sample sizes for different operation types, maximum number of threads, the language model being used, and the timeout setting.</p> <p>The output is color-coded and formatted for easy readability, with a header and separator lines to clearly delineate the configuration information.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def print_optimizer_config(self):\n    \"\"\"\n    Print the current configuration of the optimizer.\n\n    This method uses the Rich console to display a formatted output of the optimizer's\n    configuration. It includes details such as the YAML file path, sample sizes for\n    different operation types, maximum number of threads, the language model being used,\n    and the timeout setting.\n\n    The output is color-coded and formatted for easy readability, with a header and\n    separator lines to clearly delineate the configuration information.\n    \"\"\"\n    self.console.log(\n        Panel.fit(\n            \"[bold cyan]Optimizer Configuration[/bold cyan]\\n\"\n            f\"[yellow]Sample Size:[/yellow] {self.sample_size_map}\\n\"\n            f\"[yellow]Max Threads:[/yellow] {self.max_threads}\\n\"\n            f\"[yellow]Rewrite Agent Model:[/yellow] {self.llm_client.rewrite_agent_model}\\n\"\n            f\"[yellow]Judge Agent Model:[/yellow] {self.llm_client.judge_agent_model}\\n\"\n            f\"[yellow]Rate Limits:[/yellow] {self.config.get('optimizer_config', {}).get('rate_limits', {})}\\n\",\n            title=\"Optimizer Configuration\",\n        )\n    )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.resolve_anchors","title":"<code>resolve_anchors(data)</code>  <code>staticmethod</code>","text":"<p>Recursively resolve all anchors and aliases in a nested data structure.</p> <p>This static method traverses through dictionaries and lists, resolving any YAML anchors and aliases.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The data structure to resolve. Can be a dictionary, list, or any other type.</p> required <p>Returns:</p> Type Description <p>The resolved data structure with all anchors and aliases replaced by their actual values.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>@staticmethod\ndef resolve_anchors(data):\n    \"\"\"\n    Recursively resolve all anchors and aliases in a nested data structure.\n\n    This static method traverses through dictionaries and lists, resolving any YAML anchors and aliases.\n\n    Args:\n        data: The data structure to resolve. Can be a dictionary, list, or any other type.\n\n    Returns:\n        The resolved data structure with all anchors and aliases replaced by their actual values.\n    \"\"\"\n    if isinstance(data, dict):\n        return {k: Optimizer.resolve_anchors(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [Optimizer.resolve_anchors(item) for item in data]\n    else:\n        return data\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.save_optimized_config","title":"<code>save_optimized_config(optimized_config_path)</code>","text":"<p>Saves the optimized configuration to a YAML file after resolving all references and cleaning up internal optimization artifacts.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def save_optimized_config(self, optimized_config_path: str):\n    \"\"\"\n    Saves the optimized configuration to a YAML file after resolving all references\n    and cleaning up internal optimization artifacts.\n    \"\"\"\n    resolved_config = self.clean_optimized_config()\n\n    with open(optimized_config_path, \"w\") as f:\n        yaml.safe_dump(resolved_config, f, default_flow_style=False, width=80)\n        self.console.log(\n            f\"[green italic]\ud83d\udcbe Optimized config saved to {optimized_config_path}[/green italic]\"\n        )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.should_optimize","title":"<code>should_optimize(step_name, op_name)</code>","text":"<p>Analyzes whether an operation should be optimized by running it on a sample of input data and evaluating potential optimizations. Returns the optimization suggestion and relevant data.</p> Source code in <code>docetl/optimizer.py</code> <pre><code>def should_optimize(\n    self, step_name: str, op_name: str\n) -&gt; tuple[str, list[dict[str, Any]], list[dict[str, Any]], float]:\n    \"\"\"\n    Analyzes whether an operation should be optimized by running it on a sample of input data\n    and evaluating potential optimizations. Returns the optimization suggestion and relevant data.\n    \"\"\"\n    self.console.rule(\"[bold cyan]Beginning Pipeline Assessment[/bold cyan]\")\n\n    self._insert_empty_resolve_operations()\n\n    node_of_interest = self.runner.op_container_map[f\"{step_name}/{op_name}\"]\n\n    # Run the node_of_interest's children\n    input_data = []\n    for child in node_of_interest.children:\n        input_data.append(\n            child.next(\n                is_build=True,\n                sample_size_needed=SAMPLE_SIZE_MAP.get(child.config[\"type\"]),\n            )[0]\n        )\n\n    # Set the step\n    self.captured_output.set_step(step_name)\n\n    # Determine whether we should optimize the node_of_interest\n    if (\n        node_of_interest.config.get(\"type\") == \"map\"\n        or node_of_interest.config.get(\"type\") == \"filter\"\n    ):\n        # Create instance of map optimizer\n        map_optimizer = MapOptimizer(\n            self.runner,\n            self.runner._run_operation,\n            is_filter=node_of_interest.config.get(\"type\") == \"filter\",\n        )\n        should_optimize_output, input_data, output_data = (\n            map_optimizer.should_optimize(node_of_interest.config, input_data[0])\n        )\n    elif node_of_interest.config.get(\"type\") == \"reduce\":\n        reduce_optimizer = ReduceOptimizer(\n            self.runner,\n            self.runner._run_operation,\n        )\n        should_optimize_output, input_data, output_data = (\n            reduce_optimizer.should_optimize(node_of_interest.config, input_data[0])\n        )\n    elif node_of_interest.config.get(\"type\") == \"resolve\":\n        resolve_optimizer = JoinOptimizer(\n            self.runner,\n            node_of_interest.config,\n            target_recall=self.config.get(\"optimizer_config\", {})\n            .get(\"resolve\", {})\n            .get(\"target_recall\", 0.95),\n        )\n        _, should_optimize_output = resolve_optimizer.should_optimize(input_data[0])\n\n        # if should_optimize_output is empty, then we should move to the reduce operation\n        if should_optimize_output == \"\":\n            return \"\", [], [], 0.0\n    else:\n        return \"\", [], [], 0.0\n\n    # Return the string and operation cost\n    return (\n        should_optimize_output,\n        input_data,\n        output_data,\n        self.runner.total_cost + self.llm_client.total_cost,\n    )\n</code></pre>"},{"location":"api-reference/operations/","title":"LLM-Powered Operators","text":""},{"location":"api-reference/operations/#docetl.operations.map.MapOperation","title":"<code>docetl.operations.map.MapOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/map.py</code> <pre><code>class MapOperation(BaseOperation):\n    class schema(BaseOperation.schema):\n        type: str = \"map\"\n        output: dict[str, Any] | None = None\n        prompt: str | None = None\n        model: str | None = None\n        optimize: bool | None = None\n        recursively_optimize: bool | None = None\n        sample_size: int | None = None\n        tools: list[dict[str, Any]] | None = (\n            None  # FIXME: Why isn't this using the Tool data class so validation works automatically?\n        )\n        validation_rules: list[str] | None = Field(None, alias=\"validate\")\n        num_retries_on_validate_failure: int | None = None\n        drop_keys: list[str] | None = None\n        timeout: int | None = None\n        enable_observability: bool = False\n        batch_size: int | None = None\n        clustering_method: str | None = None\n        batch_prompt: str | None = None\n        litellm_completion_kwargs: dict[str, Any] = {}\n        pdf_url_key: str | None = None\n        flush_partial_result: bool = False\n        limit: int | None = Field(None, gt=0)\n        # Calibration parameters\n        calibrate: bool = False\n        num_calibration_docs: int = Field(10, gt=0)\n\n        @field_validator(\"batch_prompt\")\n        def validate_batch_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    # We'll mark it for later processing\n                    return v\n                try:\n                    template = Template(v)\n                    # Test render with a minimal inputs list to validate template\n                    template.render(inputs=[{}])\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'batch_prompt' or missing required 'inputs' variable: {str(e)}\"\n                    ) from e\n            return v\n\n        @field_validator(\"prompt\")\n        def validate_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    # We'll mark it for later processing\n                    return v\n                try:\n                    Template(v)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'prompt': {str(e)}\"\n                    ) from e\n            return v\n\n        @field_validator(\"tools\")\n        def validate_tools(cls, v):\n            if v is not None:\n                for tool in v:\n                    try:\n                        tool_obj = Tool(**tool)\n                    except Exception:\n                        raise TypeError(\"Tool must be a dictionary\")\n\n                    if not (tool_obj.code and tool_obj.function):\n                        raise ValueError(\n                            \"Tool is missing required 'code' or 'function' key\"\n                        )\n\n                    if not isinstance(tool_obj.function, ToolFunction):\n                        raise TypeError(\"'function' in tool must be a dictionary\")\n\n                    for key in [\"name\", \"description\", \"parameters\"]:\n                        if not getattr(tool_obj.function, key):\n                            raise ValueError(\n                                f\"Tool is missing required '{key}' in 'function'\"\n                            )\n            return v\n\n        @model_validator(mode=\"after\")\n        def validate_prompt_and_output_requirements(self):\n            # If drop_keys is not specified, both prompt and output must be present\n            if not self.drop_keys:\n                if not self.prompt or not self.output:\n                    raise ValueError(\n                        \"If 'drop_keys' is not specified, both 'prompt' and 'output' must be present in the configuration\"\n                    )\n\n                if self.output and not self.output.get(\"schema\"):\n                    raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n            return self\n\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.max_batch_size: int = self.config.get(\n            \"max_batch_size\", kwargs.get(\"max_batch_size\", None)\n        )\n        self.clustering_method = \"random\"\n        # Check for non-Jinja prompts and prompt user for confirmation\n        if \"prompt\" in self.config and not has_jinja_syntax(self.config[\"prompt\"]):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"prompt\"], self.config[\"name\"], \"prompt\"\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your prompt.\"\n                )\n            # Mark that we need to append document statement\n            self.config[\"_append_document_to_prompt\"] = True\n        if \"batch_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"batch_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"batch_prompt\"], self.config[\"name\"], \"batch_prompt\"\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your batch_prompt.\"\n                )\n            # Mark that we need to append document statement\n            self.config[\"_append_document_to_batch_prompt\"] = True\n\n    def _limit_applies_to_inputs(self) -&gt; bool:\n        return True\n\n    def _handle_result(self, result: dict[str, Any]) -&gt; tuple[dict | None, bool]:\n        return result, True\n\n    def _generate_calibration_context(self, input_data: list[dict]) -&gt; str:\n        \"\"\"\n        Generate calibration context by running the operation on a sample of documents\n        and using an LLM to suggest prompt improvements for consistency.\n\n        Returns:\n            str: Additional context to add to the original prompt\n        \"\"\"\n        import random\n\n        # Set seed for reproducibility\n        random.seed(42)\n\n        # Sample documents for calibration\n        num_calibration_docs = min(\n            self.config.get(\"num_calibration_docs\", 10), len(input_data)\n        )\n        if num_calibration_docs == len(input_data):\n            calibration_sample = input_data\n        else:\n            calibration_sample = random.sample(input_data, num_calibration_docs)\n\n        self.console.log(\n            f\"[bold blue]Running calibration on {num_calibration_docs} documents...[/bold blue]\"\n        )\n\n        # Temporarily disable calibration to avoid infinite recursion\n        original_calibrate = self.config.get(\"calibrate\", False)\n        self.config[\"calibrate\"] = False\n\n        try:\n            # Run the map operation on the calibration sample\n            calibration_results, _ = self.execute(calibration_sample)\n\n            # Prepare the calibration analysis prompt\n            calibration_prompt = f\"\"\"\nThe following prompt was applied to sample documents to generate these input-output pairs:\n\n\"{self.config[\"prompt\"]}\"\n\nSample inputs and their outputs:\n\"\"\"\n\n            for i, (input_doc, output_doc) in enumerate(\n                zip(calibration_sample, calibration_results)\n            ):\n                calibration_prompt += f\"\\n--- Example {i+1} ---\\n\"\n                calibration_prompt += f\"Input: {input_doc}\\n\"\n                calibration_prompt += f\"Output: {output_doc}\\n\"\n\n            calibration_prompt += \"\"\"\nBased on these examples, provide reference anchors that will be appended to the prompt to help maintain consistency when processing all documents.\n\nDO NOT provide generic advice. Instead, use specific examples from above as calibration points.\nNote that the outputs might be incorrect, because the user's prompt was not calibrated or rich in the first place.\nYou can ignore the outputs if they are incorrect, and focus on the diversity of the inputs.\n\nFormat as concrete reference points:\n- \"For reference, consider '[specific input text]' \u2192 [output] as a baseline for [category/level]\"\n- \"Documents similar to '[specific input text]' should be classified as [output]\"\n\nReference anchors:\"\"\"\n\n            # Call LLM to get calibration suggestions\n            messages = [{\"role\": \"user\", \"content\": calibration_prompt}]\n            # Use a copy of the user-provided completion kwargs so we don't mutate the original\n            # and avoid hard-coding temperature to a value that may not be supported by certain models.\n            completion_kwargs = dict(self.config.get(\"litellm_completion_kwargs\", {}))\n            # If the user did not explicitly specify a temperature, let the model default handle it\n            # to prevent incompatibility errors with providers that don't support 0.0.\n            # If a temperature is already provided, respect the user's choice.\n\n            llm_result = self.runner.api.call_llm(\n                self.config.get(\"model\", self.default_model),\n                \"calibration\",\n                messages,\n                {\"calibration_context\": \"string\"},\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                litellm_completion_kwargs=completion_kwargs,\n                op_config=self.config,\n            )\n\n            # Parse the response\n            if hasattr(llm_result, \"response\"):\n                calibration_context = self.runner.api.parse_llm_response(\n                    llm_result.response,\n                    schema={\"calibration_context\": \"string\"},\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0].get(\"calibration_context\", \"\")\n            else:\n                calibration_context = \"\"\n\n            return calibration_context\n\n        finally:\n            # Restore original calibration setting\n            self.config[\"calibrate\"] = original_calibrate\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the map operation on the provided input data.\n\n        Args:\n            input_data (list[dict]): The input data to process.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n        This method performs the following steps:\n        1. If calibration is enabled, runs calibration to improve prompt consistency\n        2. If a prompt is specified, it processes each input item using the specified prompt and LLM model\n        3. Applies gleaning if configured\n        4. Validates the output\n        5. If drop_keys is specified, it drops the specified keys from each document\n        6. Aggregates results and calculates total cost\n\n        The method uses parallel processing to improve performance.\n        \"\"\"\n        limit_value = self.config.get(\"limit\")\n\n        # Check if there's no prompt and only drop_keys\n        if \"prompt\" not in self.config and \"drop_keys\" in self.config:\n            data_to_process = input_data\n            if limit_value is not None and self._limit_applies_to_inputs():\n                data_to_process = input_data[:limit_value]\n            # If only drop_keys is specified, simply drop the keys and return\n            dropped_results = []\n            for item in data_to_process:\n                new_item = {\n                    k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n                }\n                dropped_results.append(new_item)\n                if limit_value is not None and len(dropped_results) &gt;= limit_value:\n                    break\n            return dropped_results, 0.0  # Return the modified data with no cost\n\n        if limit_value is not None and self._limit_applies_to_inputs():\n            input_data = input_data[:limit_value]\n\n        # Generate calibration context if enabled\n        calibration_context = \"\"\n        if self.config.get(\"calibrate\", False) and \"prompt\" in self.config:\n            calibration_context = self._generate_calibration_context(input_data)\n            if calibration_context:\n                # Store original prompt for potential restoration\n                self._original_prompt = self.config[\"prompt\"]\n                # Augment the prompt with calibration context\n                self.config[\"prompt\"] = (\n                    f\"{self.config['prompt']}\\n\\n{calibration_context}\"\n                )\n                self.console.log(\n                    f\"[bold green]New map ({self.config['name']}) prompt augmented with context on how to improve consistency:[/bold green] {self.config['prompt']}\"\n                )\n            else:\n                self.console.log(\n                    f\"[bold yellow]Extra context on how to improve consistency failed to generate for map ({self.config['name']}); continuing with prompt as is.[/bold yellow]\"\n                )\n\n        if self.status:\n            self.status.stop()\n\n        def _process_map_item(\n            item: dict, initial_result: dict | None = None\n        ) -&gt; tuple[dict | None, float]:\n\n            # Build retrieval context (if configured)\n            retrieval_context = self._maybe_build_retrieval_context({\"input\": item})\n            ctx = {\"input\": item, \"retrieval_context\": retrieval_context}\n            rendered = strict_render(self.config[\"prompt\"], ctx)\n            # If template didn't use retrieval_context, prepend a standard header\n            prompt = (\n                f\"Here is some extra context:\\n{retrieval_context}\\n\\n{rendered}\"\n                if retrieval_context\n                and \"retrieval_context\" not in self.config[\"prompt\"]\n                else rendered\n            )\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            if self.config.get(\"pdf_url_key\", None):\n                # Append the pdf to the prompt\n                try:\n                    pdf_url = item[self.config[\"pdf_url_key\"]]\n                except KeyError:\n                    raise ValueError(\n                        f\"PDF URL key '{self.config['pdf_url_key']}' not found in input data\"\n                    )\n\n                # Download content\n                if pdf_url.startswith(\"http\"):\n                    file_data = requests.get(pdf_url).content\n                else:\n                    with open(pdf_url, \"rb\") as f:\n                        file_data = f.read()\n                encoded_file = base64.b64encode(file_data).decode(\"utf-8\")\n                base64_url = f\"data:application/pdf;base64,{encoded_file}\"\n\n                messages[0][\"content\"] = [\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": base64_url}},\n                    {\"type\": \"text\", \"text\": prompt},\n                ]\n\n            def validation_fn(response: dict[str, Any] | ModelResponse):\n                structured_mode = (\n                    self.config.get(\"output\", {}).get(\"mode\")\n                    == OutputMode.STRUCTURED_OUTPUT.value\n                )\n                output = (\n                    self.runner.api.parse_llm_response(\n                        response,\n                        schema=self.config[\"output\"][\"schema\"],\n                        tools=self.config.get(\"tools\", None),\n                        manually_fix_errors=self.manually_fix_errors,\n                        use_structured_output=structured_mode,\n                    )[0]\n                    if isinstance(response, ModelResponse)\n                    else response\n                )\n                # Type-check output values against schema declarations\n                is_types_valid, _errors = validate_output_types(\n                    output,\n                    self.config[\"output\"][\"schema\"],\n                )\n                if not is_types_valid:\n                    return output, False\n\n                for key, value in item.items():\n                    if key not in self.config[\"output\"][\"schema\"]:\n                        output[key] = value\n                if self.runner.api.validate_output(self.config, output, self.console):\n                    return output, True\n                return output, False\n\n            if self.runner.is_cancelled:\n                raise asyncio.CancelledError(\"Operation was cancelled\")\n            llm_result = self.runner.api.call_llm(\n                self.config.get(\"model\", self.default_model),\n                \"map\",\n                messages,\n                self.config[\"output\"][\"schema\"],\n                tools=self.config.get(\"tools\", None),\n                scratchpad=None,\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                validation_config=(\n                    {\n                        \"num_retries\": self.num_retries_on_validate_failure,\n                        \"val_rule\": self.config.get(\"validate\", []),\n                        \"validation_fn\": validation_fn,\n                    }\n                ),\n                gleaning_config=self.config.get(\"gleaning\", None),\n                verbose=self.config.get(\"verbose\", False),\n                bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                initial_result=initial_result,\n                litellm_completion_kwargs=self.config.get(\n                    \"litellm_completion_kwargs\", {}\n                ),\n                op_config=self.config,\n            )\n\n            if llm_result.validated:\n                # Parse the response\n                if isinstance(llm_result.response, ModelResponse):\n                    structured_mode = (\n                        self.config.get(\"output\", {}).get(\"mode\")\n                        == OutputMode.STRUCTURED_OUTPUT.value\n                    )\n                    outputs = self.runner.api.parse_llm_response(\n                        llm_result.response,\n                        schema=self.config[\"output\"][\"schema\"],\n                        tools=self.config.get(\"tools\", None),\n                        manually_fix_errors=self.manually_fix_errors,\n                        use_structured_output=structured_mode,\n                    )\n                else:\n                    outputs = [llm_result.response]\n\n                # Augment the output with the original item\n                outputs = [{**item, **output} for output in outputs]\n                if self.config.get(\"enable_observability\", False):\n                    for output in outputs:\n                        output[f\"_observability_{self.config['name']}\"] = {\n                            \"prompt\": prompt\n                        }\n                # Add retrieved context if save_retriever_output is enabled\n                if self.config.get(\"save_retriever_output\", False):\n                    for output in outputs:\n                        output[f\"_{self.config['name']}_retrieved_context\"] = (\n                            retrieval_context if retrieval_context else \"\"\n                        )\n                return outputs, llm_result.total_cost\n\n            return None, llm_result.total_cost\n\n        # If there's a batch prompt, let's use that\n        def _process_map_batch(items: list[dict]) -&gt; tuple[list[dict], float]:\n            total_cost = 0\n            if len(items) &gt; 1 and self.config.get(\"batch_prompt\", None):\n                # Raise error if pdf_url_key is set\n                if self.config.get(\"pdf_url_key\", None):\n                    raise ValueError(\"Batch prompts do not support PDF URLs\")\n\n                batch_prompt = strict_render(\n                    self.config[\"batch_prompt\"], {\"inputs\": items}\n                )\n\n                # Issue the batch call\n                llm_result = self.runner.api.call_llm_batch(\n                    self.config.get(\"model\", self.default_model),\n                    \"batch map\",\n                    [{\"role\": \"user\", \"content\": batch_prompt}],\n                    self.config[\"output\"][\"schema\"],\n                    verbose=self.config.get(\"verbose\", False),\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                    bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                    litellm_completion_kwargs=self.config.get(\n                        \"litellm_completion_kwargs\", {}\n                    ),\n                )\n                total_cost += llm_result.total_cost\n\n                # Parse the LLM response\n                structured_mode = (\n                    self.config.get(\"output\", {}).get(\"mode\")\n                    == OutputMode.STRUCTURED_OUTPUT.value\n                )\n                parsed_output = self.runner.api.parse_llm_response(\n                    llm_result.response,\n                    self.config[\"output\"][\"schema\"],\n                    use_structured_output=structured_mode,\n                )[0].get(\"results\", [])\n                items_and_outputs = [\n                    (item, parsed_output[idx] if idx &lt; len(parsed_output) else None)\n                    for idx, item in enumerate(items)\n                ]\n            else:\n                items_and_outputs = [(item, None) for item in items]\n\n            # Run _process_map_item for each item\n            all_results = []\n            if len(items_and_outputs) &gt; 1:\n                with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\n                    futures = [\n                        executor.submit(\n                            _process_map_item,\n                            items_and_outputs[i][0],\n                            items_and_outputs[i][1],\n                        )\n                        for i in range(len(items_and_outputs))\n                    ]\n                    for i in range(len(futures)):\n                        try:\n                            results, item_cost = futures[i].result()\n                            if results is not None:\n                                all_results.extend(results)\n                            total_cost += item_cost\n                        except Exception as e:\n                            if self.config.get(\"skip_on_error\", False):\n                                self.console.log(\n                                    f\"[bold red]Error in map operation {self.config['name']}, skipping item:[/bold red] {e}\"\n                                )\n                                continue\n                            else:\n                                raise e\n            else:\n                try:\n                    results, item_cost = _process_map_item(\n                        items_and_outputs[0][0], items_and_outputs[0][1]\n                    )\n                    if results is not None:\n                        all_results.extend(results)\n                    total_cost += item_cost\n                except Exception as e:\n                    if self.config.get(\"skip_on_error\", False):\n                        self.console.log(\n                            f\"[bold red]Error in map operation {self.config['name']}, skipping item:[/bold red] {e}\"\n                        )\n                    else:\n                        raise e\n\n            return all_results, total_cost\n\n        limit_counter = 0\n        batch_size = self.max_batch_size if self.max_batch_size is not None else 1\n        total_batches = (len(input_data) + batch_size - 1) // batch_size\n        if total_batches == 0:\n            if self.status:\n                self.status.start()\n            return [], 0.0\n\n        worker_limit = self.max_batch_size or self.max_threads or 1\n        window_size = (\n            total_batches\n            if limit_value is None\n            else max(1, (limit_value + batch_size - 1) // batch_size)\n        )\n\n        results: list[dict] = []\n        total_cost = 0.0\n        limit_reached = False\n        op_name = self.config[\"name\"]\n\n        if limit_value is not None and not self._limit_applies_to_inputs():\n            self.console.log(\n                f\"[yellow]Note: Operation will terminate early once {limit_value} items pass the filter condition.[/yellow]\"\n            )\n\n        with ThreadPoolExecutor(max_workers=worker_limit) as executor:\n            with RichLoopBar(\n                total=total_batches,\n                desc=f\"Processing {op_name} (map) on all documents\",\n                console=self.console,\n            ) as pbar:\n                chunk_start = 0\n                while chunk_start &lt; total_batches and not limit_reached:\n                    chunk_end = min(total_batches, chunk_start + window_size)\n                    chunk_ordinals = list(range(chunk_start, chunk_end))\n                    futures = []\n                    for ordinal in chunk_ordinals:\n                        start_idx = ordinal * batch_size\n                        batch = input_data[start_idx : start_idx + batch_size]\n                        futures.append(executor.submit(_process_map_batch, batch))\n\n                    for relative_idx, future in enumerate(futures):\n                        if limit_value is not None and limit_counter &gt;= limit_value:\n                            limit_reached = True\n                            break\n\n                        result_list, item_cost = future.result()\n                        total_cost += item_cost\n\n                        if result_list:\n                            if \"drop_keys\" in self.config:\n                                result_list = [\n                                    {\n                                        k: v\n                                        for k, v in result.items()\n                                        if k not in self.config[\"drop_keys\"]\n                                    }\n                                    for result in result_list\n                                ]\n\n                            if self.config.get(\"flush_partial_results\", False):\n                                self.runner._flush_partial_results(\n                                    op_name, chunk_ordinals[relative_idx], result_list\n                                )\n\n                            for result in result_list:\n                                processed_result, counts_towards_limit = (\n                                    self._handle_result(result)\n                                )\n                                if processed_result is not None:\n                                    results.append(processed_result)\n\n                                if limit_value is not None and counts_towards_limit:\n                                    limit_counter += 1\n                                    if limit_counter &gt;= limit_value:\n                                        limit_reached = True\n                                        break\n\n                        pbar.update()\n\n                    chunk_start = chunk_end\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.MapOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the map operation on the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.</p> <p>This method performs the following steps: 1. If calibration is enabled, runs calibration to improve prompt consistency 2. If a prompt is specified, it processes each input item using the specified prompt and LLM model 3. Applies gleaning if configured 4. Validates the output 5. If drop_keys is specified, it drops the specified keys from each document 6. Aggregates results and calculates total cost</p> <p>The method uses parallel processing to improve performance.</p> Source code in <code>docetl/operations/map.py</code> <pre><code>def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the map operation on the provided input data.\n\n    Args:\n        input_data (list[dict]): The input data to process.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n    This method performs the following steps:\n    1. If calibration is enabled, runs calibration to improve prompt consistency\n    2. If a prompt is specified, it processes each input item using the specified prompt and LLM model\n    3. Applies gleaning if configured\n    4. Validates the output\n    5. If drop_keys is specified, it drops the specified keys from each document\n    6. Aggregates results and calculates total cost\n\n    The method uses parallel processing to improve performance.\n    \"\"\"\n    limit_value = self.config.get(\"limit\")\n\n    # Check if there's no prompt and only drop_keys\n    if \"prompt\" not in self.config and \"drop_keys\" in self.config:\n        data_to_process = input_data\n        if limit_value is not None and self._limit_applies_to_inputs():\n            data_to_process = input_data[:limit_value]\n        # If only drop_keys is specified, simply drop the keys and return\n        dropped_results = []\n        for item in data_to_process:\n            new_item = {\n                k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n            }\n            dropped_results.append(new_item)\n            if limit_value is not None and len(dropped_results) &gt;= limit_value:\n                break\n        return dropped_results, 0.0  # Return the modified data with no cost\n\n    if limit_value is not None and self._limit_applies_to_inputs():\n        input_data = input_data[:limit_value]\n\n    # Generate calibration context if enabled\n    calibration_context = \"\"\n    if self.config.get(\"calibrate\", False) and \"prompt\" in self.config:\n        calibration_context = self._generate_calibration_context(input_data)\n        if calibration_context:\n            # Store original prompt for potential restoration\n            self._original_prompt = self.config[\"prompt\"]\n            # Augment the prompt with calibration context\n            self.config[\"prompt\"] = (\n                f\"{self.config['prompt']}\\n\\n{calibration_context}\"\n            )\n            self.console.log(\n                f\"[bold green]New map ({self.config['name']}) prompt augmented with context on how to improve consistency:[/bold green] {self.config['prompt']}\"\n            )\n        else:\n            self.console.log(\n                f\"[bold yellow]Extra context on how to improve consistency failed to generate for map ({self.config['name']}); continuing with prompt as is.[/bold yellow]\"\n            )\n\n    if self.status:\n        self.status.stop()\n\n    def _process_map_item(\n        item: dict, initial_result: dict | None = None\n    ) -&gt; tuple[dict | None, float]:\n\n        # Build retrieval context (if configured)\n        retrieval_context = self._maybe_build_retrieval_context({\"input\": item})\n        ctx = {\"input\": item, \"retrieval_context\": retrieval_context}\n        rendered = strict_render(self.config[\"prompt\"], ctx)\n        # If template didn't use retrieval_context, prepend a standard header\n        prompt = (\n            f\"Here is some extra context:\\n{retrieval_context}\\n\\n{rendered}\"\n            if retrieval_context\n            and \"retrieval_context\" not in self.config[\"prompt\"]\n            else rendered\n        )\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        if self.config.get(\"pdf_url_key\", None):\n            # Append the pdf to the prompt\n            try:\n                pdf_url = item[self.config[\"pdf_url_key\"]]\n            except KeyError:\n                raise ValueError(\n                    f\"PDF URL key '{self.config['pdf_url_key']}' not found in input data\"\n                )\n\n            # Download content\n            if pdf_url.startswith(\"http\"):\n                file_data = requests.get(pdf_url).content\n            else:\n                with open(pdf_url, \"rb\") as f:\n                    file_data = f.read()\n            encoded_file = base64.b64encode(file_data).decode(\"utf-8\")\n            base64_url = f\"data:application/pdf;base64,{encoded_file}\"\n\n            messages[0][\"content\"] = [\n                {\"type\": \"image_url\", \"image_url\": {\"url\": base64_url}},\n                {\"type\": \"text\", \"text\": prompt},\n            ]\n\n        def validation_fn(response: dict[str, Any] | ModelResponse):\n            structured_mode = (\n                self.config.get(\"output\", {}).get(\"mode\")\n                == OutputMode.STRUCTURED_OUTPUT.value\n            )\n            output = (\n                self.runner.api.parse_llm_response(\n                    response,\n                    schema=self.config[\"output\"][\"schema\"],\n                    tools=self.config.get(\"tools\", None),\n                    manually_fix_errors=self.manually_fix_errors,\n                    use_structured_output=structured_mode,\n                )[0]\n                if isinstance(response, ModelResponse)\n                else response\n            )\n            # Type-check output values against schema declarations\n            is_types_valid, _errors = validate_output_types(\n                output,\n                self.config[\"output\"][\"schema\"],\n            )\n            if not is_types_valid:\n                return output, False\n\n            for key, value in item.items():\n                if key not in self.config[\"output\"][\"schema\"]:\n                    output[key] = value\n            if self.runner.api.validate_output(self.config, output, self.console):\n                return output, True\n            return output, False\n\n        if self.runner.is_cancelled:\n            raise asyncio.CancelledError(\"Operation was cancelled\")\n        llm_result = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"map\",\n            messages,\n            self.config[\"output\"][\"schema\"],\n            tools=self.config.get(\"tools\", None),\n            scratchpad=None,\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": validation_fn,\n                }\n            ),\n            gleaning_config=self.config.get(\"gleaning\", None),\n            verbose=self.config.get(\"verbose\", False),\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            initial_result=initial_result,\n            litellm_completion_kwargs=self.config.get(\n                \"litellm_completion_kwargs\", {}\n            ),\n            op_config=self.config,\n        )\n\n        if llm_result.validated:\n            # Parse the response\n            if isinstance(llm_result.response, ModelResponse):\n                structured_mode = (\n                    self.config.get(\"output\", {}).get(\"mode\")\n                    == OutputMode.STRUCTURED_OUTPUT.value\n                )\n                outputs = self.runner.api.parse_llm_response(\n                    llm_result.response,\n                    schema=self.config[\"output\"][\"schema\"],\n                    tools=self.config.get(\"tools\", None),\n                    manually_fix_errors=self.manually_fix_errors,\n                    use_structured_output=structured_mode,\n                )\n            else:\n                outputs = [llm_result.response]\n\n            # Augment the output with the original item\n            outputs = [{**item, **output} for output in outputs]\n            if self.config.get(\"enable_observability\", False):\n                for output in outputs:\n                    output[f\"_observability_{self.config['name']}\"] = {\n                        \"prompt\": prompt\n                    }\n            # Add retrieved context if save_retriever_output is enabled\n            if self.config.get(\"save_retriever_output\", False):\n                for output in outputs:\n                    output[f\"_{self.config['name']}_retrieved_context\"] = (\n                        retrieval_context if retrieval_context else \"\"\n                    )\n            return outputs, llm_result.total_cost\n\n        return None, llm_result.total_cost\n\n    # If there's a batch prompt, let's use that\n    def _process_map_batch(items: list[dict]) -&gt; tuple[list[dict], float]:\n        total_cost = 0\n        if len(items) &gt; 1 and self.config.get(\"batch_prompt\", None):\n            # Raise error if pdf_url_key is set\n            if self.config.get(\"pdf_url_key\", None):\n                raise ValueError(\"Batch prompts do not support PDF URLs\")\n\n            batch_prompt = strict_render(\n                self.config[\"batch_prompt\"], {\"inputs\": items}\n            )\n\n            # Issue the batch call\n            llm_result = self.runner.api.call_llm_batch(\n                self.config.get(\"model\", self.default_model),\n                \"batch map\",\n                [{\"role\": \"user\", \"content\": batch_prompt}],\n                self.config[\"output\"][\"schema\"],\n                verbose=self.config.get(\"verbose\", False),\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\n                    \"max_retries_per_timeout\", 2\n                ),\n                bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                litellm_completion_kwargs=self.config.get(\n                    \"litellm_completion_kwargs\", {}\n                ),\n            )\n            total_cost += llm_result.total_cost\n\n            # Parse the LLM response\n            structured_mode = (\n                self.config.get(\"output\", {}).get(\"mode\")\n                == OutputMode.STRUCTURED_OUTPUT.value\n            )\n            parsed_output = self.runner.api.parse_llm_response(\n                llm_result.response,\n                self.config[\"output\"][\"schema\"],\n                use_structured_output=structured_mode,\n            )[0].get(\"results\", [])\n            items_and_outputs = [\n                (item, parsed_output[idx] if idx &lt; len(parsed_output) else None)\n                for idx, item in enumerate(items)\n            ]\n        else:\n            items_and_outputs = [(item, None) for item in items]\n\n        # Run _process_map_item for each item\n        all_results = []\n        if len(items_and_outputs) &gt; 1:\n            with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\n                futures = [\n                    executor.submit(\n                        _process_map_item,\n                        items_and_outputs[i][0],\n                        items_and_outputs[i][1],\n                    )\n                    for i in range(len(items_and_outputs))\n                ]\n                for i in range(len(futures)):\n                    try:\n                        results, item_cost = futures[i].result()\n                        if results is not None:\n                            all_results.extend(results)\n                        total_cost += item_cost\n                    except Exception as e:\n                        if self.config.get(\"skip_on_error\", False):\n                            self.console.log(\n                                f\"[bold red]Error in map operation {self.config['name']}, skipping item:[/bold red] {e}\"\n                            )\n                            continue\n                        else:\n                            raise e\n        else:\n            try:\n                results, item_cost = _process_map_item(\n                    items_and_outputs[0][0], items_and_outputs[0][1]\n                )\n                if results is not None:\n                    all_results.extend(results)\n                total_cost += item_cost\n            except Exception as e:\n                if self.config.get(\"skip_on_error\", False):\n                    self.console.log(\n                        f\"[bold red]Error in map operation {self.config['name']}, skipping item:[/bold red] {e}\"\n                    )\n                else:\n                    raise e\n\n        return all_results, total_cost\n\n    limit_counter = 0\n    batch_size = self.max_batch_size if self.max_batch_size is not None else 1\n    total_batches = (len(input_data) + batch_size - 1) // batch_size\n    if total_batches == 0:\n        if self.status:\n            self.status.start()\n        return [], 0.0\n\n    worker_limit = self.max_batch_size or self.max_threads or 1\n    window_size = (\n        total_batches\n        if limit_value is None\n        else max(1, (limit_value + batch_size - 1) // batch_size)\n    )\n\n    results: list[dict] = []\n    total_cost = 0.0\n    limit_reached = False\n    op_name = self.config[\"name\"]\n\n    if limit_value is not None and not self._limit_applies_to_inputs():\n        self.console.log(\n            f\"[yellow]Note: Operation will terminate early once {limit_value} items pass the filter condition.[/yellow]\"\n        )\n\n    with ThreadPoolExecutor(max_workers=worker_limit) as executor:\n        with RichLoopBar(\n            total=total_batches,\n            desc=f\"Processing {op_name} (map) on all documents\",\n            console=self.console,\n        ) as pbar:\n            chunk_start = 0\n            while chunk_start &lt; total_batches and not limit_reached:\n                chunk_end = min(total_batches, chunk_start + window_size)\n                chunk_ordinals = list(range(chunk_start, chunk_end))\n                futures = []\n                for ordinal in chunk_ordinals:\n                    start_idx = ordinal * batch_size\n                    batch = input_data[start_idx : start_idx + batch_size]\n                    futures.append(executor.submit(_process_map_batch, batch))\n\n                for relative_idx, future in enumerate(futures):\n                    if limit_value is not None and limit_counter &gt;= limit_value:\n                        limit_reached = True\n                        break\n\n                    result_list, item_cost = future.result()\n                    total_cost += item_cost\n\n                    if result_list:\n                        if \"drop_keys\" in self.config:\n                            result_list = [\n                                {\n                                    k: v\n                                    for k, v in result.items()\n                                    if k not in self.config[\"drop_keys\"]\n                                }\n                                for result in result_list\n                            ]\n\n                        if self.config.get(\"flush_partial_results\", False):\n                            self.runner._flush_partial_results(\n                                op_name, chunk_ordinals[relative_idx], result_list\n                            )\n\n                        for result in result_list:\n                            processed_result, counts_towards_limit = (\n                                self._handle_result(result)\n                            )\n                            if processed_result is not None:\n                                results.append(processed_result)\n\n                            if limit_value is not None and counts_towards_limit:\n                                limit_counter += 1\n                                if limit_counter &gt;= limit_value:\n                                    limit_reached = True\n                                    break\n\n                    pbar.update()\n\n                chunk_start = chunk_end\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation","title":"<code>docetl.operations.resolve.ResolveOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>class ResolveOperation(BaseOperation):\n    class schema(BaseOperation.schema):\n        type: str = \"resolve\"\n        comparison_prompt: str\n        resolution_prompt: str | None = None\n        output: dict[str, Any] | None = None\n        embedding_model: str | None = None\n        resolution_model: str | None = None\n        comparison_model: str | None = None\n        blocking_keys: list[str] | None = None\n        blocking_threshold: float | None = Field(None, ge=0, le=1)\n        blocking_target_recall: float | None = Field(None, ge=0, le=1)\n        blocking_conditions: list[str] | None = None\n        input: dict[str, Any] | None = None\n        embedding_batch_size: int | None = Field(None, gt=0)\n        compare_batch_size: int | None = Field(None, gt=0)\n        limit_comparisons: int | None = Field(None, gt=0)\n        optimize: bool | None = None\n        timeout: int | None = Field(None, gt=0)\n        litellm_completion_kwargs: dict[str, Any] = Field(default_factory=dict)\n        enable_observability: bool = False\n\n        @field_validator(\"comparison_prompt\")\n        def validate_comparison_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    return v\n                try:\n                    comparison_template = Template(v)\n                    comparison_vars = comparison_template.environment.parse(v).find_all(\n                        jinja2.nodes.Name\n                    )\n                    comparison_var_names = {var.name for var in comparison_vars}\n                    if (\n                        \"input1\" not in comparison_var_names\n                        or \"input2\" not in comparison_var_names\n                    ):\n                        raise ValueError(\n                            f\"'comparison_prompt' must contain both 'input1' and 'input2' variables. {v}\"\n                        )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'comparison_prompt': {str(e)}\"\n                    )\n            return v\n\n        @field_validator(\"resolution_prompt\")\n        def validate_resolution_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    return v\n                try:\n                    reduction_template = Template(v)\n                    reduction_vars = reduction_template.environment.parse(v).find_all(\n                        jinja2.nodes.Name\n                    )\n                    reduction_var_names = {var.name for var in reduction_vars}\n                    if \"inputs\" not in reduction_var_names:\n                        raise ValueError(\n                            \"'resolution_prompt' must contain 'inputs' variable\"\n                        )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'resolution_prompt': {str(e)}\"\n                    )\n            return v\n\n        @field_validator(\"input\")\n        def validate_input_schema(cls, v):\n            if v is not None:\n                if \"schema\" not in v:\n                    raise ValueError(\"Missing 'schema' in 'input' configuration\")\n                if not isinstance(v[\"schema\"], dict):\n                    raise TypeError(\n                        \"'schema' in 'input' configuration must be a dictionary\"\n                    )\n            return v\n\n        @model_validator(mode=\"after\")\n        def validate_output_schema(self, info: ValidationInfo):\n            # Skip validation if we're using from dataframe accessors\n            if isinstance(info.context, dict) and info.context.get(\n                \"_from_df_accessors\"\n            ):\n                return self\n\n            if self.output is None:\n                raise ValueError(\n                    \"Missing required key 'output' in ResolveOperation configuration\"\n                )\n\n            if \"schema\" not in self.output:\n                raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n            if not isinstance(self.output[\"schema\"], dict):\n                raise TypeError(\n                    \"'schema' in 'output' configuration must be a dictionary\"\n                )\n\n            if not self.output[\"schema\"]:\n                raise ValueError(\"'schema' in 'output' configuration cannot be empty\")\n\n            return self\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Check for non-Jinja prompts and prompt user for confirmation\n        if \"comparison_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"comparison_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"comparison_prompt\"],\n                self.config[\"name\"],\n                \"comparison_prompt\",\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your comparison_prompt.\"\n                )\n            # Mark that we need to append document statement\n            # Note: comparison_prompt uses input1 and input2, so we'll handle it specially in strict_render\n            self.config[\"_append_document_to_comparison_prompt\"] = True\n        if \"resolution_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"resolution_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"resolution_prompt\"],\n                self.config[\"name\"],\n                \"resolution_prompt\",\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your resolution_prompt.\"\n                )\n            # Mark that we need to append document statement (resolution uses inputs)\n            self.config[\"_append_document_to_resolution_prompt\"] = True\n            self.config[\"_is_reduce_operation\"] = True\n\n    def compare_pair(\n        self,\n        comparison_prompt: str,\n        model: str,\n        item1: dict,\n        item2: dict,\n        blocking_keys: list[str] = [],\n        timeout_seconds: int = 120,\n        max_retries_per_timeout: int = 2,\n    ) -&gt; tuple[bool, float, str]:\n        \"\"\"\n        Compares two items using an LLM model to determine if they match.\n\n        Args:\n            comparison_prompt (str): The prompt template for comparison.\n            model (str): The LLM model to use for comparison.\n            item1 (dict): The first item to compare.\n            item2 (dict): The second item to compare.\n\n        Returns:\n            tuple[bool, float, str]: A tuple containing a boolean indicating whether the items match, the cost of the comparison, and the prompt.\n        \"\"\"\n        if blocking_keys:\n            if all(\n                key in item1\n                and key in item2\n                and str(item1[key]).lower() == str(item2[key]).lower()\n                for key in blocking_keys\n            ):\n                return True, 0, \"\"\n\n        prompt = strict_render(comparison_prompt, {\"input1\": item1, \"input2\": item2})\n        response = self.runner.api.call_llm(\n            model,\n            \"compare\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            {\"is_match\": \"bool\"},\n            timeout_seconds=timeout_seconds,\n            max_retries_per_timeout=max_retries_per_timeout,\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n            op_config=self.config,\n        )\n        output = self.runner.api.parse_llm_response(\n            response.response,\n            {\"is_match\": \"bool\"},\n        )[0]\n\n        return output[\"is_match\"], response.total_cost, prompt\n\n    def syntax_check(self) -&gt; None:\n        context = {\"_from_df_accessors\": self.runner._from_df_accessors}\n        super().syntax_check(context)\n\n    def validation_fn(self, response: dict[str, Any]):\n        output = self.runner.api.parse_llm_response(\n            response,\n            schema=self.config[\"output\"][\"schema\"],\n        )[0]\n        if self.runner.api.validate_output(self.config, output, self.console):\n            return output, True\n        return output, False\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the resolve operation on the provided dataset.\n\n        Args:\n            input_data (list[dict]): The dataset to resolve.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the resolved results and the total cost of the operation.\n\n        This method performs the following steps:\n        1. Initial blocking based on specified conditions and/or embedding similarity\n        2. Pairwise comparison of potentially matching entries using LLM\n        3. Clustering of matched entries\n        4. Resolution of each cluster into a single entry (if applicable)\n        5. Result aggregation and validation\n\n        The method also calculates and logs statistics such as comparisons saved by blocking and self-join selectivity.\n        \"\"\"\n        if len(input_data) == 0:\n            return [], 0\n\n        # Initialize observability data for all items at the start\n        if self.config.get(\"enable_observability\", False):\n            observability_key = f\"_observability_{self.config['name']}\"\n            for item in input_data:\n                if observability_key not in item:\n                    item[observability_key] = {\n                        \"comparison_prompts\": [],\n                        \"resolution_prompt\": None,\n                    }\n\n        blocking_keys = self.config.get(\"blocking_keys\", [])\n        blocking_threshold = self.config.get(\"blocking_threshold\")\n        blocking_conditions = self.config.get(\"blocking_conditions\", [])\n        limit_comparisons = self.config.get(\"limit_comparisons\")\n        total_cost = 0\n        if self.status:\n            self.status.stop()\n\n        # Track pre-computed embeddings from auto-optimization\n        precomputed_embeddings = None\n\n        # Auto-compute blocking threshold if no blocking configuration is provided\n        if not blocking_threshold and not blocking_conditions and not limit_comparisons:\n            # Get target recall from operation config (default 0.95)\n            target_recall = self.config.get(\"blocking_target_recall\", 0.95)\n            self.console.log(\n                f\"[yellow]No blocking configuration. Auto-computing threshold (target recall: {target_recall:.0%})...[/yellow]\"\n            )\n            # Determine blocking keys if not set\n            auto_blocking_keys = blocking_keys if blocking_keys else None\n            if not auto_blocking_keys:\n                prompt_template = self.config.get(\"comparison_prompt\", \"\")\n                prompt_vars = extract_jinja_variables(prompt_template)\n                prompt_vars = [\n                    var\n                    for var in prompt_vars\n                    if var not in [\"input\", \"input1\", \"input2\"]\n                ]\n                auto_blocking_keys = list(\n                    set([var.split(\".\")[-1] for var in prompt_vars])\n                )\n            if not auto_blocking_keys:\n                auto_blocking_keys = list(input_data[0].keys())\n            blocking_keys = auto_blocking_keys\n\n            # Create comparison function for threshold optimization\n            def compare_fn_for_optimization(item1, item2):\n                return self.compare_pair(\n                    self.config[\"comparison_prompt\"],\n                    self.config.get(\"comparison_model\", self.default_model),\n                    item1,\n                    item2,\n                    blocking_keys=[],  # Don't use key-based shortcut during optimization\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                )\n\n            # Run threshold optimization\n            optimizer = RuntimeBlockingOptimizer(\n                runner=self.runner,\n                config=self.config,\n                default_model=self.default_model,\n                max_threads=self.max_threads,\n                console=self.console,\n                target_recall=target_recall,\n                sample_size=min(100, len(input_data) * (len(input_data) - 1) // 4),\n            )\n            blocking_threshold, precomputed_embeddings, optimization_cost = (\n                optimizer.optimize_resolve(\n                    input_data,\n                    compare_fn_for_optimization,\n                    blocking_keys=blocking_keys,\n                )\n            )\n            total_cost += optimization_cost\n\n        input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n        if not blocking_keys:\n            # Set them to all keys in the input data\n            blocking_keys = list(input_data[0].keys())\n\n        def is_match(item1: dict[str, Any], item2: dict[str, Any]) -&gt; bool:\n            return any(\n                eval(condition, {\"input1\": item1, \"input2\": item2})\n                for condition in blocking_conditions\n            )\n\n        # Calculate embeddings if blocking_threshold is set\n        embeddings = None\n        if blocking_threshold is not None:\n            # Use precomputed embeddings if available from auto-optimization\n            if precomputed_embeddings is not None:\n                embeddings = precomputed_embeddings\n            else:\n                self.console.log(\n                    f\"[cyan]Creating embeddings for {len(input_data)} items...[/cyan]\"\n                )\n                embedding_model = self.config.get(\n                    \"embedding_model\", \"text-embedding-3-small\"\n                )\n                model_input_context_length = model_cost.get(embedding_model, {}).get(\n                    \"max_input_tokens\", 8192\n                )\n                batch_size = self.config.get(\"embedding_batch_size\", 1000)\n                embeddings = []\n                embedding_cost = 0.0\n                num_batches = (len(input_data) + batch_size - 1) // batch_size\n\n                for batch_idx in range(num_batches):\n                    start_idx = batch_idx * batch_size\n                    end_idx = min(start_idx + batch_size, len(input_data))\n                    batch = input_data[start_idx:end_idx]\n\n                    if num_batches &gt; 1:\n                        self.console.log(\n                            f\"[dim]Creating embeddings: batch {batch_idx + 1}/{num_batches} \"\n                            f\"({end_idx}/{len(input_data)} items)[/dim]\"\n                        )\n\n                    texts = [\n                        \" \".join(\n                            str(item[key]) for key in blocking_keys if key in item\n                        )[: model_input_context_length * 3]\n                        for item in batch\n                    ]\n                    response = self.runner.api.gen_embedding(\n                        model=embedding_model, input=texts\n                    )\n                    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n                    embedding_cost += completion_cost(response)\n\n                total_cost += embedding_cost\n\n        # Build a mapping of blocking key values to indices\n        # This is used later for cluster merging (when two items match, merge all items sharing their key values)\n        value_to_indices: dict[tuple[str, ...], list[int]] = {}\n        for i, item in enumerate(input_data):\n            key = tuple(str(item.get(k, \"\")) for k in blocking_keys)\n            if key not in value_to_indices:\n                value_to_indices[key] = []\n            value_to_indices[key].append(i)\n\n        # Total number of pairs to potentially compare\n        n = len(input_data)\n        total_pairs = n * (n - 1) // 2\n\n        # Apply code-based blocking conditions (check all pairs)\n        code_blocked_pairs = []\n        if blocking_conditions:\n            for i in range(n):\n                for j in range(i + 1, n):\n                    if is_match(input_data[i], input_data[j]):\n                        code_blocked_pairs.append((i, j))\n\n        # Apply cosine similarity blocking if threshold is specified\n        embedding_blocked_pairs = []\n        if blocking_threshold is not None and embeddings is not None:\n            import numpy as np\n            from sklearn.metrics.pairwise import cosine_similarity\n\n            similarity_matrix = cosine_similarity(embeddings)\n            code_blocked_set = set(code_blocked_pairs)\n\n            # Use numpy to efficiently find all pairs above threshold\n            i_indices, j_indices = np.triu_indices(n, k=1)\n            similarities = similarity_matrix[i_indices, j_indices]\n            above_threshold_mask = similarities &gt;= blocking_threshold\n\n            # Get pairs above threshold\n            above_threshold_i = i_indices[above_threshold_mask]\n            above_threshold_j = j_indices[above_threshold_mask]\n\n            # Filter out pairs already in code_blocked_set\n            embedding_blocked_pairs = [\n                (int(i), int(j))\n                for i, j in zip(above_threshold_i, above_threshold_j)\n                if (i, j) not in code_blocked_set\n            ]\n\n        # Combine pairs from both blocking methods\n        all_blocked_pairs = code_blocked_pairs + embedding_blocked_pairs\n\n        # If no blocking was applied, compare all pairs\n        if not blocking_conditions and blocking_threshold is None:\n            all_blocked_pairs = [(i, j) for i in range(n) for j in range(i + 1, n)]\n        # Apply limit_comparisons with prioritization\n        if limit_comparisons is not None and len(all_blocked_pairs) &gt; limit_comparisons:\n            # Prioritize code-based pairs, then sample from embedding pairs if needed\n            if len(code_blocked_pairs) &gt;= limit_comparisons:\n                # If we have enough code-based pairs, just sample from those\n                blocked_pairs = random.sample(code_blocked_pairs, limit_comparisons)\n                self.console.log(\n                    f\"Using {limit_comparisons} code-based pairs (had {len(code_blocked_pairs)} available)\"\n                )\n            else:\n                # Take all code-based pairs + sample from embedding pairs\n                remaining_slots = limit_comparisons - len(code_blocked_pairs)\n                sampled_embedding_pairs = random.sample(\n                    embedding_blocked_pairs,\n                    min(remaining_slots, len(embedding_blocked_pairs)),\n                )\n                blocked_pairs = code_blocked_pairs + sampled_embedding_pairs\n                self.console.log(\n                    f\"Using {len(code_blocked_pairs)} code-based + {len(sampled_embedding_pairs)} embedding-based pairs \"\n                    f\"(total: {len(blocked_pairs)})\"\n                )\n        else:\n            blocked_pairs = all_blocked_pairs\n            if len(code_blocked_pairs) &gt; 0 and len(embedding_blocked_pairs) &gt; 0:\n                self.console.log(\n                    f\"Using all {len(code_blocked_pairs)} code-based + {len(embedding_blocked_pairs)} embedding-based pairs\"\n                )\n\n        # Initialize clusters with all indices\n        clusters = [{i} for i in range(len(input_data))]\n        cluster_map = {i: i for i in range(len(input_data))}\n\n        # Modified merge_clusters to handle all indices with the same value\n\n        def merge_clusters(item1: int, item2: int) -&gt; None:\n            root1, root2 = find_cluster(item1, cluster_map), find_cluster(\n                item2, cluster_map\n            )\n            if root1 != root2:\n                if len(clusters[root1]) &lt; len(clusters[root2]):\n                    root1, root2 = root2, root1\n                clusters[root1] |= clusters[root2]\n                cluster_map[root2] = root1\n                clusters[root2] = set()\n\n                # Also merge all other indices that share the same values\n                key1 = tuple(str(input_data[item1].get(k, \"\")) for k in blocking_keys)\n                key2 = tuple(str(input_data[item2].get(k, \"\")) for k in blocking_keys)\n\n                # Merge all indices with the same values\n                for idx in value_to_indices.get(key1, []):\n                    if idx != item1:\n                        root_idx = find_cluster(idx, cluster_map)\n                        if root_idx != root1:\n                            clusters[root1] |= clusters[root_idx]\n                            cluster_map[root_idx] = root1\n                            clusters[root_idx] = set()\n\n                for idx in value_to_indices.get(key2, []):\n                    if idx != item2:\n                        root_idx = find_cluster(idx, cluster_map)\n                        if root_idx != root1:\n                            clusters[root1] |= clusters[root_idx]\n                            cluster_map[root_idx] = root1\n                            clusters[root_idx] = set()\n\n        # Compute an auto-batch size based on the number of comparisons\n        def auto_batch() -&gt; int:\n            # Maximum batch size limit for 4o-mini model\n            M = 500\n\n            n = len(input_data)\n            m = len(blocked_pairs)\n\n            # https://www.wolframalpha.com/input?i=k%28k-1%29%2F2+%2B+%28n-k%29%28k-1%29+%3D+m%2C+solve+for+k\n            # Two possible solutions for k:\n            # k = -1/2 sqrt((1 - 2n)^2 - 8m) + n + 1/2\n            # k = 1/2 (sqrt((1 - 2n)^2 - 8m) + 2n + 1)\n\n            discriminant = (1 - 2 * n) ** 2 - 8 * m\n            sqrt_discriminant = discriminant**0.5\n\n            k1 = -0.5 * sqrt_discriminant + n + 0.5\n            k2 = 0.5 * (sqrt_discriminant + 2 * n + 1)\n\n            # Take the maximum viable solution\n            k = max(k1, k2)\n            return M if k &lt; 0 else min(int(k), M)\n\n        # Compare pairs and update clusters in real-time\n        batch_size = self.config.get(\"compare_batch_size\", auto_batch())\n\n        # Log blocking summary\n        total_possible_comparisons = len(input_data) * (len(input_data) - 1) // 2\n        self.console.log(\n            f\"Comparing {len(blocked_pairs):,} pairs \"\n            f\"({len(blocked_pairs)/total_possible_comparisons*100:.1f}% of {total_possible_comparisons:,} total, \"\n            f\"batch size: {batch_size})\"\n        )\n        pair_costs = 0\n\n        pbar = RichLoopBar(\n            range(0, len(blocked_pairs), batch_size),\n            desc=f\"Processing batches of {batch_size} LLM comparisons\",\n            console=self.console,\n        )\n        last_processed = 0\n        for i in pbar:\n            batch_end = last_processed + batch_size\n            batch = blocked_pairs[last_processed:batch_end]\n            # Filter pairs for the initial batch\n            better_batch = [\n                pair\n                for pair in batch\n                if find_cluster(pair[0], cluster_map) == pair[0]\n                and find_cluster(pair[1], cluster_map) == pair[1]\n            ]\n\n            # Expand better_batch if it doesn\u2019t reach batch_size\n            while len(better_batch) &lt; batch_size and batch_end &lt; len(blocked_pairs):\n                # Move batch_end forward by batch_size to get more pairs\n                next_end = batch_end + batch_size\n                next_batch = blocked_pairs[batch_end:next_end]\n\n                better_batch.extend(\n                    pair\n                    for pair in next_batch\n                    if find_cluster(pair[0], cluster_map) == pair[0]\n                    and find_cluster(pair[1], cluster_map) == pair[1]\n                )\n\n                # Update batch_end to prevent overlapping in the next loop\n                batch_end = next_end\n            better_batch = better_batch[:batch_size]\n            last_processed = batch_end\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n                future_to_pair = {\n                    executor.submit(\n                        self.compare_pair,\n                        self.config[\"comparison_prompt\"],\n                        self.config.get(\"comparison_model\", self.default_model),\n                        input_data[pair[0]],\n                        input_data[pair[1]],\n                        blocking_keys,\n                        timeout_seconds=self.config.get(\"timeout\", 120),\n                        max_retries_per_timeout=self.config.get(\n                            \"max_retries_per_timeout\", 2\n                        ),\n                    ): pair\n                    for pair in better_batch\n                }\n\n                for future in as_completed(future_to_pair):\n                    pair = future_to_pair[future]\n                    is_match_result, cost, prompt = future.result()\n                    pair_costs += cost\n                    if is_match_result:\n                        merge_clusters(pair[0], pair[1])\n\n                    if self.config.get(\"enable_observability\", False):\n                        observability_key = f\"_observability_{self.config['name']}\"\n                        for idx in (pair[0], pair[1]):\n                            if observability_key not in input_data[idx]:\n                                input_data[idx][observability_key] = {\n                                    \"comparison_prompts\": [],\n                                    \"resolution_prompt\": None,\n                                }\n                            input_data[idx][observability_key][\n                                \"comparison_prompts\"\n                            ].append(prompt)\n\n        total_cost += pair_costs\n\n        # Collect final clusters\n        final_clusters = [cluster for cluster in clusters if cluster]\n\n        # Process each cluster\n        results = []\n\n        def process_cluster(cluster):\n            if len(cluster) &gt; 1:\n                cluster_items = [input_data[i] for i in cluster]\n                if input_schema:\n                    cluster_items = [\n                        {k: item[k] for k in input_schema.keys() if k in item}\n                        for item in cluster_items\n                    ]\n\n                resolution_prompt = strict_render(\n                    self.config[\"resolution_prompt\"], {\"inputs\": cluster_items}\n                )\n                reduction_response = self.runner.api.call_llm(\n                    self.config.get(\"resolution_model\", self.default_model),\n                    \"reduce\",\n                    [{\"role\": \"user\", \"content\": resolution_prompt}],\n                    self.config[\"output\"][\"schema\"],\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                    bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                    validation_config=(\n                        {\n                            \"val_rule\": self.config.get(\"validate\", []),\n                            \"validation_fn\": self.validation_fn,\n                        }\n                        if self.config.get(\"validate\", None)\n                        else None\n                    ),\n                    litellm_completion_kwargs=self.config.get(\n                        \"litellm_completion_kwargs\", {}\n                    ),\n                    op_config=self.config,\n                )\n                reduction_cost = reduction_response.total_cost\n\n                if self.config.get(\"enable_observability\", False):\n                    for item in [input_data[i] for i in cluster]:\n                        observability_key = f\"_observability_{self.config['name']}\"\n                        if observability_key not in item:\n                            item[observability_key] = {\n                                \"comparison_prompts\": [],\n                                \"resolution_prompt\": None,\n                            }\n                        item[observability_key][\"resolution_prompt\"] = resolution_prompt\n\n                if reduction_response.validated:\n                    reduction_output = self.runner.api.parse_llm_response(\n                        reduction_response.response,\n                        self.config[\"output\"][\"schema\"],\n                        manually_fix_errors=self.manually_fix_errors,\n                    )[0]\n\n                    # If the output is overwriting an existing key, we want to save the kv pairs\n                    keys_in_output = [\n                        k\n                        for k in set(reduction_output.keys())\n                        if k in cluster_items[0].keys()\n                    ]\n\n                    return (\n                        [\n                            {\n                                **item,\n                                f\"_kv_pairs_preresolve_{self.config['name']}\": {\n                                    k: item[k] for k in keys_in_output\n                                },\n                                **{\n                                    k: reduction_output[k]\n                                    for k in self.config[\"output\"][\"schema\"]\n                                },\n                            }\n                            for item in [input_data[i] for i in cluster]\n                        ],\n                        reduction_cost,\n                    )\n                return [], reduction_cost\n            else:\n                # Set the output schema to be the keys found in the compare_prompt\n                compare_prompt_keys = extract_jinja_variables(\n                    self.config[\"comparison_prompt\"]\n                )\n                # Get the set of keys in the compare_prompt\n                compare_prompt_keys = set(\n                    [\n                        k.replace(\"input1.\", \"\")\n                        for k in compare_prompt_keys\n                        if \"input1\" in k\n                    ]\n                )\n\n                # For each key in the output schema, find the most similar key in the compare_prompt\n                output_keys = set(self.config[\"output\"][\"schema\"].keys())\n                key_mapping = {}\n                for output_key in output_keys:\n                    best_match = None\n                    best_score = 0\n                    for compare_key in compare_prompt_keys:\n                        score = sum(\n                            c1 == c2 for c1, c2 in zip(output_key, compare_key)\n                        ) / max(len(output_key), len(compare_key))\n                        if score &gt; best_score:\n                            best_score = score\n                            best_match = compare_key\n                    key_mapping[output_key] = best_match\n\n                # Create the result dictionary using the key mapping\n                result = input_data[list(cluster)[0]].copy()\n                result[f\"_kv_pairs_preresolve_{self.config['name']}\"] = {\n                    ok: result[ck] for ok, ck in key_mapping.items() if ck in result\n                }\n                for output_key, compare_key in key_mapping.items():\n                    if compare_key in input_data[list(cluster)[0]]:\n                        result[output_key] = input_data[list(cluster)[0]][compare_key]\n                    elif output_key in input_data[list(cluster)[0]]:\n                        result[output_key] = input_data[list(cluster)[0]][output_key]\n                    else:\n                        result[output_key] = None  # or some default value\n\n                return [result], 0\n\n        # Calculate the number of records before and clusters after\n        num_records_before = len(input_data)\n        num_clusters_after = len(final_clusters)\n        self.console.log(f\"Number of keys before resolution: {num_records_before}\")\n        self.console.log(\n            f\"Number of distinct keys after resolution: {num_clusters_after}\"\n        )\n\n        # If no resolution prompt is provided, we can skip the resolution phase\n        # And simply select the most common value for each key\n        if not self.config.get(\"resolution_prompt\", None):\n            for cluster in final_clusters:\n                if len(cluster) &gt; 1:\n                    for key in self.config[\"output\"][\"keys\"]:\n                        most_common_value = max(\n                            set(input_data[i][key] for i in cluster),\n                            key=lambda x: sum(\n                                1 for i in cluster if input_data[i][key] == x\n                            ),\n                        )\n                        for i in cluster:\n                            input_data[i][key] = most_common_value\n            results = input_data\n        else:\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n                futures = [\n                    executor.submit(process_cluster, cluster)\n                    for cluster in final_clusters\n                ]\n                for future in rich_as_completed(\n                    futures,\n                    total=len(futures),\n                    desc=\"Determining resolved key for each group of equivalent keys\",\n                    console=self.console,\n                ):\n                    cluster_results, cluster_cost = future.result()\n                    results.extend(cluster_results)\n                    total_cost += cluster_cost\n\n        total_pairs = len(input_data) * (len(input_data) - 1) // 2\n        true_match_count = sum(\n            len(cluster) * (len(cluster) - 1) // 2\n            for cluster in final_clusters\n            if len(cluster) &gt; 1\n        )\n        true_match_selectivity = (\n            true_match_count / total_pairs if total_pairs &gt; 0 else 0\n        )\n        self.console.log(f\"Self-join selectivity: {true_match_selectivity:.4f}\")\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation.compare_pair","title":"<code>compare_pair(comparison_prompt, model, item1, item2, blocking_keys=[], timeout_seconds=120, max_retries_per_timeout=2)</code>","text":"<p>Compares two items using an LLM model to determine if they match.</p> <p>Parameters:</p> Name Type Description Default <code>comparison_prompt</code> <code>str</code> <p>The prompt template for comparison.</p> required <code>model</code> <code>str</code> <p>The LLM model to use for comparison.</p> required <code>item1</code> <code>dict</code> <p>The first item to compare.</p> required <code>item2</code> <code>dict</code> <p>The second item to compare.</p> required <p>Returns:</p> Type Description <code>tuple[bool, float, str]</code> <p>tuple[bool, float, str]: A tuple containing a boolean indicating whether the items match, the cost of the comparison, and the prompt.</p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>def compare_pair(\n    self,\n    comparison_prompt: str,\n    model: str,\n    item1: dict,\n    item2: dict,\n    blocking_keys: list[str] = [],\n    timeout_seconds: int = 120,\n    max_retries_per_timeout: int = 2,\n) -&gt; tuple[bool, float, str]:\n    \"\"\"\n    Compares two items using an LLM model to determine if they match.\n\n    Args:\n        comparison_prompt (str): The prompt template for comparison.\n        model (str): The LLM model to use for comparison.\n        item1 (dict): The first item to compare.\n        item2 (dict): The second item to compare.\n\n    Returns:\n        tuple[bool, float, str]: A tuple containing a boolean indicating whether the items match, the cost of the comparison, and the prompt.\n    \"\"\"\n    if blocking_keys:\n        if all(\n            key in item1\n            and key in item2\n            and str(item1[key]).lower() == str(item2[key]).lower()\n            for key in blocking_keys\n        ):\n            return True, 0, \"\"\n\n    prompt = strict_render(comparison_prompt, {\"input1\": item1, \"input2\": item2})\n    response = self.runner.api.call_llm(\n        model,\n        \"compare\",\n        [{\"role\": \"user\", \"content\": prompt}],\n        {\"is_match\": \"bool\"},\n        timeout_seconds=timeout_seconds,\n        max_retries_per_timeout=max_retries_per_timeout,\n        bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n        litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n        op_config=self.config,\n    )\n    output = self.runner.api.parse_llm_response(\n        response.response,\n        {\"is_match\": \"bool\"},\n    )[0]\n\n    return output[\"is_match\"], response.total_cost, prompt\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the resolve operation on the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>The dataset to resolve.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the resolved results and the total cost of the operation.</p> <p>This method performs the following steps: 1. Initial blocking based on specified conditions and/or embedding similarity 2. Pairwise comparison of potentially matching entries using LLM 3. Clustering of matched entries 4. Resolution of each cluster into a single entry (if applicable) 5. Result aggregation and validation</p> <p>The method also calculates and logs statistics such as comparisons saved by blocking and self-join selectivity.</p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the resolve operation on the provided dataset.\n\n    Args:\n        input_data (list[dict]): The dataset to resolve.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the resolved results and the total cost of the operation.\n\n    This method performs the following steps:\n    1. Initial blocking based on specified conditions and/or embedding similarity\n    2. Pairwise comparison of potentially matching entries using LLM\n    3. Clustering of matched entries\n    4. Resolution of each cluster into a single entry (if applicable)\n    5. Result aggregation and validation\n\n    The method also calculates and logs statistics such as comparisons saved by blocking and self-join selectivity.\n    \"\"\"\n    if len(input_data) == 0:\n        return [], 0\n\n    # Initialize observability data for all items at the start\n    if self.config.get(\"enable_observability\", False):\n        observability_key = f\"_observability_{self.config['name']}\"\n        for item in input_data:\n            if observability_key not in item:\n                item[observability_key] = {\n                    \"comparison_prompts\": [],\n                    \"resolution_prompt\": None,\n                }\n\n    blocking_keys = self.config.get(\"blocking_keys\", [])\n    blocking_threshold = self.config.get(\"blocking_threshold\")\n    blocking_conditions = self.config.get(\"blocking_conditions\", [])\n    limit_comparisons = self.config.get(\"limit_comparisons\")\n    total_cost = 0\n    if self.status:\n        self.status.stop()\n\n    # Track pre-computed embeddings from auto-optimization\n    precomputed_embeddings = None\n\n    # Auto-compute blocking threshold if no blocking configuration is provided\n    if not blocking_threshold and not blocking_conditions and not limit_comparisons:\n        # Get target recall from operation config (default 0.95)\n        target_recall = self.config.get(\"blocking_target_recall\", 0.95)\n        self.console.log(\n            f\"[yellow]No blocking configuration. Auto-computing threshold (target recall: {target_recall:.0%})...[/yellow]\"\n        )\n        # Determine blocking keys if not set\n        auto_blocking_keys = blocking_keys if blocking_keys else None\n        if not auto_blocking_keys:\n            prompt_template = self.config.get(\"comparison_prompt\", \"\")\n            prompt_vars = extract_jinja_variables(prompt_template)\n            prompt_vars = [\n                var\n                for var in prompt_vars\n                if var not in [\"input\", \"input1\", \"input2\"]\n            ]\n            auto_blocking_keys = list(\n                set([var.split(\".\")[-1] for var in prompt_vars])\n            )\n        if not auto_blocking_keys:\n            auto_blocking_keys = list(input_data[0].keys())\n        blocking_keys = auto_blocking_keys\n\n        # Create comparison function for threshold optimization\n        def compare_fn_for_optimization(item1, item2):\n            return self.compare_pair(\n                self.config[\"comparison_prompt\"],\n                self.config.get(\"comparison_model\", self.default_model),\n                item1,\n                item2,\n                blocking_keys=[],  # Don't use key-based shortcut during optimization\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\n                    \"max_retries_per_timeout\", 2\n                ),\n            )\n\n        # Run threshold optimization\n        optimizer = RuntimeBlockingOptimizer(\n            runner=self.runner,\n            config=self.config,\n            default_model=self.default_model,\n            max_threads=self.max_threads,\n            console=self.console,\n            target_recall=target_recall,\n            sample_size=min(100, len(input_data) * (len(input_data) - 1) // 4),\n        )\n        blocking_threshold, precomputed_embeddings, optimization_cost = (\n            optimizer.optimize_resolve(\n                input_data,\n                compare_fn_for_optimization,\n                blocking_keys=blocking_keys,\n            )\n        )\n        total_cost += optimization_cost\n\n    input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n    if not blocking_keys:\n        # Set them to all keys in the input data\n        blocking_keys = list(input_data[0].keys())\n\n    def is_match(item1: dict[str, Any], item2: dict[str, Any]) -&gt; bool:\n        return any(\n            eval(condition, {\"input1\": item1, \"input2\": item2})\n            for condition in blocking_conditions\n        )\n\n    # Calculate embeddings if blocking_threshold is set\n    embeddings = None\n    if blocking_threshold is not None:\n        # Use precomputed embeddings if available from auto-optimization\n        if precomputed_embeddings is not None:\n            embeddings = precomputed_embeddings\n        else:\n            self.console.log(\n                f\"[cyan]Creating embeddings for {len(input_data)} items...[/cyan]\"\n            )\n            embedding_model = self.config.get(\n                \"embedding_model\", \"text-embedding-3-small\"\n            )\n            model_input_context_length = model_cost.get(embedding_model, {}).get(\n                \"max_input_tokens\", 8192\n            )\n            batch_size = self.config.get(\"embedding_batch_size\", 1000)\n            embeddings = []\n            embedding_cost = 0.0\n            num_batches = (len(input_data) + batch_size - 1) // batch_size\n\n            for batch_idx in range(num_batches):\n                start_idx = batch_idx * batch_size\n                end_idx = min(start_idx + batch_size, len(input_data))\n                batch = input_data[start_idx:end_idx]\n\n                if num_batches &gt; 1:\n                    self.console.log(\n                        f\"[dim]Creating embeddings: batch {batch_idx + 1}/{num_batches} \"\n                        f\"({end_idx}/{len(input_data)} items)[/dim]\"\n                    )\n\n                texts = [\n                    \" \".join(\n                        str(item[key]) for key in blocking_keys if key in item\n                    )[: model_input_context_length * 3]\n                    for item in batch\n                ]\n                response = self.runner.api.gen_embedding(\n                    model=embedding_model, input=texts\n                )\n                embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n                embedding_cost += completion_cost(response)\n\n            total_cost += embedding_cost\n\n    # Build a mapping of blocking key values to indices\n    # This is used later for cluster merging (when two items match, merge all items sharing their key values)\n    value_to_indices: dict[tuple[str, ...], list[int]] = {}\n    for i, item in enumerate(input_data):\n        key = tuple(str(item.get(k, \"\")) for k in blocking_keys)\n        if key not in value_to_indices:\n            value_to_indices[key] = []\n        value_to_indices[key].append(i)\n\n    # Total number of pairs to potentially compare\n    n = len(input_data)\n    total_pairs = n * (n - 1) // 2\n\n    # Apply code-based blocking conditions (check all pairs)\n    code_blocked_pairs = []\n    if blocking_conditions:\n        for i in range(n):\n            for j in range(i + 1, n):\n                if is_match(input_data[i], input_data[j]):\n                    code_blocked_pairs.append((i, j))\n\n    # Apply cosine similarity blocking if threshold is specified\n    embedding_blocked_pairs = []\n    if blocking_threshold is not None and embeddings is not None:\n        import numpy as np\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarity_matrix = cosine_similarity(embeddings)\n        code_blocked_set = set(code_blocked_pairs)\n\n        # Use numpy to efficiently find all pairs above threshold\n        i_indices, j_indices = np.triu_indices(n, k=1)\n        similarities = similarity_matrix[i_indices, j_indices]\n        above_threshold_mask = similarities &gt;= blocking_threshold\n\n        # Get pairs above threshold\n        above_threshold_i = i_indices[above_threshold_mask]\n        above_threshold_j = j_indices[above_threshold_mask]\n\n        # Filter out pairs already in code_blocked_set\n        embedding_blocked_pairs = [\n            (int(i), int(j))\n            for i, j in zip(above_threshold_i, above_threshold_j)\n            if (i, j) not in code_blocked_set\n        ]\n\n    # Combine pairs from both blocking methods\n    all_blocked_pairs = code_blocked_pairs + embedding_blocked_pairs\n\n    # If no blocking was applied, compare all pairs\n    if not blocking_conditions and blocking_threshold is None:\n        all_blocked_pairs = [(i, j) for i in range(n) for j in range(i + 1, n)]\n    # Apply limit_comparisons with prioritization\n    if limit_comparisons is not None and len(all_blocked_pairs) &gt; limit_comparisons:\n        # Prioritize code-based pairs, then sample from embedding pairs if needed\n        if len(code_blocked_pairs) &gt;= limit_comparisons:\n            # If we have enough code-based pairs, just sample from those\n            blocked_pairs = random.sample(code_blocked_pairs, limit_comparisons)\n            self.console.log(\n                f\"Using {limit_comparisons} code-based pairs (had {len(code_blocked_pairs)} available)\"\n            )\n        else:\n            # Take all code-based pairs + sample from embedding pairs\n            remaining_slots = limit_comparisons - len(code_blocked_pairs)\n            sampled_embedding_pairs = random.sample(\n                embedding_blocked_pairs,\n                min(remaining_slots, len(embedding_blocked_pairs)),\n            )\n            blocked_pairs = code_blocked_pairs + sampled_embedding_pairs\n            self.console.log(\n                f\"Using {len(code_blocked_pairs)} code-based + {len(sampled_embedding_pairs)} embedding-based pairs \"\n                f\"(total: {len(blocked_pairs)})\"\n            )\n    else:\n        blocked_pairs = all_blocked_pairs\n        if len(code_blocked_pairs) &gt; 0 and len(embedding_blocked_pairs) &gt; 0:\n            self.console.log(\n                f\"Using all {len(code_blocked_pairs)} code-based + {len(embedding_blocked_pairs)} embedding-based pairs\"\n            )\n\n    # Initialize clusters with all indices\n    clusters = [{i} for i in range(len(input_data))]\n    cluster_map = {i: i for i in range(len(input_data))}\n\n    # Modified merge_clusters to handle all indices with the same value\n\n    def merge_clusters(item1: int, item2: int) -&gt; None:\n        root1, root2 = find_cluster(item1, cluster_map), find_cluster(\n            item2, cluster_map\n        )\n        if root1 != root2:\n            if len(clusters[root1]) &lt; len(clusters[root2]):\n                root1, root2 = root2, root1\n            clusters[root1] |= clusters[root2]\n            cluster_map[root2] = root1\n            clusters[root2] = set()\n\n            # Also merge all other indices that share the same values\n            key1 = tuple(str(input_data[item1].get(k, \"\")) for k in blocking_keys)\n            key2 = tuple(str(input_data[item2].get(k, \"\")) for k in blocking_keys)\n\n            # Merge all indices with the same values\n            for idx in value_to_indices.get(key1, []):\n                if idx != item1:\n                    root_idx = find_cluster(idx, cluster_map)\n                    if root_idx != root1:\n                        clusters[root1] |= clusters[root_idx]\n                        cluster_map[root_idx] = root1\n                        clusters[root_idx] = set()\n\n            for idx in value_to_indices.get(key2, []):\n                if idx != item2:\n                    root_idx = find_cluster(idx, cluster_map)\n                    if root_idx != root1:\n                        clusters[root1] |= clusters[root_idx]\n                        cluster_map[root_idx] = root1\n                        clusters[root_idx] = set()\n\n    # Compute an auto-batch size based on the number of comparisons\n    def auto_batch() -&gt; int:\n        # Maximum batch size limit for 4o-mini model\n        M = 500\n\n        n = len(input_data)\n        m = len(blocked_pairs)\n\n        # https://www.wolframalpha.com/input?i=k%28k-1%29%2F2+%2B+%28n-k%29%28k-1%29+%3D+m%2C+solve+for+k\n        # Two possible solutions for k:\n        # k = -1/2 sqrt((1 - 2n)^2 - 8m) + n + 1/2\n        # k = 1/2 (sqrt((1 - 2n)^2 - 8m) + 2n + 1)\n\n        discriminant = (1 - 2 * n) ** 2 - 8 * m\n        sqrt_discriminant = discriminant**0.5\n\n        k1 = -0.5 * sqrt_discriminant + n + 0.5\n        k2 = 0.5 * (sqrt_discriminant + 2 * n + 1)\n\n        # Take the maximum viable solution\n        k = max(k1, k2)\n        return M if k &lt; 0 else min(int(k), M)\n\n    # Compare pairs and update clusters in real-time\n    batch_size = self.config.get(\"compare_batch_size\", auto_batch())\n\n    # Log blocking summary\n    total_possible_comparisons = len(input_data) * (len(input_data) - 1) // 2\n    self.console.log(\n        f\"Comparing {len(blocked_pairs):,} pairs \"\n        f\"({len(blocked_pairs)/total_possible_comparisons*100:.1f}% of {total_possible_comparisons:,} total, \"\n        f\"batch size: {batch_size})\"\n    )\n    pair_costs = 0\n\n    pbar = RichLoopBar(\n        range(0, len(blocked_pairs), batch_size),\n        desc=f\"Processing batches of {batch_size} LLM comparisons\",\n        console=self.console,\n    )\n    last_processed = 0\n    for i in pbar:\n        batch_end = last_processed + batch_size\n        batch = blocked_pairs[last_processed:batch_end]\n        # Filter pairs for the initial batch\n        better_batch = [\n            pair\n            for pair in batch\n            if find_cluster(pair[0], cluster_map) == pair[0]\n            and find_cluster(pair[1], cluster_map) == pair[1]\n        ]\n\n        # Expand better_batch if it doesn\u2019t reach batch_size\n        while len(better_batch) &lt; batch_size and batch_end &lt; len(blocked_pairs):\n            # Move batch_end forward by batch_size to get more pairs\n            next_end = batch_end + batch_size\n            next_batch = blocked_pairs[batch_end:next_end]\n\n            better_batch.extend(\n                pair\n                for pair in next_batch\n                if find_cluster(pair[0], cluster_map) == pair[0]\n                and find_cluster(pair[1], cluster_map) == pair[1]\n            )\n\n            # Update batch_end to prevent overlapping in the next loop\n            batch_end = next_end\n        better_batch = better_batch[:batch_size]\n        last_processed = batch_end\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            future_to_pair = {\n                executor.submit(\n                    self.compare_pair,\n                    self.config[\"comparison_prompt\"],\n                    self.config.get(\"comparison_model\", self.default_model),\n                    input_data[pair[0]],\n                    input_data[pair[1]],\n                    blocking_keys,\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                ): pair\n                for pair in better_batch\n            }\n\n            for future in as_completed(future_to_pair):\n                pair = future_to_pair[future]\n                is_match_result, cost, prompt = future.result()\n                pair_costs += cost\n                if is_match_result:\n                    merge_clusters(pair[0], pair[1])\n\n                if self.config.get(\"enable_observability\", False):\n                    observability_key = f\"_observability_{self.config['name']}\"\n                    for idx in (pair[0], pair[1]):\n                        if observability_key not in input_data[idx]:\n                            input_data[idx][observability_key] = {\n                                \"comparison_prompts\": [],\n                                \"resolution_prompt\": None,\n                            }\n                        input_data[idx][observability_key][\n                            \"comparison_prompts\"\n                        ].append(prompt)\n\n    total_cost += pair_costs\n\n    # Collect final clusters\n    final_clusters = [cluster for cluster in clusters if cluster]\n\n    # Process each cluster\n    results = []\n\n    def process_cluster(cluster):\n        if len(cluster) &gt; 1:\n            cluster_items = [input_data[i] for i in cluster]\n            if input_schema:\n                cluster_items = [\n                    {k: item[k] for k in input_schema.keys() if k in item}\n                    for item in cluster_items\n                ]\n\n            resolution_prompt = strict_render(\n                self.config[\"resolution_prompt\"], {\"inputs\": cluster_items}\n            )\n            reduction_response = self.runner.api.call_llm(\n                self.config.get(\"resolution_model\", self.default_model),\n                \"reduce\",\n                [{\"role\": \"user\", \"content\": resolution_prompt}],\n                self.config[\"output\"][\"schema\"],\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\n                    \"max_retries_per_timeout\", 2\n                ),\n                bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                validation_config=(\n                    {\n                        \"val_rule\": self.config.get(\"validate\", []),\n                        \"validation_fn\": self.validation_fn,\n                    }\n                    if self.config.get(\"validate\", None)\n                    else None\n                ),\n                litellm_completion_kwargs=self.config.get(\n                    \"litellm_completion_kwargs\", {}\n                ),\n                op_config=self.config,\n            )\n            reduction_cost = reduction_response.total_cost\n\n            if self.config.get(\"enable_observability\", False):\n                for item in [input_data[i] for i in cluster]:\n                    observability_key = f\"_observability_{self.config['name']}\"\n                    if observability_key not in item:\n                        item[observability_key] = {\n                            \"comparison_prompts\": [],\n                            \"resolution_prompt\": None,\n                        }\n                    item[observability_key][\"resolution_prompt\"] = resolution_prompt\n\n            if reduction_response.validated:\n                reduction_output = self.runner.api.parse_llm_response(\n                    reduction_response.response,\n                    self.config[\"output\"][\"schema\"],\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n\n                # If the output is overwriting an existing key, we want to save the kv pairs\n                keys_in_output = [\n                    k\n                    for k in set(reduction_output.keys())\n                    if k in cluster_items[0].keys()\n                ]\n\n                return (\n                    [\n                        {\n                            **item,\n                            f\"_kv_pairs_preresolve_{self.config['name']}\": {\n                                k: item[k] for k in keys_in_output\n                            },\n                            **{\n                                k: reduction_output[k]\n                                for k in self.config[\"output\"][\"schema\"]\n                            },\n                        }\n                        for item in [input_data[i] for i in cluster]\n                    ],\n                    reduction_cost,\n                )\n            return [], reduction_cost\n        else:\n            # Set the output schema to be the keys found in the compare_prompt\n            compare_prompt_keys = extract_jinja_variables(\n                self.config[\"comparison_prompt\"]\n            )\n            # Get the set of keys in the compare_prompt\n            compare_prompt_keys = set(\n                [\n                    k.replace(\"input1.\", \"\")\n                    for k in compare_prompt_keys\n                    if \"input1\" in k\n                ]\n            )\n\n            # For each key in the output schema, find the most similar key in the compare_prompt\n            output_keys = set(self.config[\"output\"][\"schema\"].keys())\n            key_mapping = {}\n            for output_key in output_keys:\n                best_match = None\n                best_score = 0\n                for compare_key in compare_prompt_keys:\n                    score = sum(\n                        c1 == c2 for c1, c2 in zip(output_key, compare_key)\n                    ) / max(len(output_key), len(compare_key))\n                    if score &gt; best_score:\n                        best_score = score\n                        best_match = compare_key\n                key_mapping[output_key] = best_match\n\n            # Create the result dictionary using the key mapping\n            result = input_data[list(cluster)[0]].copy()\n            result[f\"_kv_pairs_preresolve_{self.config['name']}\"] = {\n                ok: result[ck] for ok, ck in key_mapping.items() if ck in result\n            }\n            for output_key, compare_key in key_mapping.items():\n                if compare_key in input_data[list(cluster)[0]]:\n                    result[output_key] = input_data[list(cluster)[0]][compare_key]\n                elif output_key in input_data[list(cluster)[0]]:\n                    result[output_key] = input_data[list(cluster)[0]][output_key]\n                else:\n                    result[output_key] = None  # or some default value\n\n            return [result], 0\n\n    # Calculate the number of records before and clusters after\n    num_records_before = len(input_data)\n    num_clusters_after = len(final_clusters)\n    self.console.log(f\"Number of keys before resolution: {num_records_before}\")\n    self.console.log(\n        f\"Number of distinct keys after resolution: {num_clusters_after}\"\n    )\n\n    # If no resolution prompt is provided, we can skip the resolution phase\n    # And simply select the most common value for each key\n    if not self.config.get(\"resolution_prompt\", None):\n        for cluster in final_clusters:\n            if len(cluster) &gt; 1:\n                for key in self.config[\"output\"][\"keys\"]:\n                    most_common_value = max(\n                        set(input_data[i][key] for i in cluster),\n                        key=lambda x: sum(\n                            1 for i in cluster if input_data[i][key] == x\n                        ),\n                    )\n                    for i in cluster:\n                        input_data[i][key] = most_common_value\n        results = input_data\n    else:\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(process_cluster, cluster)\n                for cluster in final_clusters\n            ]\n            for future in rich_as_completed(\n                futures,\n                total=len(futures),\n                desc=\"Determining resolved key for each group of equivalent keys\",\n                console=self.console,\n            ):\n                cluster_results, cluster_cost = future.result()\n                results.extend(cluster_results)\n                total_cost += cluster_cost\n\n    total_pairs = len(input_data) * (len(input_data) - 1) // 2\n    true_match_count = sum(\n        len(cluster) * (len(cluster) - 1) // 2\n        for cluster in final_clusters\n        if len(cluster) &gt; 1\n    )\n    true_match_selectivity = (\n        true_match_count / total_pairs if total_pairs &gt; 0 else 0\n    )\n    self.console.log(f\"Self-join selectivity: {true_match_selectivity:.4f}\")\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation","title":"<code>docetl.operations.reduce.ReduceOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that implements a reduce operation on input data using language models.</p> <p>This class extends BaseOperation to provide functionality for reducing grouped data using various strategies including batch reduce, incremental reduce, and parallel fold and merge.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>class ReduceOperation(BaseOperation):\n    \"\"\"\n    A class that implements a reduce operation on input data using language models.\n\n    This class extends BaseOperation to provide functionality for reducing grouped data\n    using various strategies including batch reduce, incremental reduce, and parallel fold and merge.\n    \"\"\"\n\n    class schema(BaseOperation.schema):\n        type: str = \"reduce\"\n        reduce_key: str | list[str]\n        output: dict[str, Any]\n        prompt: str\n        optimize: bool | None = None\n        synthesize_resolve: bool | None = None\n        model: str | None = None\n        input: dict[str, Any] | None = None\n        pass_through: bool | None = None\n        associative: bool | None = None\n        fold_prompt: str | None = None\n        fold_batch_size: int | None = Field(None, gt=0)\n        merge_prompt: str | None = None\n        merge_batch_size: int | None = Field(None, gt=0)\n        value_sampling: dict[str, Any] | None = None\n        verbose: bool | None = None\n        timeout: int | None = None\n        litellm_completion_kwargs: dict[str, Any] = Field(default_factory=dict)\n        enable_observability: bool = False\n        limit: int | None = Field(None, gt=0)\n\n        @field_validator(\"prompt\")\n        def validate_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    return v\n                try:\n                    template = Template(v)\n                    template_vars = template.environment.parse(v).find_all(\n                        jinja2.nodes.Name\n                    )\n                    template_var_names = {var.name for var in template_vars}\n                    if \"inputs\" not in template_var_names:\n                        raise ValueError(\n                            \"Prompt template must include the 'inputs' variable\"\n                        )\n                except Exception as e:\n                    raise ValueError(f\"Invalid Jinja2 template in 'prompt': {str(e)}\")\n            return v\n\n        @field_validator(\"fold_prompt\")\n        def validate_fold_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    return v\n                try:\n                    fold_template = Template(v)\n                    fold_template_vars = fold_template.environment.parse(v).find_all(\n                        jinja2.nodes.Name\n                    )\n                    fold_template_var_names = {var.name for var in fold_template_vars}\n                    required_vars = {\"inputs\", \"output\"}\n                    if not required_vars.issubset(fold_template_var_names):\n                        raise ValueError(\n                            f\"Fold template must include variables: {required_vars}. Current template includes: {fold_template_var_names}\"\n                        )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'fold_prompt': {str(e)}\"\n                    )\n            return v\n\n        @field_validator(\"merge_prompt\")\n        def validate_merge_prompt(cls, v):\n            if v is not None:\n                # Check if it has Jinja syntax\n                if not has_jinja_syntax(v):\n                    # This will be handled during initialization with user confirmation\n                    return v\n                try:\n                    merge_template = Template(v)\n                    merge_template_vars = merge_template.environment.parse(v).find_all(\n                        jinja2.nodes.Name\n                    )\n                    merge_template_var_names = {var.name for var in merge_template_vars}\n                    if \"outputs\" not in merge_template_var_names:\n                        raise ValueError(\n                            \"Merge template must include the 'outputs' variable\"\n                        )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'merge_prompt': {str(e)}\"\n                    )\n            return v\n\n        @field_validator(\"value_sampling\")\n        def validate_value_sampling(cls, v):\n            if v is not None:\n                if v[\"enabled\"]:\n                    if v[\"method\"] not in [\"random\", \"first_n\", \"cluster\", \"sem_sim\"]:\n                        raise ValueError(\n                            \"Invalid 'method'. Must be 'random', 'first_n', 'cluster', or 'sem_sim'\"\n                        )\n\n                    if v[\"method\"] == \"embedding\":\n                        if \"embedding_model\" not in v:\n                            raise ValueError(\n                                \"'embedding_model' is required when using embedding-based sampling\"\n                            )\n                        if \"embedding_keys\" not in v:\n                            raise ValueError(\n                                \"'embedding_keys' is required when using embedding-based sampling\"\n                            )\n            return v\n\n        @model_validator(mode=\"after\")\n        def validate_complex_requirements(self):\n            # Check dependencies between merge_prompt and fold_prompt\n            if self.merge_prompt and not self.fold_prompt:\n                raise ValueError(\n                    \"'fold_prompt' is required when 'merge_prompt' is specified\"\n                )\n\n            # Check batch size requirements\n            if self.fold_prompt and not self.fold_batch_size:\n                raise ValueError(\n                    \"'fold_batch_size' is required when 'fold_prompt' is specified\"\n                )\n            if self.merge_prompt and not self.merge_batch_size:\n                raise ValueError(\n                    \"'merge_batch_size' is required when 'merge_prompt' is specified\"\n                )\n\n            return self\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the ReduceOperation.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.min_samples = 5\n        self.max_samples = 1000\n        self.fold_times = deque(maxlen=self.max_samples)\n        self.merge_times = deque(maxlen=self.max_samples)\n        self.lock = Lock()\n        self.config[\"reduce_key\"] = (\n            [self.config[\"reduce_key\"]]\n            if isinstance(self.config[\"reduce_key\"], str)\n            else self.config[\"reduce_key\"]\n        )\n        self.intermediates = {}\n        self.lineage_keys = self.config.get(\"output\", {}).get(\"lineage\", [])\n        # Check for non-Jinja prompts and prompt user for confirmation\n        if \"prompt\" in self.config and not has_jinja_syntax(self.config[\"prompt\"]):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"prompt\"], self.config[\"name\"], \"prompt\"\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your prompt.\"\n                )\n            # Mark that we need to append document statement (for reduce, use inputs)\n            self.config[\"_append_document_to_prompt\"] = True\n            self.config[\"_is_reduce_operation\"] = True\n        if \"fold_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"fold_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"fold_prompt\"], self.config[\"name\"], \"fold_prompt\"\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your fold_prompt.\"\n                )\n            self.config[\"_append_document_to_fold_prompt\"] = True\n            self.config[\"_is_reduce_operation\"] = True\n        if \"merge_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"merge_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"merge_prompt\"], self.config[\"name\"], \"merge_prompt\"\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your merge_prompt.\"\n                )\n            self.config[\"_append_document_to_merge_prompt\"] = True\n            self.config[\"_is_reduce_operation\"] = True\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Execute the reduce operation on the provided input data.\n\n        This method sorts and groups the input data by the reduce key(s), then processes each group\n        using either parallel fold and merge, incremental reduce, or batch reduce strategies.\n\n        Args:\n            input_data (list[dict]): The input data to process.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.\n        \"\"\"\n        if self.config.get(\"gleaning\", {}).get(\"validation_prompt\", None):\n            self.console.log(\n                f\"Using gleaning with validation prompt: {self.config.get('gleaning', {}).get('validation_prompt', '')}\"\n            )\n\n        reduce_keys = self.config[\"reduce_key\"]\n        if isinstance(reduce_keys, str):\n            reduce_keys = [reduce_keys]\n        input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n\n        if self.status:\n            self.status.stop()\n\n        # Check if we need to group everything into one group\n        if reduce_keys == [\"_all\"] or reduce_keys == \"_all\":\n            grouped_data = [(\"_all\", input_data)]\n        else:\n            # Group the input data by the reduce key(s) while maintaining original order\n            def get_group_key(item):\n                key_values = []\n                for key in reduce_keys:\n                    value = item[key]\n                    # Special handling for list-type values\n                    if isinstance(value, list):\n                        key_values.append(\n                            tuple(sorted(value))\n                        )  # Convert list to sorted tuple\n                    else:\n                        key_values.append(value)\n                return tuple(key_values)\n\n            grouped_data = {}\n            for item in input_data:\n                key = get_group_key(item)\n                if key not in grouped_data:\n                    grouped_data[key] = []\n                grouped_data[key].append(item)\n\n            # Convert the grouped data to a list of tuples\n            grouped_data = list(grouped_data.items())\n\n        limit_value = self.config.get(\"limit\")\n        if limit_value is not None:\n            # Sort by group size (smallest first) and take the limit\n            grouped_data = sorted(grouped_data, key=lambda x: len(x[1]))\n            grouped_data = grouped_data[:limit_value]\n\n        def process_group(\n            key: tuple, group_elems: list[dict]\n        ) -&gt; tuple[dict | None, float]:\n            if input_schema:\n                group_list = [\n                    {k: item[k] for k in input_schema.keys() if k in item}\n                    for item in group_elems\n                ]\n            else:\n                group_list = group_elems\n\n            total_cost = 0.0\n            # Build retrieval context once per group\n            try:\n                retrieval_context = self._maybe_build_retrieval_context(\n                    {\n                        \"reduce_key\": dict(zip(self.config[\"reduce_key\"], key)),\n                        \"inputs\": group_list,\n                    }\n                )\n            except Exception:\n                retrieval_context = \"No extra context available.\"\n\n            # Apply value sampling if enabled\n            value_sampling = self.config.get(\"value_sampling\", {})\n            if value_sampling.get(\"enabled\", False):\n                sample_size = min(value_sampling[\"sample_size\"], len(group_list))\n                method = value_sampling[\"method\"]\n\n                if method == \"random\":\n                    group_sample = random.sample(group_list, sample_size)\n                    group_sample.sort(key=lambda x: group_list.index(x))\n                elif method == \"first_n\":\n                    group_sample = group_list[:sample_size]\n                elif method == \"cluster\":\n                    group_sample, embedding_cost = self._cluster_based_sampling(\n                        group_list, value_sampling, sample_size\n                    )\n                    group_sample.sort(key=lambda x: group_list.index(x))\n                    total_cost += embedding_cost\n                elif method == \"sem_sim\":\n                    group_sample, embedding_cost = self._semantic_similarity_sampling(\n                        key, group_list, value_sampling, sample_size\n                    )\n                    group_sample.sort(key=lambda x: group_list.index(x))\n                    total_cost += embedding_cost\n\n                group_list = group_sample\n\n            # Only execute merge-based plans if associative = True\n            if \"merge_prompt\" in self.config and self.config.get(\"associative\", True):\n                result, prompts, cost = self._parallel_fold_and_merge(\n                    key, group_list, retrieval_context\n                )\n            elif self.config.get(\"fold_batch_size\", None) and self.config.get(\n                \"fold_batch_size\"\n            ) &gt;= len(group_list):\n                # If the fold batch size is greater than or equal to the number of items in the group,\n                # we can just run a single fold operation\n                result, prompt, cost = self._batch_reduce(\n                    key, group_list, None, retrieval_context\n                )\n                prompts = [prompt]\n            elif \"fold_prompt\" in self.config:\n                result, prompts, cost = self._incremental_reduce(\n                    key, group_list, retrieval_context\n                )\n            else:\n                result, prompt, cost = self._batch_reduce(\n                    key, group_list, None, retrieval_context\n                )\n                prompts = [prompt]\n\n            total_cost += cost\n\n            # Add the counts of items in the group to the result\n            result[f\"_counts_prereduce_{self.config['name']}\"] = len(group_elems)\n\n            if self.config.get(\"enable_observability\", False):\n                # Add the _observability_{self.config['name']} key to the result\n                result[f\"_observability_{self.config['name']}\"] = {\"prompts\": prompts}\n\n            # Add retrieved context if save_retriever_output is enabled\n            if self.config.get(\"save_retriever_output\", False):\n                ctx = (\n                    retrieval_context\n                    if retrieval_context\n                    and retrieval_context != \"No extra context available.\"\n                    else \"\"\n                )\n                result[f\"_{self.config['name']}_retrieved_context\"] = ctx\n\n            # Apply pass-through at the group level\n            if (\n                result is not None\n                and self.config.get(\"pass_through\", False)\n                and group_elems\n            ):\n                for k, v in group_elems[0].items():\n                    if k not in self.config[\"output\"][\"schema\"] and k not in result:\n                        result[k] = v\n\n            # Add lineage information\n            if result is not None and self.lineage_keys:\n                lineage = []\n                for item in group_elems:\n                    lineage_item = {\n                        k: item.get(k) for k in self.lineage_keys if k in item\n                    }\n                    if lineage_item:\n                        lineage.append(lineage_item)\n                result[f\"{self.config['name']}_lineage\"] = lineage\n\n            return result, total_cost\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(process_group, key, group)\n                for key, group in grouped_data\n            ]\n            results = []\n            total_cost = 0\n            for future in rich_as_completed(\n                futures,\n                total=len(futures),\n                desc=f\"Processing {self.config['name']} (reduce) on all documents\",\n                leave=True,\n                console=self.console,\n            ):\n                output, item_cost = future.result()\n                total_cost += item_cost\n                if output is not None:\n                    results.append(output)\n\n        if limit_value is not None and len(results) &gt; limit_value:\n            results = results[:limit_value]\n\n        if self.config.get(\"persist_intermediates\", False):\n            for result in results:\n                key = tuple(result[k] for k in self.config[\"reduce_key\"])\n                if key in self.intermediates:\n                    result[f\"_{self.config['name']}_intermediates\"] = (\n                        self.intermediates[key]\n                    )\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n\n    def _cluster_based_sampling(\n        self, group_list: list[dict], value_sampling: dict, sample_size: int\n    ) -&gt; tuple[list[dict], float]:\n        if sample_size &gt;= len(group_list):\n            return group_list, 0\n\n        clusters, cost = cluster_documents(\n            group_list, value_sampling, sample_size, self.runner.api\n        )\n\n        sampled_items = []\n        idx_added_already = set()\n        num_clusters = len(clusters)\n        for i in range(sample_size):\n            # Add a random item from the cluster\n            idx = i % num_clusters\n\n            # Skip if there are no items in the cluster\n            if len(clusters[idx]) == 0:\n                continue\n\n            if len(clusters[idx]) == 1:\n                # If there's only one item in the cluster, add it directly if we haven't already\n                if idx not in idx_added_already:\n                    sampled_items.append(clusters[idx][0])\n                continue\n\n            random_choice_idx = random.randint(0, len(clusters[idx]) - 1)\n            max_attempts = 10\n            while random_choice_idx in idx_added_already and max_attempts &gt; 0:\n                random_choice_idx = random.randint(0, len(clusters[idx]) - 1)\n                max_attempts -= 1\n            idx_added_already.add(random_choice_idx)\n            sampled_items.append(clusters[idx][random_choice_idx])\n\n        return sampled_items, cost\n\n    def _semantic_similarity_sampling(\n        self, key: tuple, group_list: list[dict], value_sampling: dict, sample_size: int\n    ) -&gt; tuple[list[dict], float]:\n        embedding_model = value_sampling[\"embedding_model\"]\n        query_text = strict_render(\n            value_sampling[\"query_text\"],\n            {\"reduce_key\": dict(zip(self.config[\"reduce_key\"], key))},\n        )\n\n        embeddings, cost = get_embeddings_for_clustering(\n            group_list, value_sampling, self.runner.api\n        )\n\n        query_response = self.runner.api.gen_embedding(embedding_model, [query_text])\n        query_embedding = query_response[\"data\"][0][\"embedding\"]\n        cost += completion_cost(query_response)\n\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarities = cosine_similarity([query_embedding], embeddings)[0]\n\n        top_k_indices = np.argsort(similarities)[-sample_size:]\n\n        return [group_list[i] for i in top_k_indices], cost\n\n    def _parallel_fold_and_merge(\n        self, key: tuple, group_list: list[dict], retrieval_context: str\n    ) -&gt; tuple[dict | None, float]:\n        \"\"\"\n        Perform parallel folding and merging on a group of items.\n\n        This method implements a strategy that combines parallel folding of input items\n        and merging of intermediate results to efficiently process large groups. It works as follows:\n        1. The input group is initially divided into smaller batches for efficient processing.\n        2. The method performs an initial round of folding operations on these batches.\n        3. After the first round of folds, a few merges are performed to estimate the merge runtime.\n        4. Based on the estimated merge runtime and observed fold runtime, it calculates the optimal number of parallel folds. Subsequent rounds of folding are then performed concurrently, with the number of parallel folds determined by the runtime estimates.\n        5. The folding process repeats in rounds, progressively reducing the number of items to be processed.\n        6. Once all folding operations are complete, the method recursively performs final merges on the fold results to combine them into a final result.\n        7. Throughout this process, the method may adjust the number of parallel folds based on updated performance metrics (i.e., fold and merge runtimes) to maintain efficiency.\n\n        Args:\n            key (tuple): The reduce key tuple for the group.\n            group_list (list[dict]): The list of items in the group to be processed.\n\n        Returns:\n            tuple[dict | None, float]: A tuple containing the final merged result (or None if processing failed)\n            and the total cost of the operation.\n        \"\"\"\n        fold_batch_size = self.config[\"fold_batch_size\"]\n        merge_batch_size = self.config[\"merge_batch_size\"]\n        total_cost = 0\n        prompts = []\n\n        def calculate_num_parallel_folds():\n            fold_time, fold_default = self.get_fold_time()\n            merge_time, merge_default = self.get_merge_time()\n            num_group_items = len(group_list)\n            return (\n                max(\n                    1,\n                    int(\n                        (fold_time * num_group_items * math.log(merge_batch_size))\n                        / (fold_batch_size * merge_time)\n                    ),\n                ),\n                fold_default or merge_default,\n            )\n\n        num_parallel_folds, used_default_times = calculate_num_parallel_folds()\n        fold_results = []\n        remaining_items = group_list\n\n        if self.config.get(\"persist_intermediates\", False):\n            self.intermediates[key] = []\n            iter_count = 0\n\n        # Parallel folding and merging\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            while remaining_items:\n                # Folding phase\n                fold_futures = []\n                for i in range(min(num_parallel_folds, len(remaining_items))):\n                    batch = remaining_items[:fold_batch_size]\n                    remaining_items = remaining_items[fold_batch_size:]\n                    current_output = fold_results[i] if i &lt; len(fold_results) else None\n                    fold_futures.append(\n                        executor.submit(\n                            self._increment_fold, key, batch, current_output\n                        )\n                    )\n\n                new_fold_results = []\n                for future in as_completed(fold_futures):\n                    result, prompt, cost = future.result()\n                    total_cost += cost\n                    prompts.append(prompt)\n                    if result is not None:\n                        new_fold_results.append(result)\n                        if self.config.get(\"persist_intermediates\", False):\n                            self.intermediates[key].append(\n                                {\n                                    \"iter\": iter_count,\n                                    \"intermediate\": result,\n                                    \"scratchpad\": result[\"updated_scratchpad\"],\n                                }\n                            )\n                            iter_count += 1\n\n                # Update fold_results with new results\n                fold_results = new_fold_results + fold_results[len(new_fold_results) :]\n\n                # Single pass merging phase\n                if (\n                    len(self.merge_times) &lt; self.min_samples\n                    and len(fold_results) &gt;= merge_batch_size\n                ):\n                    merge_futures = []\n                    for i in range(0, len(fold_results), merge_batch_size):\n                        batch = fold_results[i : i + merge_batch_size]\n                        merge_futures.append(\n                            executor.submit(self._merge_results, key, batch)\n                        )\n\n                    new_results = []\n                    for future in as_completed(merge_futures):\n                        result, prompt, cost = future.result()\n                        total_cost += cost\n                        prompts.append(prompt)\n                        if result is not None:\n                            new_results.append(result)\n                            if self.config.get(\"persist_intermediates\", False):\n                                self.intermediates[key].append(\n                                    {\n                                        \"iter\": iter_count,\n                                        \"intermediate\": result,\n                                        \"scratchpad\": None,\n                                    }\n                                )\n                                iter_count += 1\n\n                    fold_results = new_results\n\n                # Recalculate num_parallel_folds if we used default times\n                if used_default_times:\n                    new_num_parallel_folds, used_default_times = (\n                        calculate_num_parallel_folds()\n                    )\n                    if not used_default_times:\n                        self.console.log(\n                            f\"Recalculated num_parallel_folds from {num_parallel_folds} to {new_num_parallel_folds}\"\n                        )\n                        num_parallel_folds = new_num_parallel_folds\n\n        # Final merging if needed\n        while len(fold_results) &gt; 1:\n            self.console.log(f\"Finished folding! Merging {len(fold_results)} items.\")\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n                merge_futures = []\n                for i in range(0, len(fold_results), merge_batch_size):\n                    batch = fold_results[i : i + merge_batch_size]\n                    merge_futures.append(\n                        executor.submit(self._merge_results, key, batch)\n                    )\n\n                new_results = []\n                for future in as_completed(merge_futures):\n                    result, prompt, cost = future.result()\n                    total_cost += cost\n                    prompts.append(prompt)\n                    if result is not None:\n                        new_results.append(result)\n                        if self.config.get(\"persist_intermediates\", False):\n                            self.intermediates[key].append(\n                                {\n                                    \"iter\": iter_count,\n                                    \"intermediate\": result,\n                                    \"scratchpad\": None,\n                                }\n                            )\n                            iter_count += 1\n\n                fold_results = new_results\n\n        return (\n            (fold_results[0], prompts, total_cost)\n            if fold_results\n            else (None, prompts, total_cost)\n        )\n\n    def _incremental_reduce(\n        self, key: tuple, group_list: list[dict], retrieval_context: str\n    ) -&gt; tuple[dict | None, list[str], float]:\n        \"\"\"\n        Perform an incremental reduce operation on a group of items.\n\n        This method processes the group in batches, incrementally folding the results.\n\n        Args:\n            key (tuple): The reduce key tuple for the group.\n            group_list (list[dict]): The list of items in the group to be processed.\n\n        Returns:\n            tuple[dict | None, list[str], float]: A tuple containing the final reduced result (or None if processing failed),\n            the list of prompts used, and the total cost of the operation.\n        \"\"\"\n        fold_batch_size = self.config[\"fold_batch_size\"]\n        total_cost = 0\n        current_output = None\n        prompts = []\n\n        # Calculate and log the number of folds to be performed\n        num_folds = (len(group_list) + fold_batch_size - 1) // fold_batch_size\n\n        scratchpad = \"\"\n        if self.config.get(\"persist_intermediates\", False):\n            self.intermediates[key] = []\n            iter_count = 0\n\n        for i in range(0, len(group_list), fold_batch_size):\n            # Log the current iteration and total number of folds\n            current_fold = i // fold_batch_size + 1\n            if self.config.get(\"verbose\", False):\n                self.console.log(\n                    f\"Processing fold {current_fold} of {num_folds} for group with key {key}\"\n                )\n            batch = group_list[i : i + fold_batch_size]\n\n            folded_output, prompt, fold_cost = self._increment_fold(\n                key, batch, current_output, scratchpad\n            )\n            total_cost += fold_cost\n            prompts.append(prompt)\n\n            if folded_output is None:\n                continue\n\n            if self.config.get(\"persist_intermediates\", False):\n                self.intermediates[key].append(\n                    {\n                        \"iter\": iter_count,\n                        \"intermediate\": folded_output,\n                        \"scratchpad\": folded_output.get(\"updated_scratchpad\", \"\"),\n                    }\n                )\n                iter_count += 1\n\n            # Pop off updated_scratchpad\n            if \"updated_scratchpad\" in folded_output:\n                scratchpad = folded_output[\"updated_scratchpad\"]\n                if self.config.get(\"verbose\", False):\n                    self.console.log(\n                        f\"Updated scratchpad for fold {current_fold}: {scratchpad}\"\n                    )\n                del folded_output[\"updated_scratchpad\"]\n\n            current_output = folded_output\n\n        return current_output, prompts, total_cost\n\n    def validation_fn(self, response: dict[str, Any]):\n        structured_mode = (\n            self.config.get(\"output\", {}).get(\"mode\")\n            == OutputMode.STRUCTURED_OUTPUT.value\n        )\n        output = self.runner.api.parse_llm_response(\n            response,\n            schema=self.config[\"output\"][\"schema\"],\n            use_structured_output=structured_mode,\n        )[0]\n        # Enforce type validation against output schema\n        is_types_valid, _errors = validate_output_types(\n            output,\n            self.config[\"output\"][\"schema\"],\n        )\n        if not is_types_valid:\n            return output, False\n        if self.runner.api.validate_output(self.config, output, self.console):\n            return output, True\n        return output, False\n\n    def _increment_fold(\n        self,\n        key: tuple,\n        batch: list[dict],\n        current_output: dict | None,\n        scratchpad: str | None = None,\n        retrieval_context: str | None = None,\n    ) -&gt; tuple[dict | None, str, float]:\n        \"\"\"\n        Perform an incremental fold operation on a batch of items.\n\n        This method folds a batch of items into the current output using the fold prompt.\n\n        Args:\n            key (tuple): The reduce key tuple for the group.\n            batch (list[dict]): The batch of items to be folded.\n            current_output (dict | None): The current accumulated output, if any.\n            scratchpad (str | None): The scratchpad to use for the fold operation.\n        Returns:\n            tuple[dict | None, str, float]: A tuple containing the folded output (or None if processing failed),\n            the prompt used, and the cost of the fold operation.\n        \"\"\"\n        if current_output is None:\n            return self._batch_reduce(key, batch, scratchpad, retrieval_context)\n\n        start_time = time.time()\n        fold_prompt = strict_render(\n            self.config[\"fold_prompt\"],\n            {\n                \"inputs\": batch,\n                \"output\": current_output,\n                \"reduce_key\": dict(zip(self.config[\"reduce_key\"], key)),\n                \"retrieval_context\": retrieval_context or \"\",\n            },\n        )\n        if retrieval_context and \"retrieval_context\" not in self.config.get(\n            \"fold_prompt\", \"\"\n        ):\n            fold_prompt = (\n                f\"Here is some extra context:\\n{retrieval_context}\\n\\n{fold_prompt}\"\n            )\n\n        response = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"reduce\",\n            [{\"role\": \"user\", \"content\": fold_prompt}],\n            self.config[\"output\"][\"schema\"],\n            scratchpad=scratchpad,\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": self.validation_fn,\n                }\n            ),\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            verbose=self.config.get(\"verbose\", False),\n            litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n            op_config=self.config,\n        )\n\n        end_time = time.time()\n        self._update_fold_time(end_time - start_time)\n        fold_cost = response.total_cost\n\n        if response.validated:\n            structured_mode = (\n                self.config.get(\"output\", {}).get(\"mode\")\n                == OutputMode.STRUCTURED_OUTPUT.value\n            )\n            folded_output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=self.config[\"output\"][\"schema\"],\n                manually_fix_errors=self.manually_fix_errors,\n                use_structured_output=structured_mode,\n            )[0]\n\n            folded_output.update(dict(zip(self.config[\"reduce_key\"], key)))\n            fold_cost = response.total_cost\n\n            return folded_output, fold_prompt, fold_cost\n\n        return None, fold_prompt, fold_cost\n\n    def _merge_results(\n        self, key: tuple, outputs: list[dict], retrieval_context: str | None = None\n    ) -&gt; tuple[dict | None, str, float]:\n        \"\"\"\n        Merge multiple outputs into a single result.\n\n        This method merges a list of outputs using the merge prompt.\n\n        Args:\n            key (tuple): The reduce key tuple for the group.\n            outputs (list[dict]): The list of outputs to be merged.\n\n        Returns:\n            tuple[dict | None, str, float]: A tuple containing the merged output (or None if processing failed),\n            the prompt used, and the cost of the merge operation.\n        \"\"\"\n        start_time = time.time()\n        merge_prompt = strict_render(\n            self.config[\"merge_prompt\"],\n            {\n                \"outputs\": outputs,\n                \"reduce_key\": dict(zip(self.config[\"reduce_key\"], key)),\n                \"retrieval_context\": retrieval_context or \"\",\n            },\n        )\n        if retrieval_context and \"retrieval_context\" not in self.config.get(\n            \"merge_prompt\", \"\"\n        ):\n            merge_prompt = (\n                f\"Here is some extra context:\\n{retrieval_context}\\n\\n{merge_prompt}\"\n            )\n        response = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"merge\",\n            [{\"role\": \"user\", \"content\": merge_prompt}],\n            self.config[\"output\"][\"schema\"],\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": self.validation_fn,\n                }\n                if self.config.get(\"validate\", None)\n                else None\n            ),\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            verbose=self.config.get(\"verbose\", False),\n            litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n            op_config=self.config,\n        )\n\n        end_time = time.time()\n        self._update_merge_time(end_time - start_time)\n        merge_cost = response.total_cost\n\n        if response.validated:\n            structured_mode = (\n                self.config.get(\"output\", {}).get(\"mode\")\n                == OutputMode.STRUCTURED_OUTPUT.value\n            )\n            merged_output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=self.config[\"output\"][\"schema\"],\n                manually_fix_errors=self.manually_fix_errors,\n                use_structured_output=structured_mode,\n            )[0]\n            merged_output.update(dict(zip(self.config[\"reduce_key\"], key)))\n            merge_cost = response.total_cost\n            return merged_output, merge_prompt, merge_cost\n\n        return None, merge_prompt, merge_cost\n\n    def get_fold_time(self) -&gt; tuple[float, bool]:\n        \"\"\"\n        Get the average fold time or a default value.\n\n        Returns:\n            tuple[float, bool]: A tuple containing the average fold time (or default) and a boolean\n            indicating whether the default value was used.\n        \"\"\"\n        if \"fold_time\" in self.config:\n            return self.config[\"fold_time\"], False\n        with self.lock:\n            if len(self.fold_times) &gt;= self.min_samples:\n                return sum(self.fold_times) / len(self.fold_times), False\n        return 1.0, True  # Default to 1 second if no data is available\n\n    def get_merge_time(self) -&gt; tuple[float, bool]:\n        \"\"\"\n        Get the average merge time or a default value.\n\n        Returns:\n            tuple[float, bool]: A tuple containing the average merge time (or default) and a boolean\n            indicating whether the default value was used.\n        \"\"\"\n        if \"merge_time\" in self.config:\n            return self.config[\"merge_time\"], False\n        with self.lock:\n            if len(self.merge_times) &gt;= self.min_samples:\n                return sum(self.merge_times) / len(self.merge_times), False\n        return 1.0, True  # Default to 1 second if no data is available\n\n    def _update_fold_time(self, time: float) -&gt; None:\n        \"\"\"\n        Update the fold time statistics.\n\n        Args:\n            time (float): The time taken for a fold operation.\n        \"\"\"\n        with self.lock:\n            self.fold_times.append(time)\n\n    def _update_merge_time(self, time: float) -&gt; None:\n        \"\"\"\n        Update the merge time statistics.\n\n        Args:\n            time (float): The time taken for a merge operation.\n        \"\"\"\n        with self.lock:\n            self.merge_times.append(time)\n\n    def _batch_reduce(\n        self,\n        key: tuple,\n        group_list: list[dict],\n        scratchpad: str | None = None,\n        retrieval_context: str | None = None,\n    ) -&gt; tuple[dict | None, str, float]:\n        \"\"\"\n        Perform a batch reduce operation on a group of items.\n\n        This method reduces a group of items into a single output using the reduce prompt.\n\n        Args:\n            key (tuple): The reduce key tuple for the group.\n            group_list (list[dict]): The list of items to be reduced.\n            scratchpad (str | None): The scratchpad to use for the reduce operation.\n        Returns:\n            tuple[dict | None, str, float]: A tuple containing the reduced output (or None if processing failed),\n            the prompt used, and the cost of the reduce operation.\n        \"\"\"\n        prompt = strict_render(\n            self.config[\"prompt\"],\n            {\n                \"reduce_key\": dict(zip(self.config[\"reduce_key\"], key)),\n                \"inputs\": group_list,\n                \"retrieval_context\": retrieval_context or \"\",\n            },\n        )\n        if retrieval_context and \"retrieval_context\" not in self.config.get(\n            \"prompt\", \"\"\n        ):\n            prompt = f\"Here is some extra context:\\n{retrieval_context}\\n\\n{prompt}\"\n        item_cost = 0\n\n        response = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"reduce\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            self.config[\"output\"][\"schema\"],\n            scratchpad=scratchpad,\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": self.validation_fn,\n                }\n                if self.config.get(\"validate\", None)\n                else None\n            ),\n            gleaning_config=self.config.get(\"gleaning\", None),\n            verbose=self.config.get(\"verbose\", False),\n            litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n            op_config=self.config,\n        )\n\n        item_cost += response.total_cost\n\n        if response.validated:\n            structured_mode = (\n                self.config.get(\"output\", {}).get(\"mode\")\n                == OutputMode.STRUCTURED_OUTPUT.value\n            )\n            output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=self.config[\"output\"][\"schema\"],\n                manually_fix_errors=self.manually_fix_errors,\n                use_structured_output=structured_mode,\n            )[0]\n            output.update(dict(zip(self.config[\"reduce_key\"], key)))\n\n            return output, prompt, item_cost\n        return None, prompt, item_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the ReduceOperation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the ReduceOperation.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.min_samples = 5\n    self.max_samples = 1000\n    self.fold_times = deque(maxlen=self.max_samples)\n    self.merge_times = deque(maxlen=self.max_samples)\n    self.lock = Lock()\n    self.config[\"reduce_key\"] = (\n        [self.config[\"reduce_key\"]]\n        if isinstance(self.config[\"reduce_key\"], str)\n        else self.config[\"reduce_key\"]\n    )\n    self.intermediates = {}\n    self.lineage_keys = self.config.get(\"output\", {}).get(\"lineage\", [])\n    # Check for non-Jinja prompts and prompt user for confirmation\n    if \"prompt\" in self.config and not has_jinja_syntax(self.config[\"prompt\"]):\n        if not prompt_user_for_non_jinja_confirmation(\n            self.config[\"prompt\"], self.config[\"name\"], \"prompt\"\n        ):\n            raise ValueError(\n                f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your prompt.\"\n            )\n        # Mark that we need to append document statement (for reduce, use inputs)\n        self.config[\"_append_document_to_prompt\"] = True\n        self.config[\"_is_reduce_operation\"] = True\n    if \"fold_prompt\" in self.config and not has_jinja_syntax(\n        self.config[\"fold_prompt\"]\n    ):\n        if not prompt_user_for_non_jinja_confirmation(\n            self.config[\"fold_prompt\"], self.config[\"name\"], \"fold_prompt\"\n        ):\n            raise ValueError(\n                f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your fold_prompt.\"\n            )\n        self.config[\"_append_document_to_fold_prompt\"] = True\n        self.config[\"_is_reduce_operation\"] = True\n    if \"merge_prompt\" in self.config and not has_jinja_syntax(\n        self.config[\"merge_prompt\"]\n    ):\n        if not prompt_user_for_non_jinja_confirmation(\n            self.config[\"merge_prompt\"], self.config[\"name\"], \"merge_prompt\"\n        ):\n            raise ValueError(\n                f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your merge_prompt.\"\n            )\n        self.config[\"_append_document_to_merge_prompt\"] = True\n        self.config[\"_is_reduce_operation\"] = True\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Execute the reduce operation on the provided input data.</p> <p>This method sorts and groups the input data by the reduce key(s), then processes each group using either parallel fold and merge, incremental reduce, or batch reduce strategies.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Execute the reduce operation on the provided input data.\n\n    This method sorts and groups the input data by the reduce key(s), then processes each group\n    using either parallel fold and merge, incremental reduce, or batch reduce strategies.\n\n    Args:\n        input_data (list[dict]): The input data to process.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.\n    \"\"\"\n    if self.config.get(\"gleaning\", {}).get(\"validation_prompt\", None):\n        self.console.log(\n            f\"Using gleaning with validation prompt: {self.config.get('gleaning', {}).get('validation_prompt', '')}\"\n        )\n\n    reduce_keys = self.config[\"reduce_key\"]\n    if isinstance(reduce_keys, str):\n        reduce_keys = [reduce_keys]\n    input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n\n    if self.status:\n        self.status.stop()\n\n    # Check if we need to group everything into one group\n    if reduce_keys == [\"_all\"] or reduce_keys == \"_all\":\n        grouped_data = [(\"_all\", input_data)]\n    else:\n        # Group the input data by the reduce key(s) while maintaining original order\n        def get_group_key(item):\n            key_values = []\n            for key in reduce_keys:\n                value = item[key]\n                # Special handling for list-type values\n                if isinstance(value, list):\n                    key_values.append(\n                        tuple(sorted(value))\n                    )  # Convert list to sorted tuple\n                else:\n                    key_values.append(value)\n            return tuple(key_values)\n\n        grouped_data = {}\n        for item in input_data:\n            key = get_group_key(item)\n            if key not in grouped_data:\n                grouped_data[key] = []\n            grouped_data[key].append(item)\n\n        # Convert the grouped data to a list of tuples\n        grouped_data = list(grouped_data.items())\n\n    limit_value = self.config.get(\"limit\")\n    if limit_value is not None:\n        # Sort by group size (smallest first) and take the limit\n        grouped_data = sorted(grouped_data, key=lambda x: len(x[1]))\n        grouped_data = grouped_data[:limit_value]\n\n    def process_group(\n        key: tuple, group_elems: list[dict]\n    ) -&gt; tuple[dict | None, float]:\n        if input_schema:\n            group_list = [\n                {k: item[k] for k in input_schema.keys() if k in item}\n                for item in group_elems\n            ]\n        else:\n            group_list = group_elems\n\n        total_cost = 0.0\n        # Build retrieval context once per group\n        try:\n            retrieval_context = self._maybe_build_retrieval_context(\n                {\n                    \"reduce_key\": dict(zip(self.config[\"reduce_key\"], key)),\n                    \"inputs\": group_list,\n                }\n            )\n        except Exception:\n            retrieval_context = \"No extra context available.\"\n\n        # Apply value sampling if enabled\n        value_sampling = self.config.get(\"value_sampling\", {})\n        if value_sampling.get(\"enabled\", False):\n            sample_size = min(value_sampling[\"sample_size\"], len(group_list))\n            method = value_sampling[\"method\"]\n\n            if method == \"random\":\n                group_sample = random.sample(group_list, sample_size)\n                group_sample.sort(key=lambda x: group_list.index(x))\n            elif method == \"first_n\":\n                group_sample = group_list[:sample_size]\n            elif method == \"cluster\":\n                group_sample, embedding_cost = self._cluster_based_sampling(\n                    group_list, value_sampling, sample_size\n                )\n                group_sample.sort(key=lambda x: group_list.index(x))\n                total_cost += embedding_cost\n            elif method == \"sem_sim\":\n                group_sample, embedding_cost = self._semantic_similarity_sampling(\n                    key, group_list, value_sampling, sample_size\n                )\n                group_sample.sort(key=lambda x: group_list.index(x))\n                total_cost += embedding_cost\n\n            group_list = group_sample\n\n        # Only execute merge-based plans if associative = True\n        if \"merge_prompt\" in self.config and self.config.get(\"associative\", True):\n            result, prompts, cost = self._parallel_fold_and_merge(\n                key, group_list, retrieval_context\n            )\n        elif self.config.get(\"fold_batch_size\", None) and self.config.get(\n            \"fold_batch_size\"\n        ) &gt;= len(group_list):\n            # If the fold batch size is greater than or equal to the number of items in the group,\n            # we can just run a single fold operation\n            result, prompt, cost = self._batch_reduce(\n                key, group_list, None, retrieval_context\n            )\n            prompts = [prompt]\n        elif \"fold_prompt\" in self.config:\n            result, prompts, cost = self._incremental_reduce(\n                key, group_list, retrieval_context\n            )\n        else:\n            result, prompt, cost = self._batch_reduce(\n                key, group_list, None, retrieval_context\n            )\n            prompts = [prompt]\n\n        total_cost += cost\n\n        # Add the counts of items in the group to the result\n        result[f\"_counts_prereduce_{self.config['name']}\"] = len(group_elems)\n\n        if self.config.get(\"enable_observability\", False):\n            # Add the _observability_{self.config['name']} key to the result\n            result[f\"_observability_{self.config['name']}\"] = {\"prompts\": prompts}\n\n        # Add retrieved context if save_retriever_output is enabled\n        if self.config.get(\"save_retriever_output\", False):\n            ctx = (\n                retrieval_context\n                if retrieval_context\n                and retrieval_context != \"No extra context available.\"\n                else \"\"\n            )\n            result[f\"_{self.config['name']}_retrieved_context\"] = ctx\n\n        # Apply pass-through at the group level\n        if (\n            result is not None\n            and self.config.get(\"pass_through\", False)\n            and group_elems\n        ):\n            for k, v in group_elems[0].items():\n                if k not in self.config[\"output\"][\"schema\"] and k not in result:\n                    result[k] = v\n\n        # Add lineage information\n        if result is not None and self.lineage_keys:\n            lineage = []\n            for item in group_elems:\n                lineage_item = {\n                    k: item.get(k) for k in self.lineage_keys if k in item\n                }\n                if lineage_item:\n                    lineage.append(lineage_item)\n            result[f\"{self.config['name']}_lineage\"] = lineage\n\n        return result, total_cost\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        futures = [\n            executor.submit(process_group, key, group)\n            for key, group in grouped_data\n        ]\n        results = []\n        total_cost = 0\n        for future in rich_as_completed(\n            futures,\n            total=len(futures),\n            desc=f\"Processing {self.config['name']} (reduce) on all documents\",\n            leave=True,\n            console=self.console,\n        ):\n            output, item_cost = future.result()\n            total_cost += item_cost\n            if output is not None:\n                results.append(output)\n\n    if limit_value is not None and len(results) &gt; limit_value:\n        results = results[:limit_value]\n\n    if self.config.get(\"persist_intermediates\", False):\n        for result in results:\n            key = tuple(result[k] for k in self.config[\"reduce_key\"])\n            if key in self.intermediates:\n                result[f\"_{self.config['name']}_intermediates\"] = (\n                    self.intermediates[key]\n                )\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.get_fold_time","title":"<code>get_fold_time()</code>","text":"<p>Get the average fold time or a default value.</p> <p>Returns:</p> Type Description <code>float</code> <p>tuple[float, bool]: A tuple containing the average fold time (or default) and a boolean</p> <code>bool</code> <p>indicating whether the default value was used.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def get_fold_time(self) -&gt; tuple[float, bool]:\n    \"\"\"\n    Get the average fold time or a default value.\n\n    Returns:\n        tuple[float, bool]: A tuple containing the average fold time (or default) and a boolean\n        indicating whether the default value was used.\n    \"\"\"\n    if \"fold_time\" in self.config:\n        return self.config[\"fold_time\"], False\n    with self.lock:\n        if len(self.fold_times) &gt;= self.min_samples:\n            return sum(self.fold_times) / len(self.fold_times), False\n    return 1.0, True  # Default to 1 second if no data is available\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.get_merge_time","title":"<code>get_merge_time()</code>","text":"<p>Get the average merge time or a default value.</p> <p>Returns:</p> Type Description <code>float</code> <p>tuple[float, bool]: A tuple containing the average merge time (or default) and a boolean</p> <code>bool</code> <p>indicating whether the default value was used.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def get_merge_time(self) -&gt; tuple[float, bool]:\n    \"\"\"\n    Get the average merge time or a default value.\n\n    Returns:\n        tuple[float, bool]: A tuple containing the average merge time (or default) and a boolean\n        indicating whether the default value was used.\n    \"\"\"\n    if \"merge_time\" in self.config:\n        return self.config[\"merge_time\"], False\n    with self.lock:\n        if len(self.merge_times) &gt;= self.min_samples:\n            return sum(self.merge_times) / len(self.merge_times), False\n    return 1.0, True  # Default to 1 second if no data is available\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.ParallelMapOperation","title":"<code>docetl.operations.map.ParallelMapOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/map.py</code> <pre><code>class ParallelMapOperation(BaseOperation):\n    class schema(BaseOperation.schema):\n        type: str = \"parallel_map\"\n        prompts: list[dict[str, Any]] | None = None\n        output: dict[str, Any] | None = None\n        drop_keys: list[str] | None = None\n        enable_observability: bool = False\n        pdf_url_key: str | None = None\n\n        @field_validator(\"prompts\")\n        def validate_prompts(cls, v):\n            if v is not None:\n                if not v:\n                    raise ValueError(\"The 'prompts' list cannot be empty\")\n\n                for i, prompt_config in enumerate(v):\n                    # Validate required keys exist\n                    if \"prompt\" not in prompt_config:\n                        raise ValueError(\n                            f\"Missing required key 'prompt' in prompt configuration {i}\"\n                        )\n                    if \"output_keys\" not in prompt_config:\n                        raise ValueError(\n                            f\"Missing required key 'output_keys' in prompt configuration {i}\"\n                        )\n\n                    # Validate output_keys is not empty\n                    if not prompt_config[\"output_keys\"]:\n                        raise ValueError(\n                            f\"'output_keys' list in prompt configuration {i} cannot be empty\"\n                        )\n\n                    # Check if the prompt is a valid Jinja2 template\n                    try:\n                        Template(prompt_config[\"prompt\"])\n                    except Exception as e:\n                        raise ValueError(\n                            f\"Invalid Jinja2 template in prompt configuration {i}: {str(e)}\"\n                        ) from e\n            return v\n\n        @model_validator(mode=\"after\")\n        def validate_prompt_requirements(self):\n            # If drop_keys is not specified, prompts must be present\n            if not self.drop_keys and not self.prompts:\n                raise ValueError(\n                    \"If 'drop_keys' is not specified, 'prompts' must be present in the configuration\"\n                )\n\n            # Check if all output schema keys are covered by the prompts\n            if self.prompts and self.output and \"schema\" in self.output:\n                output_schema = self.output[\"schema\"]\n                output_keys_covered = set()\n                for prompt_config in self.prompts:\n                    output_keys_covered.update(prompt_config[\"output_keys\"])\n\n                missing_keys = set(output_schema.keys()) - output_keys_covered\n                if missing_keys:\n                    raise ValueError(\n                        f\"The following output schema keys are not covered by any prompt: {missing_keys}\"\n                    )\n\n            return self\n\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the parallel map operation on the provided input data.\n\n        Args:\n            input_data (list[dict]): The input data to process.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n        This method performs the following steps:\n        1. If prompts are specified, it processes each input item using multiple prompts in parallel\n        2. Aggregates results from different prompts for each input item\n        3. Validates the combined output for each item\n        4. If drop_keys is specified, it drops the specified keys from each document\n        5. Calculates total cost of the operation\n        \"\"\"\n        results = {}\n        total_cost = 0\n        output_schema = self.config.get(\"output\", {}).get(\"schema\", {})\n\n        # Check if there's no prompt and only drop_keys\n        if \"prompts\" not in self.config and \"drop_keys\" in self.config:\n            # If only drop_keys is specified, simply drop the keys and return\n            dropped_results = []\n            for item in input_data:\n                new_item = {\n                    k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n                }\n                dropped_results.append(new_item)\n            return dropped_results, 0.0  # Return the modified data with no cost\n\n        if self.status:\n            self.status.stop()\n\n        def process_prompt(item, prompt_config):\n            prompt = strict_render(prompt_config[\"prompt\"], {\"input\": item})\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            if self.config.get(\"pdf_url_key\", None):\n                try:\n                    pdf_url = item[self.config[\"pdf_url_key\"]]\n                except KeyError:\n                    raise ValueError(\n                        f\"PDF URL key '{self.config['pdf_url_key']}' not found in input data\"\n                    )\n                # Download content\n                if pdf_url.startswith(\"http\"):\n                    file_data = requests.get(pdf_url).content\n                else:\n                    with open(pdf_url, \"rb\") as f:\n                        file_data = f.read()\n                encoded_file = base64.b64encode(file_data).decode(\"utf-8\")\n                base64_url = f\"data:application/pdf;base64,{encoded_file}\"\n\n                messages[0][\"content\"] = [\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": base64_url}},\n                    {\"type\": \"text\", \"text\": prompt},\n                ]\n\n            local_output_schema = {\n                key: output_schema.get(key, \"string\")\n                for key in prompt_config[\"output_keys\"]\n            }\n            model = prompt_config.get(\"model\", self.default_model)\n            if not model:\n                model = self.default_model\n\n            # Start of Selection\n            # If there are tools, we need to pass in the tools\n            response = self.runner.api.call_llm(\n                model,\n                \"parallel_map\",\n                messages,\n                local_output_schema,\n                tools=prompt_config.get(\"tools\", None),\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                gleaning_config=prompt_config.get(\"gleaning\", None),\n                bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                litellm_completion_kwargs=self.config.get(\n                    \"litellm_completion_kwargs\", {}\n                ),\n                op_config=self.config,\n            )\n            structured_mode = (\n                self.config.get(\"output\", {}).get(\"mode\")\n                == OutputMode.STRUCTURED_OUTPUT.value\n            )\n            output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=local_output_schema,\n                tools=prompt_config.get(\"tools\", None),\n                manually_fix_errors=self.manually_fix_errors,\n                use_structured_output=structured_mode,\n            )[0]\n            return output, prompt, response.total_cost\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            if \"prompts\" in self.config:\n                # Create all futures at once\n                all_futures = [\n                    executor.submit(process_prompt, item, prompt_config)\n                    for item in input_data\n                    for prompt_config in self.config[\"prompts\"]\n                ]\n\n                # Process results in order\n                for i in tqdm(\n                    range(len(all_futures)),\n                    desc=\"Processing parallel map items\",\n                ):\n                    future = all_futures[i]\n                    output, prompt, cost = future.result()\n                    total_cost += cost\n\n                    # Determine which item this future corresponds to\n                    item_index = i // len(self.config[\"prompts\"])\n                    prompt_index = i % len(self.config[\"prompts\"])\n\n                    # Initialize or update the item_result\n                    if prompt_index == 0:\n                        item_result = input_data[item_index].copy()\n                        results[item_index] = item_result\n\n                    # Fetch the item_result\n                    item_result = results[item_index]\n\n                    if self.config.get(\"enable_observability\", False):\n                        if f\"_observability_{self.config['name']}\" not in item_result:\n                            item_result[f\"_observability_{self.config['name']}\"] = {}\n                        item_result[f\"_observability_{self.config['name']}\"].update(\n                            {f\"prompt_{prompt_index}\": prompt}\n                        )\n\n                    # Update the item_result with the output\n                    item_result.update(output)\n\n            else:\n                results = {i: item.copy() for i, item in enumerate(input_data)}\n\n        # Apply drop_keys if specified\n        if \"drop_keys\" in self.config:\n            drop_keys = self.config[\"drop_keys\"]\n            for item in results.values():\n                for key in drop_keys:\n                    item.pop(key, None)\n\n        if self.status:\n            self.status.start()\n\n        # Return the results in order\n        return [results[i] for i in range(len(input_data)) if i in results], total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.ParallelMapOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the parallel map operation on the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.</p> <p>This method performs the following steps: 1. If prompts are specified, it processes each input item using multiple prompts in parallel 2. Aggregates results from different prompts for each input item 3. Validates the combined output for each item 4. If drop_keys is specified, it drops the specified keys from each document 5. Calculates total cost of the operation</p> Source code in <code>docetl/operations/map.py</code> <pre><code>def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the parallel map operation on the provided input data.\n\n    Args:\n        input_data (list[dict]): The input data to process.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n    This method performs the following steps:\n    1. If prompts are specified, it processes each input item using multiple prompts in parallel\n    2. Aggregates results from different prompts for each input item\n    3. Validates the combined output for each item\n    4. If drop_keys is specified, it drops the specified keys from each document\n    5. Calculates total cost of the operation\n    \"\"\"\n    results = {}\n    total_cost = 0\n    output_schema = self.config.get(\"output\", {}).get(\"schema\", {})\n\n    # Check if there's no prompt and only drop_keys\n    if \"prompts\" not in self.config and \"drop_keys\" in self.config:\n        # If only drop_keys is specified, simply drop the keys and return\n        dropped_results = []\n        for item in input_data:\n            new_item = {\n                k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n            }\n            dropped_results.append(new_item)\n        return dropped_results, 0.0  # Return the modified data with no cost\n\n    if self.status:\n        self.status.stop()\n\n    def process_prompt(item, prompt_config):\n        prompt = strict_render(prompt_config[\"prompt\"], {\"input\": item})\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        if self.config.get(\"pdf_url_key\", None):\n            try:\n                pdf_url = item[self.config[\"pdf_url_key\"]]\n            except KeyError:\n                raise ValueError(\n                    f\"PDF URL key '{self.config['pdf_url_key']}' not found in input data\"\n                )\n            # Download content\n            if pdf_url.startswith(\"http\"):\n                file_data = requests.get(pdf_url).content\n            else:\n                with open(pdf_url, \"rb\") as f:\n                    file_data = f.read()\n            encoded_file = base64.b64encode(file_data).decode(\"utf-8\")\n            base64_url = f\"data:application/pdf;base64,{encoded_file}\"\n\n            messages[0][\"content\"] = [\n                {\"type\": \"image_url\", \"image_url\": {\"url\": base64_url}},\n                {\"type\": \"text\", \"text\": prompt},\n            ]\n\n        local_output_schema = {\n            key: output_schema.get(key, \"string\")\n            for key in prompt_config[\"output_keys\"]\n        }\n        model = prompt_config.get(\"model\", self.default_model)\n        if not model:\n            model = self.default_model\n\n        # Start of Selection\n        # If there are tools, we need to pass in the tools\n        response = self.runner.api.call_llm(\n            model,\n            \"parallel_map\",\n            messages,\n            local_output_schema,\n            tools=prompt_config.get(\"tools\", None),\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            gleaning_config=prompt_config.get(\"gleaning\", None),\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            litellm_completion_kwargs=self.config.get(\n                \"litellm_completion_kwargs\", {}\n            ),\n            op_config=self.config,\n        )\n        structured_mode = (\n            self.config.get(\"output\", {}).get(\"mode\")\n            == OutputMode.STRUCTURED_OUTPUT.value\n        )\n        output = self.runner.api.parse_llm_response(\n            response.response,\n            schema=local_output_schema,\n            tools=prompt_config.get(\"tools\", None),\n            manually_fix_errors=self.manually_fix_errors,\n            use_structured_output=structured_mode,\n        )[0]\n        return output, prompt, response.total_cost\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        if \"prompts\" in self.config:\n            # Create all futures at once\n            all_futures = [\n                executor.submit(process_prompt, item, prompt_config)\n                for item in input_data\n                for prompt_config in self.config[\"prompts\"]\n            ]\n\n            # Process results in order\n            for i in tqdm(\n                range(len(all_futures)),\n                desc=\"Processing parallel map items\",\n            ):\n                future = all_futures[i]\n                output, prompt, cost = future.result()\n                total_cost += cost\n\n                # Determine which item this future corresponds to\n                item_index = i // len(self.config[\"prompts\"])\n                prompt_index = i % len(self.config[\"prompts\"])\n\n                # Initialize or update the item_result\n                if prompt_index == 0:\n                    item_result = input_data[item_index].copy()\n                    results[item_index] = item_result\n\n                # Fetch the item_result\n                item_result = results[item_index]\n\n                if self.config.get(\"enable_observability\", False):\n                    if f\"_observability_{self.config['name']}\" not in item_result:\n                        item_result[f\"_observability_{self.config['name']}\"] = {}\n                    item_result[f\"_observability_{self.config['name']}\"].update(\n                        {f\"prompt_{prompt_index}\": prompt}\n                    )\n\n                # Update the item_result with the output\n                item_result.update(output)\n\n        else:\n            results = {i: item.copy() for i, item in enumerate(input_data)}\n\n    # Apply drop_keys if specified\n    if \"drop_keys\" in self.config:\n        drop_keys = self.config[\"drop_keys\"]\n        for item in results.values():\n            for key in drop_keys:\n                item.pop(key, None)\n\n    if self.status:\n        self.status.start()\n\n    # Return the results in order\n    return [results[i] for i in range(len(input_data)) if i in results], total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.filter.FilterOperation","title":"<code>docetl.operations.filter.FilterOperation</code>","text":"<p>               Bases: <code>MapOperation</code></p> Source code in <code>docetl/operations/filter.py</code> <pre><code>class FilterOperation(MapOperation):\n    class schema(MapOperation.schema):\n        type: str = \"filter\"\n        prompt: str\n        output: dict[str, Any]\n\n        @model_validator(mode=\"after\")\n        def validate_filter_output_schema(self):\n            # Check that schema exists and has the right structure for filtering\n            schema_dict = self.output[\"schema\"]\n\n            # Filter out _short_explanation for validation\n            schema = {k: v for k, v in schema_dict.items() if k != \"_short_explanation\"}\n            if len(schema) != 1:\n                raise ValueError(\n                    \"The 'schema' in 'output' configuration must have exactly one key-value pair that maps to a boolean value\"\n                )\n\n            key, value = next(iter(schema.items()))\n            if value not in [\"bool\", \"boolean\"]:\n                raise TypeError(\n                    f\"The value in the 'schema' must be of type bool, got {value}\"\n                )\n\n            return self\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._filter_key = next(\n            iter(\n                [\n                    k\n                    for k in self.config[\"output\"][\"schema\"].keys()\n                    if k != \"_short_explanation\"\n                ]\n            )\n        )\n        self._filter_is_build = False\n\n    def _limit_applies_to_inputs(self) -&gt; bool:\n        return False\n\n    def _handle_result(self, result: dict[str, Any]) -&gt; tuple[dict | None, bool]:\n        keep_record = bool(result.get(self._filter_key))\n        result.pop(self._filter_key, None)\n\n        if self._filter_is_build or keep_record:\n            return result, keep_record\n        return None, False\n\n    def execute(\n        self, input_data: list[dict], is_build: bool = False\n    ) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the filter operation on the input data.\n\n        Args:\n            input_data (list[dict]): A list of dictionaries to process.\n            is_build (bool): Whether the operation is being executed in the build phase. Defaults to False.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the filtered list of dictionaries\n            and the total cost of the operation.\n        \"\"\"\n        previous_state = self._filter_is_build\n        self._filter_is_build = is_build\n        try:\n            return super().execute(input_data)\n        finally:\n            self._filter_is_build = previous_state\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.filter.FilterOperation.execute","title":"<code>execute(input_data, is_build=False)</code>","text":"<p>Executes the filter operation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>A list of dictionaries to process.</p> required <code>is_build</code> <code>bool</code> <p>Whether the operation is being executed in the build phase. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>tuple[list[dict], float]: A tuple containing the filtered list of dictionaries</p> <code>float</code> <p>and the total cost of the operation.</p> Source code in <code>docetl/operations/filter.py</code> <pre><code>def execute(\n    self, input_data: list[dict], is_build: bool = False\n) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the filter operation on the input data.\n\n    Args:\n        input_data (list[dict]): A list of dictionaries to process.\n        is_build (bool): Whether the operation is being executed in the build phase. Defaults to False.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the filtered list of dictionaries\n        and the total cost of the operation.\n    \"\"\"\n    previous_state = self._filter_is_build\n    self._filter_is_build = is_build\n    try:\n        return super().execute(input_data)\n    finally:\n        self._filter_is_build = previous_state\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation","title":"<code>docetl.operations.equijoin.EquijoinOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>class EquijoinOperation(BaseOperation):\n    class schema(BaseOperation.schema):\n        type: str = \"equijoin\"\n        comparison_prompt: str\n        output: dict[str, Any] | None = None\n        blocking_threshold: float | None = None\n        blocking_target_recall: float | None = None\n        blocking_conditions: list[str] | None = None\n        limits: dict[str, int] | None = None\n        comparison_model: str | None = None\n        optimize: bool | None = None\n        embedding_model: str | None = None\n        embedding_batch_size: int | None = None\n        compare_batch_size: int | None = None\n        limit_comparisons: int | None = None\n        blocking_keys: dict[str, list[str]] | None = None\n        timeout: int | None = None\n        litellm_completion_kwargs: dict[str, Any] = {}\n\n        @field_validator(\"blocking_keys\")\n        def validate_blocking_keys(cls, v):\n            if v is not None:\n                if \"left\" not in v or \"right\" not in v:\n                    raise ValueError(\n                        \"Both 'left' and 'right' must be specified in 'blocking_keys'\"\n                    )\n            return v\n\n        @field_validator(\"limits\")\n        def validate_limits(cls, v):\n            if v is not None:\n                if \"left\" not in v or \"right\" not in v:\n                    raise ValueError(\n                        \"Both 'left' and 'right' must be specified in 'limits'\"\n                    )\n            return v\n\n        @field_validator(\"comparison_prompt\")\n        def validate_comparison_prompt(cls, v):\n            # Check if it has Jinja syntax\n            if not has_jinja_syntax(v):\n                # This will be handled during initialization with user confirmation\n                return v\n            # If it has Jinja syntax, validate it's a valid template\n            from jinja2 import Template\n\n            try:\n                Template(v)\n            except Exception as e:\n                raise ValueError(\n                    f\"Invalid Jinja2 template in 'comparison_prompt': {str(e)}\"\n                )\n            return v\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Check for non-Jinja prompts and prompt user for confirmation\n        if \"comparison_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"comparison_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"comparison_prompt\"],\n                self.config[\"name\"],\n                \"comparison_prompt\",\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your comparison_prompt.\"\n                )\n            # Mark that we need to append document statement\n            # Note: equijoin uses left and right, so we'll handle it in strict_render\n            self.config[\"_append_document_to_comparison_prompt\"] = True\n\n    def compare_pair(\n        self,\n        comparison_prompt: str,\n        model: str,\n        item1: dict,\n        item2: dict,\n        timeout_seconds: int = 120,\n        max_retries_per_timeout: int = 2,\n    ) -&gt; tuple[bool, float]:\n        \"\"\"\n        Compares two items using an LLM model to determine if they match.\n\n        Args:\n            comparison_prompt (str): The prompt template for comparison.\n            model (str): The LLM model to use for comparison.\n            item1 (dict): The first item to compare.\n            item2 (dict): The second item to compare.\n            timeout_seconds (int): The timeout for the LLM call in seconds.\n            max_retries_per_timeout (int): The maximum number of retries per timeout.\n\n        Returns:\n            tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.\n        \"\"\"\n\n        try:\n            prompt = strict_render(comparison_prompt, {\"left\": item1, \"right\": item2})\n        except Exception as e:\n            self.console.log(f\"[red]Error rendering prompt: {e}[/red]\")\n            return False, 0\n        response = self.runner.api.call_llm(\n            model,\n            \"compare\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            {\"is_match\": \"bool\"},\n            timeout_seconds=timeout_seconds,\n            max_retries_per_timeout=max_retries_per_timeout,\n            bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n            litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n            op_config=self.config,\n        )\n        cost = 0\n        try:\n            cost = response.total_cost\n            output = self.runner.api.parse_llm_response(\n                response.response, {\"is_match\": \"bool\"}\n            )[0]\n        except Exception as e:\n            self.console.log(f\"[red]Error parsing LLM response: {e}[/red]\")\n            return False, cost\n        return output[\"is_match\"], cost\n\n    def execute(\n        self, left_data: list[dict], right_data: list[dict]\n    ) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the equijoin operation on the provided datasets.\n\n        Args:\n            left_data (list[dict]): The left dataset to join.\n            right_data (list[dict]): The right dataset to join.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the joined results and the total cost of the operation.\n\n        Usage:\n        ```python\n        from docetl.operations import EquijoinOperation\n\n        config = {\n            \"blocking_keys\": {\n                \"left\": [\"id\"],\n                \"right\": [\"user_id\"]\n            },\n            \"limits\": {\n                \"left\": 1,\n                \"right\": 1\n            },\n            \"comparison_prompt\": \"Compare {{left}} and {{right}} and determine if they match.\",\n            \"blocking_threshold\": 0.8,\n            \"blocking_conditions\": [\"left['id'] == right['user_id']\"],\n            \"limit_comparisons\": 1000\n        }\n        equijoin_op = EquijoinOperation(config)\n        left_data = [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n        right_data = [{\"user_id\": 1, \"age\": 30}, {\"user_id\": 2, \"age\": 25}]\n        results, cost = equijoin_op.execute(left_data, right_data)\n        print(f\"Joined results: {results}\")\n        print(f\"Total cost: {cost}\")\n        ```\n\n        This method performs the following steps:\n        1. Initial blocking based on specified conditions (if any)\n        2. Embedding-based blocking (if threshold is provided)\n        3. LLM-based comparison for blocked pairs\n        4. Result aggregation and validation\n\n        The method also calculates and logs statistics such as comparisons saved by blocking and join selectivity.\n        \"\"\"\n\n        blocking_keys = self.config.get(\"blocking_keys\", {})\n        left_keys = blocking_keys.get(\n            \"left\", list(left_data[0].keys()) if left_data else []\n        )\n        right_keys = blocking_keys.get(\n            \"right\", list(right_data[0].keys()) if right_data else []\n        )\n        limits = self.config.get(\n            \"limits\", {\"left\": float(\"inf\"), \"right\": float(\"inf\")}\n        )\n        left_limit = limits[\"left\"]\n        right_limit = limits[\"right\"]\n        blocking_threshold = self.config.get(\"blocking_threshold\")\n        blocking_conditions = self.config.get(\"blocking_conditions\", [])\n        limit_comparisons = self.config.get(\"limit_comparisons\")\n        total_cost = 0\n\n        if len(left_data) == 0 or len(right_data) == 0:\n            return [], 0\n\n        if self.status:\n            self.status.stop()\n\n        # Track pre-computed embeddings from auto-optimization\n        precomputed_left_embeddings = None\n        precomputed_right_embeddings = None\n\n        # Auto-compute blocking threshold if no blocking configuration is provided\n        if not blocking_threshold and not blocking_conditions and not limit_comparisons:\n            # Get target recall from operation config (default 0.95)\n            target_recall = self.config.get(\"blocking_target_recall\", 0.95)\n            self.console.log(\n                f\"[yellow]No blocking configuration. Auto-computing threshold (target recall: {target_recall:.0%})...[/yellow]\"\n            )\n\n            # Create comparison function for threshold optimization\n            def compare_fn_for_optimization(left_item, right_item):\n                return self.compare_pair(\n                    self.config[\"comparison_prompt\"],\n                    self.config.get(\"comparison_model\", self.default_model),\n                    left_item,\n                    right_item,\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                )\n\n            # Run threshold optimization\n            optimizer = RuntimeBlockingOptimizer(\n                runner=self.runner,\n                config=self.config,\n                default_model=self.default_model,\n                max_threads=self.max_threads,\n                console=self.console,\n                target_recall=target_recall,\n                sample_size=min(100, len(left_data) * len(right_data) // 4),\n            )\n            (\n                blocking_threshold,\n                precomputed_left_embeddings,\n                precomputed_right_embeddings,\n                optimization_cost,\n            ) = optimizer.optimize_equijoin(\n                left_data,\n                right_data,\n                compare_fn_for_optimization,\n                left_keys=left_keys,\n                right_keys=right_keys,\n            )\n            total_cost += optimization_cost\n            self.console.log(\n                f\"[green]Using auto-computed blocking threshold: {blocking_threshold}[/green]\"\n            )\n\n        # Initial blocking using multiprocessing\n        num_processes = min(cpu_count(), len(left_data))\n\n        self.console.log(\n            f\"Starting to run code-based blocking rules for {len(left_data)} left and {len(right_data)} right rows ({len(left_data) * len(right_data)} total pairs) with {num_processes} processes...\"\n        )\n\n        with Pool(\n            processes=num_processes,\n            initializer=init_worker,\n            initargs=(right_data, blocking_conditions),\n        ) as pool:\n            blocked_pairs_nested = pool.map(process_left_item, left_data)\n\n        # Flatten the nested list of blocked pairs\n        blocked_pairs = [pair for sublist in blocked_pairs_nested for pair in sublist]\n\n        # Check if we have exceeded the pairwise comparison limit\n        if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n            # Sample pairs based on cardinality and length\n            sampled_pairs = stratified_length_sample(\n                blocked_pairs, limit_comparisons, sample_size=1000, console=self.console\n            )\n\n            # Calculate number of dropped pairs\n            dropped_pairs = len(blocked_pairs) - limit_comparisons\n\n            # Prompt the user for confirmation\n            if self.status:\n                self.status.stop()\n            if not Confirm.ask(\n                f\"[yellow]Warning: {dropped_pairs} pairs will be dropped due to the comparison limit. \"\n                f\"Proceeding with {limit_comparisons} randomly sampled pairs. \"\n                f\"Do you want to continue?[/yellow]\",\n                console=self.console,\n            ):\n                raise ValueError(\"Operation cancelled by user due to pair limit.\")\n\n            if self.status:\n                self.status.start()\n\n            blocked_pairs = sampled_pairs\n\n        self.console.log(\n            f\"Number of blocked pairs after initial blocking: {len(blocked_pairs)}\"\n        )\n\n        if blocking_threshold is not None:\n            # Use precomputed embeddings if available from auto-optimization\n            if (\n                precomputed_left_embeddings is not None\n                and precomputed_right_embeddings is not None\n            ):\n                left_embeddings = precomputed_left_embeddings\n                right_embeddings = precomputed_right_embeddings\n            else:\n                embedding_model = self.config.get(\"embedding_model\", self.default_model)\n                model_input_context_length = model_cost.get(embedding_model, {}).get(\n                    \"max_input_tokens\", 8192\n                )\n                batch_size = 2000\n\n                def get_embeddings(\n                    input_data: list[dict[str, Any]], keys: list[str], name: str\n                ) -&gt; tuple[list[list[float]], float]:\n                    texts = [\n                        \" \".join(str(item[key]) for key in keys if key in item)[\n                            : model_input_context_length * 4\n                        ]\n                        for item in input_data\n                    ]\n                    embeddings = []\n                    embedding_cost = 0\n                    num_batches = (len(texts) + batch_size - 1) // batch_size\n\n                    for batch_idx, i in enumerate(range(0, len(texts), batch_size)):\n                        batch = texts[i : i + batch_size]\n                        if num_batches &gt; 1:\n                            self.console.log(\n                                f\"[dim]Creating {name} embeddings: batch {batch_idx + 1}/{num_batches} \"\n                                f\"({min(i + batch_size, len(texts))}/{len(texts)} items)[/dim]\"\n                            )\n                        response = self.runner.api.gen_embedding(\n                            model=embedding_model,\n                            input=batch,\n                        )\n                        embeddings.extend(\n                            [data[\"embedding\"] for data in response[\"data\"]]\n                        )\n                        embedding_cost += completion_cost(response)\n                    return embeddings, embedding_cost\n\n                self.console.log(\n                    f\"[cyan]Creating embeddings for {len(left_data)} left + {len(right_data)} right items...[/cyan]\"\n                )\n                left_embeddings, left_cost = get_embeddings(\n                    left_data, left_keys, \"left\"\n                )\n                right_embeddings, right_cost = get_embeddings(\n                    right_data, right_keys, \"right\"\n                )\n                total_cost += left_cost + right_cost\n\n            # Compute all cosine similarities in one call\n            from sklearn.metrics.pairwise import cosine_similarity\n\n            similarities = cosine_similarity(left_embeddings, right_embeddings)\n\n            # Additional blocking based on embeddings\n            # Find indices where similarity is above threshold\n            above_threshold = np.argwhere(similarities &gt;= blocking_threshold)\n            self.console.log(\n                f\"There are {above_threshold.shape[0]} pairs above the threshold.\"\n            )\n            block_pair_set = set(\n                (get_hashable_key(left_item), get_hashable_key(right_item))\n                for left_item, right_item in blocked_pairs\n            )\n\n            # If limit_comparisons is set, take only the top pairs\n            if limit_comparisons is not None:\n                # First, get all pairs above threshold\n                above_threshold_pairs = [(int(i), int(j)) for i, j in above_threshold]\n\n                # Sort these pairs by their similarity scores\n                sorted_pairs = sorted(\n                    above_threshold_pairs,\n                    key=lambda pair: similarities[pair[0], pair[1]],\n                    reverse=True,\n                )\n\n                # Take the top 'limit_comparisons' pairs\n                top_pairs = sorted_pairs[:limit_comparisons]\n\n                # Create new blocked_pairs based on top similarities and existing blocked pairs\n                new_blocked_pairs = []\n                remaining_limit = limit_comparisons - len(blocked_pairs)\n\n                # First, include all existing blocked pairs\n                final_blocked_pairs = blocked_pairs.copy()\n\n                # Then, add new pairs from top similarities until we reach the limit\n                for i, j in top_pairs:\n                    if remaining_limit &lt;= 0:\n                        break\n                    left_item, right_item = left_data[i], right_data[j]\n                    left_key = get_hashable_key(left_item)\n                    right_key = get_hashable_key(right_item)\n                    if (left_key, right_key) not in block_pair_set:\n                        new_blocked_pairs.append((left_item, right_item))\n                        block_pair_set.add((left_key, right_key))\n                        remaining_limit -= 1\n\n                final_blocked_pairs.extend(new_blocked_pairs)\n                blocked_pairs = final_blocked_pairs\n\n                self.console.log(\n                    f\"Limited comparisons to top {limit_comparisons} pairs, including {len(blocked_pairs) - len(new_blocked_pairs)} from code-based blocking and {len(new_blocked_pairs)} based on cosine similarity. Lowest cosine similarity included: {similarities[top_pairs[-1]]:.4f}\"\n                )\n            else:\n                # Add new pairs to blocked_pairs\n                for i, j in above_threshold:\n                    left_item, right_item = left_data[i], right_data[j]\n                    left_key = get_hashable_key(left_item)\n                    right_key = get_hashable_key(right_item)\n                    if (left_key, right_key) not in block_pair_set:\n                        blocked_pairs.append((left_item, right_item))\n                        block_pair_set.add((left_key, right_key))\n\n        # If there are no blocking conditions or embedding threshold, use all pairs\n        if not blocking_conditions and blocking_threshold is None:\n            blocked_pairs = [\n                (left_item, right_item)\n                for left_item in left_data\n                for right_item in right_data\n            ]\n\n        # If there's a limit on the number of comparisons, randomly sample pairs\n        if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n            self.console.log(\n                f\"Randomly sampling {limit_comparisons} pairs out of {len(blocked_pairs)} blocked pairs.\"\n            )\n            blocked_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n        self.console.log(\n            f\"Total pairs to compare after blocking and sampling: {len(blocked_pairs)}\"\n        )\n\n        # Calculate and print statistics\n        total_possible_comparisons = len(left_data) * len(right_data)\n        comparisons_made = len(blocked_pairs)\n        comparisons_saved = total_possible_comparisons - comparisons_made\n        self.console.log(\n            f\"[green]Comparisons saved by blocking: {comparisons_saved} \"\n            f\"({(comparisons_saved / total_possible_comparisons) * 100:.2f}%)[/green]\"\n        )\n\n        left_match_counts = defaultdict(int)\n        right_match_counts = defaultdict(int)\n        results = []\n        comparison_costs = 0\n\n        if self.status:\n            self.status.stop()\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            future_to_pair = {\n                executor.submit(\n                    self.compare_pair,\n                    self.config[\"comparison_prompt\"],\n                    self.config.get(\"comparison_model\", self.default_model),\n                    left,\n                    right,\n                    self.config.get(\"timeout\", 120),\n                    self.config.get(\"max_retries_per_timeout\", 2),\n                ): (left, right)\n                for left, right in blocked_pairs\n            }\n\n            pbar = RichLoopBar(\n                range(len(future_to_pair)),\n                desc=\"Comparing pairs\",\n                console=self.console,\n            )\n\n            for i in pbar:\n                future = list(future_to_pair.keys())[i]\n                pair = future_to_pair[future]\n                is_match, cost = future.result()\n                comparison_costs += cost\n\n                if is_match:\n                    joined_item = {}\n                    left_item, right_item = pair\n                    left_key_hash = get_hashable_key(left_item)\n                    right_key_hash = get_hashable_key(right_item)\n                    if (\n                        left_match_counts[left_key_hash] &gt;= left_limit\n                        or right_match_counts[right_key_hash] &gt;= right_limit\n                    ):\n                        continue\n\n                    for key, value in left_item.items():\n                        joined_item[f\"{key}_left\" if key in right_item else key] = value\n                    for key, value in right_item.items():\n                        joined_item[f\"{key}_right\" if key in left_item else key] = value\n                    if self.runner.api.validate_output(\n                        self.config, joined_item, self.console\n                    ):\n                        results.append(joined_item)\n                        left_match_counts[left_key_hash] += 1\n                        right_match_counts[right_key_hash] += 1\n\n                    # TODO: support retry in validation failure\n\n        total_cost += comparison_costs\n\n        if self.status:\n            self.status.start()\n\n        # Calculate and print the join selectivity\n        join_selectivity = (\n            len(results) / (len(left_data) * len(right_data))\n            if len(left_data) * len(right_data) &gt; 0\n            else 0\n        )\n        self.console.log(f\"Equijoin selectivity: {join_selectivity:.4f}\")\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation.compare_pair","title":"<code>compare_pair(comparison_prompt, model, item1, item2, timeout_seconds=120, max_retries_per_timeout=2)</code>","text":"<p>Compares two items using an LLM model to determine if they match.</p> <p>Parameters:</p> Name Type Description Default <code>comparison_prompt</code> <code>str</code> <p>The prompt template for comparison.</p> required <code>model</code> <code>str</code> <p>The LLM model to use for comparison.</p> required <code>item1</code> <code>dict</code> <p>The first item to compare.</p> required <code>item2</code> <code>dict</code> <p>The second item to compare.</p> required <code>timeout_seconds</code> <code>int</code> <p>The timeout for the LLM call in seconds.</p> <code>120</code> <code>max_retries_per_timeout</code> <code>int</code> <p>The maximum number of retries per timeout.</p> <code>2</code> <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.</p> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>def compare_pair(\n    self,\n    comparison_prompt: str,\n    model: str,\n    item1: dict,\n    item2: dict,\n    timeout_seconds: int = 120,\n    max_retries_per_timeout: int = 2,\n) -&gt; tuple[bool, float]:\n    \"\"\"\n    Compares two items using an LLM model to determine if they match.\n\n    Args:\n        comparison_prompt (str): The prompt template for comparison.\n        model (str): The LLM model to use for comparison.\n        item1 (dict): The first item to compare.\n        item2 (dict): The second item to compare.\n        timeout_seconds (int): The timeout for the LLM call in seconds.\n        max_retries_per_timeout (int): The maximum number of retries per timeout.\n\n    Returns:\n        tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.\n    \"\"\"\n\n    try:\n        prompt = strict_render(comparison_prompt, {\"left\": item1, \"right\": item2})\n    except Exception as e:\n        self.console.log(f\"[red]Error rendering prompt: {e}[/red]\")\n        return False, 0\n    response = self.runner.api.call_llm(\n        model,\n        \"compare\",\n        [{\"role\": \"user\", \"content\": prompt}],\n        {\"is_match\": \"bool\"},\n        timeout_seconds=timeout_seconds,\n        max_retries_per_timeout=max_retries_per_timeout,\n        bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n        litellm_completion_kwargs=self.config.get(\"litellm_completion_kwargs\", {}),\n        op_config=self.config,\n    )\n    cost = 0\n    try:\n        cost = response.total_cost\n        output = self.runner.api.parse_llm_response(\n            response.response, {\"is_match\": \"bool\"}\n        )[0]\n    except Exception as e:\n        self.console.log(f\"[red]Error parsing LLM response: {e}[/red]\")\n        return False, cost\n    return output[\"is_match\"], cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation.execute","title":"<code>execute(left_data, right_data)</code>","text":"<p>Executes the equijoin operation on the provided datasets.</p> <p>Parameters:</p> Name Type Description Default <code>left_data</code> <code>list[dict]</code> <p>The left dataset to join.</p> required <code>right_data</code> <code>list[dict]</code> <p>The right dataset to join.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the joined results and the total cost of the operation.</p> <p>Usage: <pre><code>from docetl.operations import EquijoinOperation\n\nconfig = {\n    \"blocking_keys\": {\n        \"left\": [\"id\"],\n        \"right\": [\"user_id\"]\n    },\n    \"limits\": {\n        \"left\": 1,\n        \"right\": 1\n    },\n    \"comparison_prompt\": \"Compare {{left}} and {{right}} and determine if they match.\",\n    \"blocking_threshold\": 0.8,\n    \"blocking_conditions\": [\"left['id'] == right['user_id']\"],\n    \"limit_comparisons\": 1000\n}\nequijoin_op = EquijoinOperation(config)\nleft_data = [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\nright_data = [{\"user_id\": 1, \"age\": 30}, {\"user_id\": 2, \"age\": 25}]\nresults, cost = equijoin_op.execute(left_data, right_data)\nprint(f\"Joined results: {results}\")\nprint(f\"Total cost: {cost}\")\n</code></pre></p> <p>This method performs the following steps: 1. Initial blocking based on specified conditions (if any) 2. Embedding-based blocking (if threshold is provided) 3. LLM-based comparison for blocked pairs 4. Result aggregation and validation</p> <p>The method also calculates and logs statistics such as comparisons saved by blocking and join selectivity.</p> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>def execute(\n    self, left_data: list[dict], right_data: list[dict]\n) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the equijoin operation on the provided datasets.\n\n    Args:\n        left_data (list[dict]): The left dataset to join.\n        right_data (list[dict]): The right dataset to join.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the joined results and the total cost of the operation.\n\n    Usage:\n    ```python\n    from docetl.operations import EquijoinOperation\n\n    config = {\n        \"blocking_keys\": {\n            \"left\": [\"id\"],\n            \"right\": [\"user_id\"]\n        },\n        \"limits\": {\n            \"left\": 1,\n            \"right\": 1\n        },\n        \"comparison_prompt\": \"Compare {{left}} and {{right}} and determine if they match.\",\n        \"blocking_threshold\": 0.8,\n        \"blocking_conditions\": [\"left['id'] == right['user_id']\"],\n        \"limit_comparisons\": 1000\n    }\n    equijoin_op = EquijoinOperation(config)\n    left_data = [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n    right_data = [{\"user_id\": 1, \"age\": 30}, {\"user_id\": 2, \"age\": 25}]\n    results, cost = equijoin_op.execute(left_data, right_data)\n    print(f\"Joined results: {results}\")\n    print(f\"Total cost: {cost}\")\n    ```\n\n    This method performs the following steps:\n    1. Initial blocking based on specified conditions (if any)\n    2. Embedding-based blocking (if threshold is provided)\n    3. LLM-based comparison for blocked pairs\n    4. Result aggregation and validation\n\n    The method also calculates and logs statistics such as comparisons saved by blocking and join selectivity.\n    \"\"\"\n\n    blocking_keys = self.config.get(\"blocking_keys\", {})\n    left_keys = blocking_keys.get(\n        \"left\", list(left_data[0].keys()) if left_data else []\n    )\n    right_keys = blocking_keys.get(\n        \"right\", list(right_data[0].keys()) if right_data else []\n    )\n    limits = self.config.get(\n        \"limits\", {\"left\": float(\"inf\"), \"right\": float(\"inf\")}\n    )\n    left_limit = limits[\"left\"]\n    right_limit = limits[\"right\"]\n    blocking_threshold = self.config.get(\"blocking_threshold\")\n    blocking_conditions = self.config.get(\"blocking_conditions\", [])\n    limit_comparisons = self.config.get(\"limit_comparisons\")\n    total_cost = 0\n\n    if len(left_data) == 0 or len(right_data) == 0:\n        return [], 0\n\n    if self.status:\n        self.status.stop()\n\n    # Track pre-computed embeddings from auto-optimization\n    precomputed_left_embeddings = None\n    precomputed_right_embeddings = None\n\n    # Auto-compute blocking threshold if no blocking configuration is provided\n    if not blocking_threshold and not blocking_conditions and not limit_comparisons:\n        # Get target recall from operation config (default 0.95)\n        target_recall = self.config.get(\"blocking_target_recall\", 0.95)\n        self.console.log(\n            f\"[yellow]No blocking configuration. Auto-computing threshold (target recall: {target_recall:.0%})...[/yellow]\"\n        )\n\n        # Create comparison function for threshold optimization\n        def compare_fn_for_optimization(left_item, right_item):\n            return self.compare_pair(\n                self.config[\"comparison_prompt\"],\n                self.config.get(\"comparison_model\", self.default_model),\n                left_item,\n                right_item,\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\n                    \"max_retries_per_timeout\", 2\n                ),\n            )\n\n        # Run threshold optimization\n        optimizer = RuntimeBlockingOptimizer(\n            runner=self.runner,\n            config=self.config,\n            default_model=self.default_model,\n            max_threads=self.max_threads,\n            console=self.console,\n            target_recall=target_recall,\n            sample_size=min(100, len(left_data) * len(right_data) // 4),\n        )\n        (\n            blocking_threshold,\n            precomputed_left_embeddings,\n            precomputed_right_embeddings,\n            optimization_cost,\n        ) = optimizer.optimize_equijoin(\n            left_data,\n            right_data,\n            compare_fn_for_optimization,\n            left_keys=left_keys,\n            right_keys=right_keys,\n        )\n        total_cost += optimization_cost\n        self.console.log(\n            f\"[green]Using auto-computed blocking threshold: {blocking_threshold}[/green]\"\n        )\n\n    # Initial blocking using multiprocessing\n    num_processes = min(cpu_count(), len(left_data))\n\n    self.console.log(\n        f\"Starting to run code-based blocking rules for {len(left_data)} left and {len(right_data)} right rows ({len(left_data) * len(right_data)} total pairs) with {num_processes} processes...\"\n    )\n\n    with Pool(\n        processes=num_processes,\n        initializer=init_worker,\n        initargs=(right_data, blocking_conditions),\n    ) as pool:\n        blocked_pairs_nested = pool.map(process_left_item, left_data)\n\n    # Flatten the nested list of blocked pairs\n    blocked_pairs = [pair for sublist in blocked_pairs_nested for pair in sublist]\n\n    # Check if we have exceeded the pairwise comparison limit\n    if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n        # Sample pairs based on cardinality and length\n        sampled_pairs = stratified_length_sample(\n            blocked_pairs, limit_comparisons, sample_size=1000, console=self.console\n        )\n\n        # Calculate number of dropped pairs\n        dropped_pairs = len(blocked_pairs) - limit_comparisons\n\n        # Prompt the user for confirmation\n        if self.status:\n            self.status.stop()\n        if not Confirm.ask(\n            f\"[yellow]Warning: {dropped_pairs} pairs will be dropped due to the comparison limit. \"\n            f\"Proceeding with {limit_comparisons} randomly sampled pairs. \"\n            f\"Do you want to continue?[/yellow]\",\n            console=self.console,\n        ):\n            raise ValueError(\"Operation cancelled by user due to pair limit.\")\n\n        if self.status:\n            self.status.start()\n\n        blocked_pairs = sampled_pairs\n\n    self.console.log(\n        f\"Number of blocked pairs after initial blocking: {len(blocked_pairs)}\"\n    )\n\n    if blocking_threshold is not None:\n        # Use precomputed embeddings if available from auto-optimization\n        if (\n            precomputed_left_embeddings is not None\n            and precomputed_right_embeddings is not None\n        ):\n            left_embeddings = precomputed_left_embeddings\n            right_embeddings = precomputed_right_embeddings\n        else:\n            embedding_model = self.config.get(\"embedding_model\", self.default_model)\n            model_input_context_length = model_cost.get(embedding_model, {}).get(\n                \"max_input_tokens\", 8192\n            )\n            batch_size = 2000\n\n            def get_embeddings(\n                input_data: list[dict[str, Any]], keys: list[str], name: str\n            ) -&gt; tuple[list[list[float]], float]:\n                texts = [\n                    \" \".join(str(item[key]) for key in keys if key in item)[\n                        : model_input_context_length * 4\n                    ]\n                    for item in input_data\n                ]\n                embeddings = []\n                embedding_cost = 0\n                num_batches = (len(texts) + batch_size - 1) // batch_size\n\n                for batch_idx, i in enumerate(range(0, len(texts), batch_size)):\n                    batch = texts[i : i + batch_size]\n                    if num_batches &gt; 1:\n                        self.console.log(\n                            f\"[dim]Creating {name} embeddings: batch {batch_idx + 1}/{num_batches} \"\n                            f\"({min(i + batch_size, len(texts))}/{len(texts)} items)[/dim]\"\n                        )\n                    response = self.runner.api.gen_embedding(\n                        model=embedding_model,\n                        input=batch,\n                    )\n                    embeddings.extend(\n                        [data[\"embedding\"] for data in response[\"data\"]]\n                    )\n                    embedding_cost += completion_cost(response)\n                return embeddings, embedding_cost\n\n            self.console.log(\n                f\"[cyan]Creating embeddings for {len(left_data)} left + {len(right_data)} right items...[/cyan]\"\n            )\n            left_embeddings, left_cost = get_embeddings(\n                left_data, left_keys, \"left\"\n            )\n            right_embeddings, right_cost = get_embeddings(\n                right_data, right_keys, \"right\"\n            )\n            total_cost += left_cost + right_cost\n\n        # Compute all cosine similarities in one call\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarities = cosine_similarity(left_embeddings, right_embeddings)\n\n        # Additional blocking based on embeddings\n        # Find indices where similarity is above threshold\n        above_threshold = np.argwhere(similarities &gt;= blocking_threshold)\n        self.console.log(\n            f\"There are {above_threshold.shape[0]} pairs above the threshold.\"\n        )\n        block_pair_set = set(\n            (get_hashable_key(left_item), get_hashable_key(right_item))\n            for left_item, right_item in blocked_pairs\n        )\n\n        # If limit_comparisons is set, take only the top pairs\n        if limit_comparisons is not None:\n            # First, get all pairs above threshold\n            above_threshold_pairs = [(int(i), int(j)) for i, j in above_threshold]\n\n            # Sort these pairs by their similarity scores\n            sorted_pairs = sorted(\n                above_threshold_pairs,\n                key=lambda pair: similarities[pair[0], pair[1]],\n                reverse=True,\n            )\n\n            # Take the top 'limit_comparisons' pairs\n            top_pairs = sorted_pairs[:limit_comparisons]\n\n            # Create new blocked_pairs based on top similarities and existing blocked pairs\n            new_blocked_pairs = []\n            remaining_limit = limit_comparisons - len(blocked_pairs)\n\n            # First, include all existing blocked pairs\n            final_blocked_pairs = blocked_pairs.copy()\n\n            # Then, add new pairs from top similarities until we reach the limit\n            for i, j in top_pairs:\n                if remaining_limit &lt;= 0:\n                    break\n                left_item, right_item = left_data[i], right_data[j]\n                left_key = get_hashable_key(left_item)\n                right_key = get_hashable_key(right_item)\n                if (left_key, right_key) not in block_pair_set:\n                    new_blocked_pairs.append((left_item, right_item))\n                    block_pair_set.add((left_key, right_key))\n                    remaining_limit -= 1\n\n            final_blocked_pairs.extend(new_blocked_pairs)\n            blocked_pairs = final_blocked_pairs\n\n            self.console.log(\n                f\"Limited comparisons to top {limit_comparisons} pairs, including {len(blocked_pairs) - len(new_blocked_pairs)} from code-based blocking and {len(new_blocked_pairs)} based on cosine similarity. Lowest cosine similarity included: {similarities[top_pairs[-1]]:.4f}\"\n            )\n        else:\n            # Add new pairs to blocked_pairs\n            for i, j in above_threshold:\n                left_item, right_item = left_data[i], right_data[j]\n                left_key = get_hashable_key(left_item)\n                right_key = get_hashable_key(right_item)\n                if (left_key, right_key) not in block_pair_set:\n                    blocked_pairs.append((left_item, right_item))\n                    block_pair_set.add((left_key, right_key))\n\n    # If there are no blocking conditions or embedding threshold, use all pairs\n    if not blocking_conditions and blocking_threshold is None:\n        blocked_pairs = [\n            (left_item, right_item)\n            for left_item in left_data\n            for right_item in right_data\n        ]\n\n    # If there's a limit on the number of comparisons, randomly sample pairs\n    if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n        self.console.log(\n            f\"Randomly sampling {limit_comparisons} pairs out of {len(blocked_pairs)} blocked pairs.\"\n        )\n        blocked_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n    self.console.log(\n        f\"Total pairs to compare after blocking and sampling: {len(blocked_pairs)}\"\n    )\n\n    # Calculate and print statistics\n    total_possible_comparisons = len(left_data) * len(right_data)\n    comparisons_made = len(blocked_pairs)\n    comparisons_saved = total_possible_comparisons - comparisons_made\n    self.console.log(\n        f\"[green]Comparisons saved by blocking: {comparisons_saved} \"\n        f\"({(comparisons_saved / total_possible_comparisons) * 100:.2f}%)[/green]\"\n    )\n\n    left_match_counts = defaultdict(int)\n    right_match_counts = defaultdict(int)\n    results = []\n    comparison_costs = 0\n\n    if self.status:\n        self.status.stop()\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        future_to_pair = {\n            executor.submit(\n                self.compare_pair,\n                self.config[\"comparison_prompt\"],\n                self.config.get(\"comparison_model\", self.default_model),\n                left,\n                right,\n                self.config.get(\"timeout\", 120),\n                self.config.get(\"max_retries_per_timeout\", 2),\n            ): (left, right)\n            for left, right in blocked_pairs\n        }\n\n        pbar = RichLoopBar(\n            range(len(future_to_pair)),\n            desc=\"Comparing pairs\",\n            console=self.console,\n        )\n\n        for i in pbar:\n            future = list(future_to_pair.keys())[i]\n            pair = future_to_pair[future]\n            is_match, cost = future.result()\n            comparison_costs += cost\n\n            if is_match:\n                joined_item = {}\n                left_item, right_item = pair\n                left_key_hash = get_hashable_key(left_item)\n                right_key_hash = get_hashable_key(right_item)\n                if (\n                    left_match_counts[left_key_hash] &gt;= left_limit\n                    or right_match_counts[right_key_hash] &gt;= right_limit\n                ):\n                    continue\n\n                for key, value in left_item.items():\n                    joined_item[f\"{key}_left\" if key in right_item else key] = value\n                for key, value in right_item.items():\n                    joined_item[f\"{key}_right\" if key in left_item else key] = value\n                if self.runner.api.validate_output(\n                    self.config, joined_item, self.console\n                ):\n                    results.append(joined_item)\n                    left_match_counts[left_key_hash] += 1\n                    right_match_counts[right_key_hash] += 1\n\n                # TODO: support retry in validation failure\n\n    total_cost += comparison_costs\n\n    if self.status:\n        self.status.start()\n\n    # Calculate and print the join selectivity\n    join_selectivity = (\n        len(results) / (len(left_data) * len(right_data))\n        if len(left_data) * len(right_data) &gt; 0\n        else 0\n    )\n    self.console.log(f\"Equijoin selectivity: {join_selectivity:.4f}\")\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.cluster.ClusterOperation","title":"<code>docetl.operations.cluster.ClusterOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/cluster.py</code> <pre><code>class ClusterOperation(BaseOperation):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.max_batch_size: int = self.config.get(\n            \"max_batch_size\", kwargs.get(\"max_batch_size\", float(\"inf\"))\n        )\n        # Check for non-Jinja prompts and prompt user for confirmation\n        if \"summary_prompt\" in self.config and not has_jinja_syntax(\n            self.config[\"summary_prompt\"]\n        ):\n            if not prompt_user_for_non_jinja_confirmation(\n                self.config[\"summary_prompt\"], self.config[\"name\"], \"summary_prompt\"\n            ):\n                raise ValueError(\n                    f\"Operation '{self.config['name']}' cancelled by user. Please add Jinja2 template syntax to your summary_prompt.\"\n                )\n            # Mark that we need to append document statement (cluster uses inputs)\n            self.config[\"_append_document_to_prompt\"] = True\n            self.config[\"_is_reduce_operation\"] = True\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks the configuration of the ClusterOperation for required keys and valid structure.\n\n        Raises:\n            ValueError: If required keys are missing or invalid in the configuration.\n            TypeError: If configuration values have incorrect types.\n        \"\"\"\n        required_keys = [\"embedding_keys\", \"summary_schema\", \"summary_prompt\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in ClusterOperation configuration\"\n                )\n\n        if not isinstance(self.config[\"embedding_keys\"], list):\n            raise TypeError(\"'embedding_keys' must be a list of strings\")\n\n        if \"output_key\" in self.config:\n            if not isinstance(self.config[\"output_key\"], str):\n                raise TypeError(\"'output_key' must be a string\")\n\n        if not isinstance(self.config[\"summary_schema\"], dict):\n            raise TypeError(\"'summary_schema' must be a dictionary\")\n\n        if not isinstance(self.config[\"summary_prompt\"], str):\n            raise TypeError(\"'prompt' must be a string\")\n\n        # Check if the prompt has Jinja syntax\n        if not has_jinja_syntax(self.config[\"summary_prompt\"]):\n            # This will be handled during initialization with user confirmation\n            pass\n        else:\n            # Check if the prompt is a valid Jinja2 template\n            try:\n                Template(self.config[\"summary_prompt\"])\n            except Exception as e:\n                raise ValueError(f\"Invalid Jinja2 template in 'prompt': {str(e)}\")\n\n        # Check optional parameters\n        if \"max_batch_size\" in self.config:\n            if not isinstance(self.config[\"max_batch_size\"], int):\n                raise TypeError(\"'max_batch_size' must be an integer\")\n\n        if \"embedding_model\" in self.config:\n            if not isinstance(self.config[\"embedding_model\"], str):\n                raise TypeError(\"'embedding_model' must be a string\")\n\n        if \"model\" in self.config:\n            if not isinstance(self.config[\"model\"], str):\n                raise TypeError(\"'model' must be a string\")\n\n        if \"validate\" in self.config:\n            if not isinstance(self.config[\"validate\"], list):\n                raise TypeError(\"'validate' must be a list of strings\")\n            for rule in self.config[\"validate\"]:\n                if not isinstance(rule, str):\n                    raise TypeError(\"Each validation rule must be a string\")\n\n    def execute(\n        self, input_data: list[dict], is_build: bool = False\n    ) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the cluster operation on the input data. Modifies the\n        input data and returns it in place.\n\n        Args:\n            input_data (list[dict]): A list of dictionaries to process.\n            is_build (bool): Whether the operation is being executed\n              in the build phase. Defaults to False.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the clustered\n              list of dictionaries and the total cost of the operation.\n        \"\"\"\n        if not input_data:\n            return input_data, 0\n\n        if len(input_data) == 1:\n            input_data[0][self.config.get(\"output_key\", \"clusters\")] = ()\n            return input_data, 0\n\n        embeddings, cost = get_embeddings_for_clustering(\n            input_data, self.config, self.runner.api\n        )\n\n        tree = self.agglomerative_cluster_of_embeddings(input_data, embeddings)\n\n        if \"collapse\" in self.config:\n            tree = self.collapse_tree(tree, collapse=self.config[\"collapse\"])\n\n        self.prompt_template = Template(self.config[\"summary_prompt\"])\n        cost += self.annotate_clustering_tree(tree)\n        self.annotate_leaves(tree)\n\n        return input_data, cost\n\n    def agglomerative_cluster_of_embeddings(self, input_data, embeddings):\n        import sklearn.cluster\n\n        cl = sklearn.cluster.AgglomerativeClustering(\n            compute_full_tree=True, compute_distances=True\n        )\n        cl.fit(embeddings)\n\n        nsamples = len(embeddings)\n\n        def build_tree(i):\n            if i &lt; nsamples:\n                res = input_data[i]\n                #                res[\"embedding\"] = list(embeddings[i])\n                return res\n            return {\n                \"children\": [\n                    build_tree(cl.children_[i - nsamples, 0]),\n                    build_tree(cl.children_[i - nsamples, 1]),\n                ],\n                \"distance\": cl.distances_[i - nsamples],\n            }\n\n        return build_tree(nsamples + len(cl.children_) - 1)\n\n    def get_tree_distances(self, t):\n        res = set()\n        if \"distance\" in t:\n            res.update(\n                set(\n                    [\n                        t[\"distance\"] - child[\"distance\"]\n                        for child in t[\"children\"]\n                        if \"distance\" in child\n                    ]\n                )\n            )\n        if \"children\" in t:\n            for child in t[\"children\"]:\n                res.update(self.get_tree_distances(child))\n        return res\n\n    def _collapse_tree(self, t, parent_dist=None, collapse=None):\n        if \"children\" in t:\n            if (\n                \"distance\" in t\n                and parent_dist is not None\n                and collapse is not None\n                and parent_dist - t[\"distance\"] &lt; collapse\n            ):\n                return [\n                    grandchild\n                    for child in t[\"children\"]\n                    for grandchild in self._collapse_tree(\n                        child, parent_dist=parent_dist, collapse=collapse\n                    )\n                ]\n            else:\n                res = dict(t)\n                res[\"children\"] = [\n                    grandchild\n                    for idx, child in enumerate(t[\"children\"])\n                    for grandchild in self._collapse_tree(\n                        child, parent_dist=t[\"distance\"], collapse=collapse\n                    )\n                ]\n                return [res]\n        else:\n            return [t]\n\n    def collapse_tree(self, tree, collapse=None):\n        if collapse is not None:\n            tree_distances = np.array(sorted(self.get_tree_distances(tree)))\n            collapse = tree_distances[int(len(tree_distances) * collapse)]\n        return self._collapse_tree(tree, collapse=collapse)[0]\n\n    def annotate_clustering_tree(self, t):\n        if \"children\" in t:\n            with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\n                futures = [\n                    executor.submit(self.annotate_clustering_tree, child)\n                    for child in t[\"children\"]\n                ]\n\n                total_cost = 0\n                pbar = RichLoopBar(\n                    range(len(futures)),\n                    desc=f\"Processing {self.config['name']} (map) on all documents\",\n                    console=self.console,\n                )\n                for i in pbar:\n                    total_cost += futures[i].result()\n                    pbar.update(i)\n\n            prompt = strict_render(self.prompt_template, {\"inputs\": t[\"children\"]})\n\n            def validation_fn(response: dict[str, Any]):\n                output = self.runner.api.parse_llm_response(\n                    response,\n                    schema=self.config[\"summary_schema\"],\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                if self.runner.api.validate_output(self.config, output, self.console):\n                    return output, True\n                return output, False\n\n            response = self.runner.api.call_llm(\n                model=self.config.get(\"model\", self.default_model),\n                op_type=\"cluster\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                output_schema=self.config[\"summary_schema\"],\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                bypass_cache=self.config.get(\"bypass_cache\", self.bypass_cache),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                validation_config=(\n                    {\n                        \"num_retries\": self.num_retries_on_validate_failure,\n                        \"val_rule\": self.config.get(\"validate\", []),\n                        \"validation_fn\": validation_fn,\n                    }\n                    if self.config.get(\"validate\", None)\n                    else None\n                ),\n                verbose=self.config.get(\"verbose\", False),\n                litellm_completion_kwargs=self.config.get(\n                    \"litellm_completion_kwargs\", {}\n                ),\n                op_config=self.config,\n            )\n            total_cost += response.total_cost\n            if response.validated:\n                output = self.runner.api.parse_llm_response(\n                    response.response,\n                    schema=self.config[\"summary_schema\"],\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                t.update(output)\n\n            return total_cost\n        return 0\n\n    def annotate_leaves(self, tree, path=()):\n        if \"children\" in tree:\n            item = dict(tree)\n            item.pop(\"children\")\n            for child in tree[\"children\"]:\n                self.annotate_leaves(child, path=(item,) + path)\n        else:\n            tree[self.config.get(\"output_key\", \"clusters\")] = path\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.cluster.ClusterOperation.execute","title":"<code>execute(input_data, is_build=False)</code>","text":"<p>Executes the cluster operation on the input data. Modifies the input data and returns it in place.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>A list of dictionaries to process.</p> required <code>is_build</code> <code>bool</code> <p>Whether the operation is being executed in the build phase. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the clustered list of dictionaries and the total cost of the operation.</p> Source code in <code>docetl/operations/cluster.py</code> <pre><code>def execute(\n    self, input_data: list[dict], is_build: bool = False\n) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the cluster operation on the input data. Modifies the\n    input data and returns it in place.\n\n    Args:\n        input_data (list[dict]): A list of dictionaries to process.\n        is_build (bool): Whether the operation is being executed\n          in the build phase. Defaults to False.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the clustered\n          list of dictionaries and the total cost of the operation.\n    \"\"\"\n    if not input_data:\n        return input_data, 0\n\n    if len(input_data) == 1:\n        input_data[0][self.config.get(\"output_key\", \"clusters\")] = ()\n        return input_data, 0\n\n    embeddings, cost = get_embeddings_for_clustering(\n        input_data, self.config, self.runner.api\n    )\n\n    tree = self.agglomerative_cluster_of_embeddings(input_data, embeddings)\n\n    if \"collapse\" in self.config:\n        tree = self.collapse_tree(tree, collapse=self.config[\"collapse\"])\n\n    self.prompt_template = Template(self.config[\"summary_prompt\"])\n    cost += self.annotate_clustering_tree(tree)\n    self.annotate_leaves(tree)\n\n    return input_data, cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.cluster.ClusterOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks the configuration of the ClusterOperation for required keys and valid structure.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or invalid in the configuration.</p> <code>TypeError</code> <p>If configuration values have incorrect types.</p> Source code in <code>docetl/operations/cluster.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks the configuration of the ClusterOperation for required keys and valid structure.\n\n    Raises:\n        ValueError: If required keys are missing or invalid in the configuration.\n        TypeError: If configuration values have incorrect types.\n    \"\"\"\n    required_keys = [\"embedding_keys\", \"summary_schema\", \"summary_prompt\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in ClusterOperation configuration\"\n            )\n\n    if not isinstance(self.config[\"embedding_keys\"], list):\n        raise TypeError(\"'embedding_keys' must be a list of strings\")\n\n    if \"output_key\" in self.config:\n        if not isinstance(self.config[\"output_key\"], str):\n            raise TypeError(\"'output_key' must be a string\")\n\n    if not isinstance(self.config[\"summary_schema\"], dict):\n        raise TypeError(\"'summary_schema' must be a dictionary\")\n\n    if not isinstance(self.config[\"summary_prompt\"], str):\n        raise TypeError(\"'prompt' must be a string\")\n\n    # Check if the prompt has Jinja syntax\n    if not has_jinja_syntax(self.config[\"summary_prompt\"]):\n        # This will be handled during initialization with user confirmation\n        pass\n    else:\n        # Check if the prompt is a valid Jinja2 template\n        try:\n            Template(self.config[\"summary_prompt\"])\n        except Exception as e:\n            raise ValueError(f\"Invalid Jinja2 template in 'prompt': {str(e)}\")\n\n    # Check optional parameters\n    if \"max_batch_size\" in self.config:\n        if not isinstance(self.config[\"max_batch_size\"], int):\n            raise TypeError(\"'max_batch_size' must be an integer\")\n\n    if \"embedding_model\" in self.config:\n        if not isinstance(self.config[\"embedding_model\"], str):\n            raise TypeError(\"'embedding_model' must be a string\")\n\n    if \"model\" in self.config:\n        if not isinstance(self.config[\"model\"], str):\n            raise TypeError(\"'model' must be a string\")\n\n    if \"validate\" in self.config:\n        if not isinstance(self.config[\"validate\"], list):\n            raise TypeError(\"'validate' must be a list of strings\")\n        for rule in self.config[\"validate\"]:\n            if not isinstance(rule, str):\n                raise TypeError(\"Each validation rule must be a string\")\n</code></pre>"},{"location":"api-reference/operations/#auxiliary-operators","title":"Auxiliary Operators","text":""},{"location":"api-reference/operations/#docetl.operations.split.SplitOperation","title":"<code>docetl.operations.split.SplitOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that implements a split operation on input data, dividing it into manageable chunks.</p> <p>This class extends BaseOperation to: 1. Split input data into chunks of specified size based on the 'split_key' and 'token_count' configuration. 2. Assign unique identifiers to each original document and number chunks sequentially. 3. Return results containing:    - {split_key}_chunk: The content of the split chunk.    - {name}_id: A unique identifier for each original document.    - {name}_chunk_num: The sequential number of the chunk within its original document.</p> Source code in <code>docetl/operations/split.py</code> <pre><code>class SplitOperation(BaseOperation):\n    \"\"\"\n    A class that implements a split operation on input data, dividing it into manageable chunks.\n\n    This class extends BaseOperation to:\n    1. Split input data into chunks of specified size based on the 'split_key' and 'token_count' configuration.\n    2. Assign unique identifiers to each original document and number chunks sequentially.\n    3. Return results containing:\n       - {split_key}_chunk: The content of the split chunk.\n       - {name}_id: A unique identifier for each original document.\n       - {name}_chunk_num: The sequential number of the chunk within its original document.\n    \"\"\"\n\n    class schema(BaseOperation.schema):\n        type: str = \"split\"\n        split_key: str\n        method: str\n        method_kwargs: dict[str, Any]\n        model: str | None = None\n\n        @field_validator(\"method\")\n        def validate_method(cls, v):\n            if v not in [\"token_count\", \"delimiter\"]:\n                raise ValueError(\n                    f\"Invalid method '{v}'. Must be 'token_count' or 'delimiter'\"\n                )\n            return v\n\n        @model_validator(mode=\"after\")\n        def validate_method_kwargs(self):\n            if self.method == \"token_count\":\n                num_tokens = self.method_kwargs.get(\"num_tokens\")\n                if num_tokens is None or num_tokens &lt;= 0:\n                    raise ValueError(\"'num_tokens' must be a positive integer\")\n            elif self.method == \"delimiter\":\n                if \"delimiter\" not in self.method_kwargs:\n                    raise ValueError(\"'delimiter' is required for delimiter method\")\n            return self\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = self.config[\"name\"]\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        split_key = self.config[\"split_key\"]\n        method = self.config[\"method\"]\n        method_kwargs = self.config[\"method_kwargs\"]\n        try:\n            encoder = tiktoken.encoding_for_model(\n                self.config[\"method_kwargs\"]\n                .get(\"model\", self.default_model)\n                .split(\"/\")[-1]\n            )\n        except Exception:\n            encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n\n        results = []\n        cost = 0.0\n\n        for item in input_data:\n            if split_key not in item:\n                raise KeyError(f\"Split key '{split_key}' not found in item\")\n\n            content = item[split_key]\n            doc_id = str(uuid.uuid4())\n\n            if method == \"token_count\":\n                token_count = method_kwargs[\"num_tokens\"]\n                tokens = encoder.encode(content)\n\n                for chunk_num, i in enumerate(\n                    range(0, len(tokens), token_count), start=1\n                ):\n                    chunk_tokens = tokens[i : i + token_count]\n                    chunk = encoder.decode(chunk_tokens)\n\n                    result = item.copy()\n                    result.update(\n                        {\n                            f\"{split_key}_chunk\": chunk,\n                            f\"{self.name}_id\": doc_id,\n                            f\"{self.name}_chunk_num\": chunk_num,\n                        }\n                    )\n                    results.append(result)\n\n            elif method == \"delimiter\":\n                delimiter = method_kwargs[\"delimiter\"]\n                num_splits_to_group = method_kwargs.get(\"num_splits_to_group\", 1)\n                chunks = content.split(delimiter)\n\n                # Get rid of empty chunks\n                chunks = [chunk for chunk in chunks if chunk.strip()]\n\n                for chunk_num, i in enumerate(\n                    range(0, len(chunks), num_splits_to_group), start=1\n                ):\n                    grouped_chunks = chunks[i : i + num_splits_to_group]\n                    joined_chunk = delimiter.join(grouped_chunks).strip()\n\n                    result = item.copy()\n                    result.update(\n                        {\n                            f\"{split_key}_chunk\": joined_chunk,\n                            f\"{self.name}_id\": doc_id,\n                            f\"{self.name}_chunk_num\": chunk_num,\n                        }\n                    )\n                    results.append(result)\n\n        return results, cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation","title":"<code>docetl.operations.gather.GatherOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that implements a gather operation on input data, adding contextual information from surrounding chunks.</p> <p>This class extends BaseOperation to: 1. Group chunks by their document ID. 2. Order chunks within each group. 3. Add peripheral context to each chunk based on the configuration. 4. Include headers for each chunk and its upward hierarchy. 5. Return results containing the rendered chunks with added context, including information about skipped characters and headers.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>class GatherOperation(BaseOperation):\n    \"\"\"\n    A class that implements a gather operation on input data, adding contextual information from surrounding chunks.\n\n    This class extends BaseOperation to:\n    1. Group chunks by their document ID.\n    2. Order chunks within each group.\n    3. Add peripheral context to each chunk based on the configuration.\n    4. Include headers for each chunk and its upward hierarchy.\n    5. Return results containing the rendered chunks with added context, including information about skipped characters and headers.\n    \"\"\"\n\n    class schema(BaseOperation.schema):\n        type: str = \"gather\"\n        content_key: str\n        doc_id_key: str\n        order_key: str\n        peripheral_chunks: dict[str, Any] | None = None\n        doc_header_key: str | None = None\n        main_chunk_start: str | None = None\n        main_chunk_end: str | None = None\n\n        @field_validator(\"peripheral_chunks\")\n        def validate_peripheral_chunks(cls, v):\n            for direction in [\"previous\", \"next\"]:\n                if direction not in v:\n                    continue\n                for section in [\"head\", \"middle\", \"tail\"]:\n                    if section in v[direction]:\n                        section_config = v[direction][section]\n                        if section != \"middle\" and \"count\" not in section_config:\n                            raise ValueError(\n                                f\"Missing 'count' in {direction}.{section} configuration\"\n                            )\n            return v\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the GatherOperation.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"Perform a syntax check on the operation configuration.\"\"\"\n        # Validate the schema using Pydantic\n        self.schema(**self.config)\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Execute the gather operation on the input data.\n\n        Args:\n            input_data (list[dict]): The input data to process.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the processed results and the cost of the operation.\n        \"\"\"\n        content_key = self.config[\"content_key\"]\n        doc_id_key = self.config[\"doc_id_key\"]\n        order_key = self.config[\"order_key\"]\n        peripheral_config = self.config.get(\"peripheral_chunks\", {})\n        main_chunk_start = self.config.get(\n            \"main_chunk_start\", \"--- Begin Main Chunk ---\"\n        )\n        main_chunk_end = self.config.get(\"main_chunk_end\", \"--- End Main Chunk ---\")\n        doc_header_key = self.config.get(\"doc_header_key\", None)\n        results = []\n        cost = 0.0\n\n        # Group chunks by document ID\n        grouped_chunks = {}\n        for item in input_data:\n            doc_id = item[doc_id_key]\n            if doc_id not in grouped_chunks:\n                grouped_chunks[doc_id] = []\n            grouped_chunks[doc_id].append(item)\n\n        # Process each group of chunks\n        for chunks in grouped_chunks.values():\n            # Sort chunks by their order within the document\n            chunks.sort(key=lambda x: x[order_key])\n\n            # Process each chunk with its peripheral context and headers\n            for i, chunk in enumerate(chunks):\n                rendered_chunk = self.render_chunk_with_context(\n                    chunks,\n                    i,\n                    peripheral_config,\n                    content_key,\n                    order_key,\n                    main_chunk_start,\n                    main_chunk_end,\n                    doc_header_key,\n                )\n\n                result = chunk.copy()\n                result[f\"{content_key}_rendered\"] = rendered_chunk\n                results.append(result)\n\n        return results, cost\n\n    def render_chunk_with_context(\n        self,\n        chunks: list[dict],\n        current_index: int,\n        peripheral_config: dict,\n        content_key: str,\n        order_key: str,\n        main_chunk_start: str,\n        main_chunk_end: str,\n        doc_header_key: str,\n    ) -&gt; str:\n        \"\"\"\n        Render a chunk with its peripheral context and headers.\n\n        Args:\n            chunks (list[dict]): List of all chunks in the document.\n            current_index (int): Index of the current chunk being processed.\n            peripheral_config (dict): Configuration for peripheral chunks.\n            content_key (str): Key for the content in each chunk.\n            order_key (str): Key for the order of each chunk.\n            main_chunk_start (str): String to mark the start of the main chunk.\n            main_chunk_end (str): String to mark the end of the main chunk.\n            doc_header_key (str): The key for the headers in the current chunk.\n\n        Returns:\n            str: Renderted chunk with context and headers.\n        \"\"\"\n\n        # If there are no peripheral chunks, return the main chunk\n        if not peripheral_config:\n            return chunks[current_index][content_key]\n\n        combined_parts = [\"--- Previous Context ---\"]\n\n        combined_parts.extend(\n            self.process_peripheral_chunks(\n                chunks[:current_index],\n                peripheral_config.get(\"previous\", {}),\n                content_key,\n                order_key,\n            )\n        )\n        combined_parts.append(\"--- End Previous Context ---\\n\")\n\n        # Process main chunk\n        main_chunk = chunks[current_index]\n        if headers := self.render_hierarchy_headers(\n            main_chunk, chunks[: current_index + 1], doc_header_key\n        ):\n            combined_parts.append(headers)\n        combined_parts.extend(\n            (\n                f\"{main_chunk_start}\",\n                f\"{main_chunk[content_key]}\",\n                f\"{main_chunk_end}\",\n                \"\\n--- Next Context ---\",\n            )\n        )\n        combined_parts.extend(\n            self.process_peripheral_chunks(\n                chunks[current_index + 1 :],\n                peripheral_config.get(\"next\", {}),\n                content_key,\n                order_key,\n            )\n        )\n        combined_parts.append(\"--- End Next Context ---\")\n\n        return \"\\n\".join(combined_parts)\n\n    def process_peripheral_chunks(\n        self,\n        chunks: list[dict],\n        config: dict,\n        content_key: str,\n        order_key: str,\n        reverse: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Process peripheral chunks according to the configuration.\n\n        Args:\n            chunks (list[dict]): List of chunks to process.\n            config (dict): Configuration for processing peripheral chunks.\n            content_key (str): Key for the content in each chunk.\n            order_key (str): Key for the order of each chunk.\n            reverse (bool, optional): Whether to process chunks in reverse order. Defaults to False.\n\n        Returns:\n            list[str]: List of processed chunk strings.\n        \"\"\"\n        if reverse:\n            chunks = list(reversed(chunks))\n\n        processed_parts = []\n        included_chunks = []\n        total_chunks = len(chunks)\n\n        head_config = config.get(\"head\", {})\n        tail_config = config.get(\"tail\", {})\n\n        head_count = int(head_config.get(\"count\", 0))\n        tail_count = int(tail_config.get(\"count\", 0))\n        in_skip = False\n        skip_char_count = 0\n\n        for i, chunk in enumerate(chunks):\n            if i &lt; head_count:\n                section = \"head\"\n            elif i &gt;= total_chunks - tail_count:\n                section = \"tail\"\n            elif \"middle\" in config:\n                section = \"middle\"\n            else:\n                # Show number of characters skipped\n                skipped_chars = len(chunk[content_key])\n                if not in_skip:\n                    skip_char_count = skipped_chars\n                    in_skip = True\n                else:\n                    skip_char_count += skipped_chars\n\n                continue\n\n            if in_skip:\n                processed_parts.append(\n                    f\"[... {skip_char_count} characters skipped ...]\"\n                )\n                in_skip = False\n                skip_char_count = 0\n\n            section_config = config.get(section, {})\n            section_content_key = section_config.get(\"content_key\", content_key)\n\n            is_summary = section_content_key != content_key\n            summary_suffix = \" (Summary)\" if is_summary else \"\"\n\n            chunk_prefix = f\"[Chunk {chunk[order_key]}{summary_suffix}]\"\n            processed_parts.extend((chunk_prefix, f\"{chunk[section_content_key]}\"))\n            included_chunks.append(chunk)\n\n        if in_skip:\n            processed_parts.append(f\"[... {skip_char_count} characters skipped ...]\")\n\n        if reverse:\n            processed_parts = list(reversed(processed_parts))\n\n        return processed_parts\n\n    def render_hierarchy_headers(\n        self,\n        current_chunk: dict,\n        chunks: list[dict],\n        doc_header_key: str,\n    ) -&gt; str:\n        \"\"\"\n        Render headers for the current chunk's hierarchy.\n\n        Args:\n            current_chunk (dict): The current chunk being processed.\n            chunks (list[dict]): List of chunks up to and including the current chunk.\n            doc_header_key (str): The key for the headers in the current chunk.\n        Returns:\n            str: Renderted headers in the current chunk's hierarchy.\n        \"\"\"\n        current_hierarchy = {}\n\n        if doc_header_key is None:\n            return \"\"\n\n        # Find the largest/highest level in the current chunk\n        current_chunk_headers = current_chunk.get(doc_header_key, [])\n\n        # If there are no headers in the current chunk, return an empty string\n        if not current_chunk_headers:\n            return \"\"\n\n        highest_level = float(\"inf\")  # Initialize with positive infinity\n        for header_info in current_chunk_headers:\n            try:\n                level = header_info.get(\"level\")\n                if level is not None and level &lt; highest_level:\n                    highest_level = level\n            except Exception as e:\n                self.runner.console.log(f\"[red]Error processing header: {e}[/red]\")\n                self.runner.console.log(f\"[red]Header: {header_info}[/red]\")\n                return \"\"\n\n        # If no headers found in the current chunk, set highest_level to None\n        if highest_level == float(\"inf\"):\n            highest_level = None\n\n        for chunk in chunks:\n            for header_info in chunk.get(doc_header_key, []):\n                try:\n                    header = header_info[\"header\"]\n                    level = header_info[\"level\"]\n                    if header and level:\n                        current_hierarchy[level] = header\n                    # Clear lower levels when a higher level header is found\n                    for lower_level in range(level + 1, len(current_hierarchy) + 1):\n                        if lower_level in current_hierarchy:\n                            current_hierarchy[lower_level] = None\n                except Exception as e:\n                    self.runner.console.log(f\"[red]Error processing header: {e}[/red]\")\n                    self.runner.console.log(f\"[red]Header: {header_info}[/red]\")\n                    return \"\"\n\n        rendered_headers = [\n            f\"{'#' * level} {header}\"\n            for level, header in sorted(current_hierarchy.items())\n            if header is not None and (highest_level is None or level &lt; highest_level)\n        ]\n        rendered_headers = \" &gt; \".join(rendered_headers)\n        return f\"_Current Section:_ {rendered_headers}\" if rendered_headers else \"\"\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the GatherOperation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>docetl/operations/gather.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize the GatherOperation.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Execute the gather operation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], float]</code> <p>tuple[list[dict], float]: A tuple containing the processed results and the cost of the operation.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Execute the gather operation on the input data.\n\n    Args:\n        input_data (list[dict]): The input data to process.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the processed results and the cost of the operation.\n    \"\"\"\n    content_key = self.config[\"content_key\"]\n    doc_id_key = self.config[\"doc_id_key\"]\n    order_key = self.config[\"order_key\"]\n    peripheral_config = self.config.get(\"peripheral_chunks\", {})\n    main_chunk_start = self.config.get(\n        \"main_chunk_start\", \"--- Begin Main Chunk ---\"\n    )\n    main_chunk_end = self.config.get(\"main_chunk_end\", \"--- End Main Chunk ---\")\n    doc_header_key = self.config.get(\"doc_header_key\", None)\n    results = []\n    cost = 0.0\n\n    # Group chunks by document ID\n    grouped_chunks = {}\n    for item in input_data:\n        doc_id = item[doc_id_key]\n        if doc_id not in grouped_chunks:\n            grouped_chunks[doc_id] = []\n        grouped_chunks[doc_id].append(item)\n\n    # Process each group of chunks\n    for chunks in grouped_chunks.values():\n        # Sort chunks by their order within the document\n        chunks.sort(key=lambda x: x[order_key])\n\n        # Process each chunk with its peripheral context and headers\n        for i, chunk in enumerate(chunks):\n            rendered_chunk = self.render_chunk_with_context(\n                chunks,\n                i,\n                peripheral_config,\n                content_key,\n                order_key,\n                main_chunk_start,\n                main_chunk_end,\n                doc_header_key,\n            )\n\n            result = chunk.copy()\n            result[f\"{content_key}_rendered\"] = rendered_chunk\n            results.append(result)\n\n    return results, cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.process_peripheral_chunks","title":"<code>process_peripheral_chunks(chunks, config, content_key, order_key, reverse=False)</code>","text":"<p>Process peripheral chunks according to the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[dict]</code> <p>List of chunks to process.</p> required <code>config</code> <code>dict</code> <p>Configuration for processing peripheral chunks.</p> required <code>content_key</code> <code>str</code> <p>Key for the content in each chunk.</p> required <code>order_key</code> <code>str</code> <p>Key for the order of each chunk.</p> required <code>reverse</code> <code>bool</code> <p>Whether to process chunks in reverse order. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of processed chunk strings.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def process_peripheral_chunks(\n    self,\n    chunks: list[dict],\n    config: dict,\n    content_key: str,\n    order_key: str,\n    reverse: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Process peripheral chunks according to the configuration.\n\n    Args:\n        chunks (list[dict]): List of chunks to process.\n        config (dict): Configuration for processing peripheral chunks.\n        content_key (str): Key for the content in each chunk.\n        order_key (str): Key for the order of each chunk.\n        reverse (bool, optional): Whether to process chunks in reverse order. Defaults to False.\n\n    Returns:\n        list[str]: List of processed chunk strings.\n    \"\"\"\n    if reverse:\n        chunks = list(reversed(chunks))\n\n    processed_parts = []\n    included_chunks = []\n    total_chunks = len(chunks)\n\n    head_config = config.get(\"head\", {})\n    tail_config = config.get(\"tail\", {})\n\n    head_count = int(head_config.get(\"count\", 0))\n    tail_count = int(tail_config.get(\"count\", 0))\n    in_skip = False\n    skip_char_count = 0\n\n    for i, chunk in enumerate(chunks):\n        if i &lt; head_count:\n            section = \"head\"\n        elif i &gt;= total_chunks - tail_count:\n            section = \"tail\"\n        elif \"middle\" in config:\n            section = \"middle\"\n        else:\n            # Show number of characters skipped\n            skipped_chars = len(chunk[content_key])\n            if not in_skip:\n                skip_char_count = skipped_chars\n                in_skip = True\n            else:\n                skip_char_count += skipped_chars\n\n            continue\n\n        if in_skip:\n            processed_parts.append(\n                f\"[... {skip_char_count} characters skipped ...]\"\n            )\n            in_skip = False\n            skip_char_count = 0\n\n        section_config = config.get(section, {})\n        section_content_key = section_config.get(\"content_key\", content_key)\n\n        is_summary = section_content_key != content_key\n        summary_suffix = \" (Summary)\" if is_summary else \"\"\n\n        chunk_prefix = f\"[Chunk {chunk[order_key]}{summary_suffix}]\"\n        processed_parts.extend((chunk_prefix, f\"{chunk[section_content_key]}\"))\n        included_chunks.append(chunk)\n\n    if in_skip:\n        processed_parts.append(f\"[... {skip_char_count} characters skipped ...]\")\n\n    if reverse:\n        processed_parts = list(reversed(processed_parts))\n\n    return processed_parts\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.render_chunk_with_context","title":"<code>render_chunk_with_context(chunks, current_index, peripheral_config, content_key, order_key, main_chunk_start, main_chunk_end, doc_header_key)</code>","text":"<p>Render a chunk with its peripheral context and headers.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[dict]</code> <p>List of all chunks in the document.</p> required <code>current_index</code> <code>int</code> <p>Index of the current chunk being processed.</p> required <code>peripheral_config</code> <code>dict</code> <p>Configuration for peripheral chunks.</p> required <code>content_key</code> <code>str</code> <p>Key for the content in each chunk.</p> required <code>order_key</code> <code>str</code> <p>Key for the order of each chunk.</p> required <code>main_chunk_start</code> <code>str</code> <p>String to mark the start of the main chunk.</p> required <code>main_chunk_end</code> <code>str</code> <p>String to mark the end of the main chunk.</p> required <code>doc_header_key</code> <code>str</code> <p>The key for the headers in the current chunk.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Renderted chunk with context and headers.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def render_chunk_with_context(\n    self,\n    chunks: list[dict],\n    current_index: int,\n    peripheral_config: dict,\n    content_key: str,\n    order_key: str,\n    main_chunk_start: str,\n    main_chunk_end: str,\n    doc_header_key: str,\n) -&gt; str:\n    \"\"\"\n    Render a chunk with its peripheral context and headers.\n\n    Args:\n        chunks (list[dict]): List of all chunks in the document.\n        current_index (int): Index of the current chunk being processed.\n        peripheral_config (dict): Configuration for peripheral chunks.\n        content_key (str): Key for the content in each chunk.\n        order_key (str): Key for the order of each chunk.\n        main_chunk_start (str): String to mark the start of the main chunk.\n        main_chunk_end (str): String to mark the end of the main chunk.\n        doc_header_key (str): The key for the headers in the current chunk.\n\n    Returns:\n        str: Renderted chunk with context and headers.\n    \"\"\"\n\n    # If there are no peripheral chunks, return the main chunk\n    if not peripheral_config:\n        return chunks[current_index][content_key]\n\n    combined_parts = [\"--- Previous Context ---\"]\n\n    combined_parts.extend(\n        self.process_peripheral_chunks(\n            chunks[:current_index],\n            peripheral_config.get(\"previous\", {}),\n            content_key,\n            order_key,\n        )\n    )\n    combined_parts.append(\"--- End Previous Context ---\\n\")\n\n    # Process main chunk\n    main_chunk = chunks[current_index]\n    if headers := self.render_hierarchy_headers(\n        main_chunk, chunks[: current_index + 1], doc_header_key\n    ):\n        combined_parts.append(headers)\n    combined_parts.extend(\n        (\n            f\"{main_chunk_start}\",\n            f\"{main_chunk[content_key]}\",\n            f\"{main_chunk_end}\",\n            \"\\n--- Next Context ---\",\n        )\n    )\n    combined_parts.extend(\n        self.process_peripheral_chunks(\n            chunks[current_index + 1 :],\n            peripheral_config.get(\"next\", {}),\n            content_key,\n            order_key,\n        )\n    )\n    combined_parts.append(\"--- End Next Context ---\")\n\n    return \"\\n\".join(combined_parts)\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.render_hierarchy_headers","title":"<code>render_hierarchy_headers(current_chunk, chunks, doc_header_key)</code>","text":"<p>Render headers for the current chunk's hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>current_chunk</code> <code>dict</code> <p>The current chunk being processed.</p> required <code>chunks</code> <code>list[dict]</code> <p>List of chunks up to and including the current chunk.</p> required <code>doc_header_key</code> <code>str</code> <p>The key for the headers in the current chunk.</p> required <p>Returns:     str: Renderted headers in the current chunk's hierarchy.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def render_hierarchy_headers(\n    self,\n    current_chunk: dict,\n    chunks: list[dict],\n    doc_header_key: str,\n) -&gt; str:\n    \"\"\"\n    Render headers for the current chunk's hierarchy.\n\n    Args:\n        current_chunk (dict): The current chunk being processed.\n        chunks (list[dict]): List of chunks up to and including the current chunk.\n        doc_header_key (str): The key for the headers in the current chunk.\n    Returns:\n        str: Renderted headers in the current chunk's hierarchy.\n    \"\"\"\n    current_hierarchy = {}\n\n    if doc_header_key is None:\n        return \"\"\n\n    # Find the largest/highest level in the current chunk\n    current_chunk_headers = current_chunk.get(doc_header_key, [])\n\n    # If there are no headers in the current chunk, return an empty string\n    if not current_chunk_headers:\n        return \"\"\n\n    highest_level = float(\"inf\")  # Initialize with positive infinity\n    for header_info in current_chunk_headers:\n        try:\n            level = header_info.get(\"level\")\n            if level is not None and level &lt; highest_level:\n                highest_level = level\n        except Exception as e:\n            self.runner.console.log(f\"[red]Error processing header: {e}[/red]\")\n            self.runner.console.log(f\"[red]Header: {header_info}[/red]\")\n            return \"\"\n\n    # If no headers found in the current chunk, set highest_level to None\n    if highest_level == float(\"inf\"):\n        highest_level = None\n\n    for chunk in chunks:\n        for header_info in chunk.get(doc_header_key, []):\n            try:\n                header = header_info[\"header\"]\n                level = header_info[\"level\"]\n                if header and level:\n                    current_hierarchy[level] = header\n                # Clear lower levels when a higher level header is found\n                for lower_level in range(level + 1, len(current_hierarchy) + 1):\n                    if lower_level in current_hierarchy:\n                        current_hierarchy[lower_level] = None\n            except Exception as e:\n                self.runner.console.log(f\"[red]Error processing header: {e}[/red]\")\n                self.runner.console.log(f\"[red]Header: {header_info}[/red]\")\n                return \"\"\n\n    rendered_headers = [\n        f\"{'#' * level} {header}\"\n        for level, header in sorted(current_hierarchy.items())\n        if header is not None and (highest_level is None or level &lt; highest_level)\n    ]\n    rendered_headers = \" &gt; \".join(rendered_headers)\n    return f\"_Current Section:_ {rendered_headers}\" if rendered_headers else \"\"\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Perform a syntax check on the operation configuration.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"Perform a syntax check on the operation configuration.\"\"\"\n    # Validate the schema using Pydantic\n    self.schema(**self.config)\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.unnest.UnnestOperation","title":"<code>docetl.operations.unnest.UnnestOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that represents an operation to unnest a list-like or dictionary value in a dictionary into multiple dictionaries.</p> <p>This operation takes a list of dictionaries and a specified key, and creates new dictionaries based on the value type: - For list-like values: Creates a new dictionary for each element in the list, copying all other key-value pairs. - For dictionary values: Expands specified fields from the nested dictionary into the parent dictionary.</p> Inherits from <p>BaseOperation</p> <p>Usage: <pre><code>from docetl.operations import UnnestOperation\n\n# Unnesting a list\nconfig_list = {\"unnest_key\": \"tags\"}\ninput_data_list = [\n    {\"id\": 1, \"tags\": [\"a\", \"b\", \"c\"]},\n    {\"id\": 2, \"tags\": [\"d\", \"e\"]}\n]\n\nunnest_op_list = UnnestOperation(config_list)\nresult_list, _ = unnest_op_list.execute(input_data_list)\n\n# Result will be:\n# [\n#     {\"id\": 1, \"tags\": \"a\"},\n#     {\"id\": 1, \"tags\": \"b\"},\n#     {\"id\": 1, \"tags\": \"c\"},\n#     {\"id\": 2, \"tags\": \"d\"},\n#     {\"id\": 2, \"tags\": \"e\"}\n# ]\n\n# Unnesting a dictionary\nconfig_dict = {\"unnest_key\": \"user\", \"expand_fields\": [\"name\", \"age\"]}\ninput_data_dict = [\n    {\"id\": 1, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n    {\"id\": 2, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n]\n\nunnest_op_dict = UnnestOperation(config_dict)\nresult_dict, _ = unnest_op_dict.execute(input_data_dict)\n\n# Result will be:\n# [\n#     {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n#     {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n# ]\n</code></pre></p> Source code in <code>docetl/operations/unnest.py</code> <pre><code>class UnnestOperation(BaseOperation):\n    \"\"\"\n    A class that represents an operation to unnest a list-like or dictionary value in a dictionary into multiple dictionaries.\n\n    This operation takes a list of dictionaries and a specified key, and creates new dictionaries based on the value type:\n    - For list-like values: Creates a new dictionary for each element in the list, copying all other key-value pairs.\n    - For dictionary values: Expands specified fields from the nested dictionary into the parent dictionary.\n\n    Inherits from:\n        BaseOperation\n\n    Usage:\n    ```python\n    from docetl.operations import UnnestOperation\n\n    # Unnesting a list\n    config_list = {\"unnest_key\": \"tags\"}\n    input_data_list = [\n        {\"id\": 1, \"tags\": [\"a\", \"b\", \"c\"]},\n        {\"id\": 2, \"tags\": [\"d\", \"e\"]}\n    ]\n\n    unnest_op_list = UnnestOperation(config_list)\n    result_list, _ = unnest_op_list.execute(input_data_list)\n\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"tags\": \"a\"},\n    #     {\"id\": 1, \"tags\": \"b\"},\n    #     {\"id\": 1, \"tags\": \"c\"},\n    #     {\"id\": 2, \"tags\": \"d\"},\n    #     {\"id\": 2, \"tags\": \"e\"}\n    # ]\n\n    # Unnesting a dictionary\n    config_dict = {\"unnest_key\": \"user\", \"expand_fields\": [\"name\", \"age\"]}\n    input_data_dict = [\n        {\"id\": 1, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n        {\"id\": 2, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n    ]\n\n    unnest_op_dict = UnnestOperation(config_dict)\n    result_dict, _ = unnest_op_dict.execute(input_data_dict)\n\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n    #     {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n    # ]\n    ```\n    \"\"\"\n\n    class schema(BaseOperation.schema):\n        type: str = \"unnest\"\n        unnest_key: str\n        keep_empty: bool | None = None\n        expand_fields: list[str] | None = None\n        recursive: bool | None = None\n        depth: int | None = None\n\n    def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n        \"\"\"\n        Executes the unnest operation on the input data.\n\n        Args:\n            input_data (list[dict]): A list of dictionaries to process.\n\n        Returns:\n            tuple[list[dict], float]: A tuple containing the processed list of dictionaries\n            and a float value (always 0 in this implementation).\n\n        Raises:\n            KeyError: If the specified unnest_key is not found in an input dictionary.\n            TypeError: If the value of the unnest_key is not iterable (list, tuple, set, or dict).\n            ValueError: If unnesting a dictionary and 'expand_fields' is not provided in the config.\n\n        The operation supports unnesting of both list-like values and dictionary values:\n\n        1. For list-like values (list, tuple, set):\n           Each element in the list becomes a separate dictionary in the output.\n\n        2. For dictionary values:\n           The operation expands specified fields from the nested dictionary into the parent dictionary.\n           The 'expand_fields' config parameter must be provided to specify which fields to expand.\n\n        Examples:\n        ```python\n        # Unnesting a list\n        unnest_op = UnnestOperation({\"unnest_key\": \"colors\"})\n        input_data = [\n            {\"id\": 1, \"colors\": [\"red\", \"blue\"]},\n            {\"id\": 2, \"colors\": [\"green\"]}\n        ]\n        result, _ = unnest_op.execute(input_data)\n        # Result will be:\n        # [\n        #     {\"id\": 1, \"colors\": \"red\"},\n        #     {\"id\": 1, \"colors\": \"blue\"},\n        #     {\"id\": 2, \"colors\": \"green\"}\n        # ]\n\n        # Unnesting a dictionary\n        unnest_op = UnnestOperation({\"unnest_key\": \"details\", \"expand_fields\": [\"color\", \"size\"]})\n        input_data = [\n            {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}},\n            {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}}\n        ]\n        result, _ = unnest_op.execute(input_data)\n        # Result will be:\n        # [\n        #     {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}, \"color\": \"red\", \"size\": \"large\"},\n        #     {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}, \"color\": \"blue\", \"size\": \"medium\"}\n        # ]\n        ```\n\n        Note: When unnesting dictionaries, the original nested dictionary is preserved in the output,\n        and the specified fields are expanded into the parent dictionary.\n        \"\"\"\n\n        unnest_key = self.config[\"unnest_key\"]\n        recursive = self.config.get(\"recursive\", False)\n        depth = self.config.get(\"depth\", None)\n        if not depth:\n            depth = 1 if not recursive else float(\"inf\")\n        results = []\n\n        def unnest_recursive(item, key, level=0):\n            if level == 0 and not isinstance(item[key], (list, tuple, set, dict)):\n                raise TypeError(f\"Value of unnest key '{key}' is not iterable\")\n\n            if level &gt; 0 and not isinstance(item[key], (list, tuple, set, dict)):\n                return [item]\n\n            if level &gt;= depth:\n                return [item]\n\n            if isinstance(item[key], dict):\n                expand_fields = self.config.get(\"expand_fields\")\n                if expand_fields is None:\n                    expand_fields = item[key].keys()\n                new_item = copy.deepcopy(item)\n                for field in expand_fields:\n                    if field in new_item[key]:\n                        new_item[field] = new_item[key][field]\n                    else:\n                        new_item[field] = None\n                return [new_item]\n            else:\n                nested_results = []\n                for value in item[key]:\n                    new_item = copy.deepcopy(item)\n                    new_item[key] = value\n                    if recursive and isinstance(value, (list, tuple, set, dict)):\n                        nested_results.extend(\n                            unnest_recursive(new_item, key, level + 1)\n                        )\n                    else:\n                        nested_results.append(new_item)\n                return nested_results\n\n        for item in input_data:\n            if unnest_key not in item:\n                raise KeyError(\n                    f\"Unnest key '{unnest_key}' not found in item. Other keys are {item.keys()}\"\n                )\n\n            results.extend(unnest_recursive(item, unnest_key))\n\n            if not item[unnest_key] and self.config.get(\"keep_empty\", False):\n                expand_fields = self.config.get(\"expand_fields\")\n                new_item = copy.deepcopy(item)\n                if isinstance(item[unnest_key], dict):\n                    if expand_fields is None:\n                        expand_fields = item[unnest_key].keys()\n                    for field in expand_fields:\n                        new_item[field] = None\n                else:\n                    new_item[unnest_key] = None\n                results.append(new_item)\n\n        # Assert that no keys are missing after the operation\n        if results:\n            original_keys = set(input_data[0].keys())\n            assert original_keys.issubset(\n                set(results[0].keys())\n            ), \"Keys lost during unnest operation\"\n\n        return results, 0\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.unnest.UnnestOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the unnest operation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[dict]</code> <p>A list of dictionaries to process.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>tuple[list[dict], float]: A tuple containing the processed list of dictionaries</p> <code>float</code> <p>and a float value (always 0 in this implementation).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified unnest_key is not found in an input dictionary.</p> <code>TypeError</code> <p>If the value of the unnest_key is not iterable (list, tuple, set, or dict).</p> <code>ValueError</code> <p>If unnesting a dictionary and 'expand_fields' is not provided in the config.</p> <p>The operation supports unnesting of both list-like values and dictionary values:</p> <ol> <li> <p>For list-like values (list, tuple, set):    Each element in the list becomes a separate dictionary in the output.</p> </li> <li> <p>For dictionary values:    The operation expands specified fields from the nested dictionary into the parent dictionary.    The 'expand_fields' config parameter must be provided to specify which fields to expand.</p> </li> </ol> <p>Examples: <pre><code># Unnesting a list\nunnest_op = UnnestOperation({\"unnest_key\": \"colors\"})\ninput_data = [\n    {\"id\": 1, \"colors\": [\"red\", \"blue\"]},\n    {\"id\": 2, \"colors\": [\"green\"]}\n]\nresult, _ = unnest_op.execute(input_data)\n# Result will be:\n# [\n#     {\"id\": 1, \"colors\": \"red\"},\n#     {\"id\": 1, \"colors\": \"blue\"},\n#     {\"id\": 2, \"colors\": \"green\"}\n# ]\n\n# Unnesting a dictionary\nunnest_op = UnnestOperation({\"unnest_key\": \"details\", \"expand_fields\": [\"color\", \"size\"]})\ninput_data = [\n    {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}},\n    {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}}\n]\nresult, _ = unnest_op.execute(input_data)\n# Result will be:\n# [\n#     {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}, \"color\": \"red\", \"size\": \"large\"},\n#     {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}, \"color\": \"blue\", \"size\": \"medium\"}\n# ]\n</code></pre></p> <p>Note: When unnesting dictionaries, the original nested dictionary is preserved in the output, and the specified fields are expanded into the parent dictionary.</p> Source code in <code>docetl/operations/unnest.py</code> <pre><code>def execute(self, input_data: list[dict]) -&gt; tuple[list[dict], float]:\n    \"\"\"\n    Executes the unnest operation on the input data.\n\n    Args:\n        input_data (list[dict]): A list of dictionaries to process.\n\n    Returns:\n        tuple[list[dict], float]: A tuple containing the processed list of dictionaries\n        and a float value (always 0 in this implementation).\n\n    Raises:\n        KeyError: If the specified unnest_key is not found in an input dictionary.\n        TypeError: If the value of the unnest_key is not iterable (list, tuple, set, or dict).\n        ValueError: If unnesting a dictionary and 'expand_fields' is not provided in the config.\n\n    The operation supports unnesting of both list-like values and dictionary values:\n\n    1. For list-like values (list, tuple, set):\n       Each element in the list becomes a separate dictionary in the output.\n\n    2. For dictionary values:\n       The operation expands specified fields from the nested dictionary into the parent dictionary.\n       The 'expand_fields' config parameter must be provided to specify which fields to expand.\n\n    Examples:\n    ```python\n    # Unnesting a list\n    unnest_op = UnnestOperation({\"unnest_key\": \"colors\"})\n    input_data = [\n        {\"id\": 1, \"colors\": [\"red\", \"blue\"]},\n        {\"id\": 2, \"colors\": [\"green\"]}\n    ]\n    result, _ = unnest_op.execute(input_data)\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"colors\": \"red\"},\n    #     {\"id\": 1, \"colors\": \"blue\"},\n    #     {\"id\": 2, \"colors\": \"green\"}\n    # ]\n\n    # Unnesting a dictionary\n    unnest_op = UnnestOperation({\"unnest_key\": \"details\", \"expand_fields\": [\"color\", \"size\"]})\n    input_data = [\n        {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}},\n        {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}}\n    ]\n    result, _ = unnest_op.execute(input_data)\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}, \"color\": \"red\", \"size\": \"large\"},\n    #     {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}, \"color\": \"blue\", \"size\": \"medium\"}\n    # ]\n    ```\n\n    Note: When unnesting dictionaries, the original nested dictionary is preserved in the output,\n    and the specified fields are expanded into the parent dictionary.\n    \"\"\"\n\n    unnest_key = self.config[\"unnest_key\"]\n    recursive = self.config.get(\"recursive\", False)\n    depth = self.config.get(\"depth\", None)\n    if not depth:\n        depth = 1 if not recursive else float(\"inf\")\n    results = []\n\n    def unnest_recursive(item, key, level=0):\n        if level == 0 and not isinstance(item[key], (list, tuple, set, dict)):\n            raise TypeError(f\"Value of unnest key '{key}' is not iterable\")\n\n        if level &gt; 0 and not isinstance(item[key], (list, tuple, set, dict)):\n            return [item]\n\n        if level &gt;= depth:\n            return [item]\n\n        if isinstance(item[key], dict):\n            expand_fields = self.config.get(\"expand_fields\")\n            if expand_fields is None:\n                expand_fields = item[key].keys()\n            new_item = copy.deepcopy(item)\n            for field in expand_fields:\n                if field in new_item[key]:\n                    new_item[field] = new_item[key][field]\n                else:\n                    new_item[field] = None\n            return [new_item]\n        else:\n            nested_results = []\n            for value in item[key]:\n                new_item = copy.deepcopy(item)\n                new_item[key] = value\n                if recursive and isinstance(value, (list, tuple, set, dict)):\n                    nested_results.extend(\n                        unnest_recursive(new_item, key, level + 1)\n                    )\n                else:\n                    nested_results.append(new_item)\n            return nested_results\n\n    for item in input_data:\n        if unnest_key not in item:\n            raise KeyError(\n                f\"Unnest key '{unnest_key}' not found in item. Other keys are {item.keys()}\"\n            )\n\n        results.extend(unnest_recursive(item, unnest_key))\n\n        if not item[unnest_key] and self.config.get(\"keep_empty\", False):\n            expand_fields = self.config.get(\"expand_fields\")\n            new_item = copy.deepcopy(item)\n            if isinstance(item[unnest_key], dict):\n                if expand_fields is None:\n                    expand_fields = item[unnest_key].keys()\n                for field in expand_fields:\n                    new_item[field] = None\n            else:\n                new_item[unnest_key] = None\n            results.append(new_item)\n\n    # Assert that no keys are missing after the operation\n    if results:\n        original_keys = set(input_data[0].keys())\n        assert original_keys.issubset(\n            set(results[0].keys())\n        ), \"Keys lost during unnest operation\"\n\n    return results, 0\n</code></pre>"},{"location":"api-reference/optimizers/","title":"Optimizers","text":""},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer","title":"<code>docetl.optimizers.map_optimizer.optimizer.MapOptimizer</code>","text":"<p>A class for optimizing map operations in data processing pipelines.</p> <p>This optimizer analyzes the input operation configuration and data, and generates optimized plans for executing the operation. It can create plans for chunking, metadata extraction, gleaning, chain decomposition, and parallel execution.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict[str, Any]</code> <p>The configuration dictionary for the optimizer.</p> <code>console</code> <code>Console</code> <p>A Rich console object for pretty printing.</p> <code>llm_client</code> <code>LLMClient</code> <p>A client for interacting with a language model.</p> <code>_run_operation</code> <code>Callable</code> <p>A function to execute operations.</p> <code>max_threads</code> <code>int</code> <p>The maximum number of threads to use for parallel execution.</p> <code>timeout</code> <code>int</code> <p>The timeout in seconds for operation execution.</p> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>class MapOptimizer:\n    \"\"\"\n    A class for optimizing map operations in data processing pipelines.\n\n    This optimizer analyzes the input operation configuration and data,\n    and generates optimized plans for executing the operation. It can\n    create plans for chunking, metadata extraction, gleaning, chain\n    decomposition, and parallel execution.\n\n    Attributes:\n        config (dict[str, Any]): The configuration dictionary for the optimizer.\n        console (Console): A Rich console object for pretty printing.\n        llm_client (LLMClient): A client for interacting with a language model.\n        _run_operation (Callable): A function to execute operations.\n        max_threads (int): The maximum number of threads to use for parallel execution.\n        timeout (int): The timeout in seconds for operation execution.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        runner,\n        run_operation: Callable,\n        timeout: int = 10,\n        is_filter: bool = False,\n        depth: int = 1,\n    ):\n        \"\"\"\n        Initialize the MapOptimizer.\n\n        Args:\n            runner (Runner): The runner object.\n            run_operation (Callable): A function to execute operations.\n            timeout (int, optional): The timeout in seconds for operation execution. Defaults to 10.\n            is_filter (bool, optional): If True, the operation is a filter operation. Defaults to False.\n        \"\"\"\n        self.runner = runner\n        self.config = runner.config\n        self.console = runner.console\n        self.llm_client = runner.optimizer.llm_client\n        self._run_operation = run_operation\n        self.max_threads = runner.max_threads\n        self.timeout = runner.optimizer.timeout\n        self._num_plans_to_evaluate_in_parallel = 5\n        self.is_filter = is_filter\n        self.k_to_pairwise_compare = 6\n\n        self.plan_generator = PlanGenerator(\n            runner,\n            self.llm_client,\n            self.console,\n            self.config,\n            run_operation,\n            self.max_threads,\n            is_filter,\n            depth,\n        )\n        self.evaluator = Evaluator(\n            self.llm_client,\n            self.console,\n            self._run_operation,\n            self.timeout,\n            self._num_plans_to_evaluate_in_parallel,\n            self.is_filter,\n        )\n        self.prompt_generator = PromptGenerator(\n            self.runner,\n            self.llm_client,\n            self.console,\n            self.config,\n            self.max_threads,\n            self.is_filter,\n        )\n\n    def should_optimize(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; tuple[str, list[dict[str, Any]], list[dict[str, Any]]]:\n        \"\"\"\n        Determine if the given operation configuration should be optimized.\n        \"\"\"\n        (\n            input_data,\n            output_data,\n            _,\n            _,\n            validator_prompt,\n            assessment,\n            data_exceeds_limit,\n        ) = self._should_optimize_helper(op_config, input_data)\n        if data_exceeds_limit or assessment.get(\"needs_improvement\", True):\n            assessment_str = (\n                \"\\n\".join(assessment.get(\"reasons\", []))\n                + \"\\n\\nHere are some improvements that may help:\\n\"\n                + \"\\n\".join(assessment.get(\"improvements\", []))\n            )\n            if data_exceeds_limit:\n                assessment_str += \"\\nAlso, the input data exceeds the token limit.\"\n            return assessment_str, input_data, output_data\n        else:\n            return \"\", input_data, output_data\n\n    def _should_optimize_helper(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; tuple[\n        list[dict[str, Any]],\n        list[dict[str, Any]],\n        int,\n        float,\n        str,\n        dict[str, Any],\n        bool,\n    ]:\n        \"\"\"\n        Determine if the given operation configuration should be optimized.\n        Create a custom validator prompt and assess the operation's performance\n        using the validator.\n        \"\"\"\n        self.console.post_optimizer_status(StageType.SAMPLE_RUN)\n        input_data = copy.deepcopy(input_data)\n        # Add id to each input_data\n        for i in range(len(input_data)):\n            input_data[i][\"_map_opt_id\"] = str(uuid.uuid4())\n\n        # Define the token limit (adjust as needed)\n        model_input_context_length = model_cost.get(\n            op_config.get(\"model\", self.config.get(\"default_model\")), {}\n        ).get(\"max_input_tokens\", 8192)\n\n        # Render the prompt with all sample inputs and count tokens\n        total_tokens = 0\n        exceed_count = 0\n        for sample in input_data:\n            rendered_prompt = Template(op_config[\"prompt\"]).render(input=sample)\n            prompt_tokens = count_tokens(\n                rendered_prompt,\n                op_config.get(\"model\", self.config.get(\"default_model\")),\n            )\n            total_tokens += prompt_tokens\n\n            if prompt_tokens &gt; model_input_context_length:\n                exceed_count += 1\n\n        # Calculate average tokens and percentage of samples exceeding limit\n        avg_tokens = total_tokens / len(input_data)\n        exceed_percentage = (exceed_count / len(input_data)) * 100\n\n        data_exceeds_limit = exceed_count &gt; 0\n        if exceed_count &gt; 0:\n            self.console.log(\n                f\"[yellow]Warning: {exceed_percentage:.2f}% of prompts exceed token limit. \"\n                f\"Average token count: {avg_tokens:.2f}. \"\n                f\"Truncating input data when generating validators.[/yellow]\"\n            )\n\n        # Execute the original operation on the sample data\n        no_change_start = time.time()\n        output_data = self._run_operation(op_config, input_data, is_build=True)\n        no_change_runtime = time.time() - no_change_start\n\n        # Capture output for the sample run\n        self.runner.optimizer.captured_output.save_optimizer_output(\n            stage_type=StageType.SAMPLE_RUN,\n            output={\n                \"operation_config\": op_config,\n                \"input_data\": input_data,\n                \"output_data\": output_data,\n            },\n        )\n\n        # Generate custom validator prompt\n        self.console.post_optimizer_status(StageType.SHOULD_OPTIMIZE)\n        validator_prompt = self.prompt_generator._generate_validator_prompt(\n            op_config, input_data, output_data\n        )\n\n        # Log the validator prompt\n        self.console.log(\"[bold]Validator Prompt:[/bold]\")\n        self.console.log(validator_prompt)\n        self.console.log(\"\\n\")  # Add a newline for better readability\n\n        # Step 2: Use the validator prompt to assess the operation's performance\n        assessment = self.evaluator._assess_operation(\n            op_config, input_data, output_data, validator_prompt\n        )\n\n        # Print out the assessment\n        self.console.log(\n            f\"[bold]Assessment for whether we should improve operation {op_config['name']}:[/bold]\"\n        )\n        for key, value in assessment.items():\n            self.console.log(f\"[bold cyan]{key}:[/bold cyan] [yellow]{value}[/yellow]\")\n        self.console.log(\"\\n\")  # Add a newline for better readability\n\n        self.runner.optimizer.captured_output.save_optimizer_output(\n            stage_type=StageType.SHOULD_OPTIMIZE,\n            output={\n                \"validator_prompt\": validator_prompt,\n                \"needs_improvement\": assessment.get(\"needs_improvement\", True),\n                \"reasons\": assessment.get(\"reasons\", []),\n                \"improvements\": assessment.get(\"improvements\", []),\n            },\n        )\n        self.console.post_optimizer_rationale(\n            assessment.get(\"needs_improvement\", True),\n            \"\\n\".join(assessment.get(\"reasons\", []))\n            + \"\\n\\n\"\n            + \"\\n\".join(assessment.get(\"improvements\", [])),\n            validator_prompt,\n        )\n\n        return (\n            input_data,\n            output_data,\n            model_input_context_length,\n            no_change_runtime,\n            validator_prompt,\n            assessment,\n            data_exceeds_limit,\n        )\n\n    def optimize(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        plan_types: list[str] | None = [\"chunk\", \"proj_synthesis\", \"glean\"],\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"\n        Optimize the given operation configuration for the input data.\n        Uses a staged evaluation approach:\n        1. For data exceeding limits: Try all plan types at once\n        2. For data within limits:\n            - First try gleaning/proj synthesis\n            - Compare with baseline\n            - Selectively try chunking plans based on initial results\n        \"\"\"\n        # Verify that the plan types are valid\n        for plan_type in plan_types:\n            if plan_type not in [\"chunk\", \"proj_synthesis\", \"glean\"]:\n                raise ValueError(\n                    f\"Invalid plan type: {plan_type}. Valid plan types are: chunk, proj_synthesis, glean.\"\n                )\n\n        (\n            input_data,\n            output_data,\n            model_input_context_length,\n            no_change_runtime,\n            validator_prompt,\n            assessment,\n            data_exceeds_limit,\n        ) = self._should_optimize_helper(op_config, input_data)\n\n        if not self.config.get(\"optimizer_config\", {}).get(\"force_decompose\", False):\n            if not data_exceeds_limit and not assessment.get(\"needs_improvement\", True):\n                self.console.log(\n                    f\"[green]No improvement needed for operation {op_config['name']}[/green]\"\n                )\n                return (\n                    [op_config],\n                    output_data,\n                    self.plan_generator.subplan_optimizer_cost,\n                )\n\n        # Select consistent evaluation samples\n        num_evaluations = min(5, len(input_data))\n        evaluation_samples = select_evaluation_samples(input_data, num_evaluations)\n\n        if data_exceeds_limit:\n            # For data exceeding limits, try all plan types at once\n            return self._evaluate_all_plans(\n                op_config,\n                input_data,\n                evaluation_samples,\n                validator_prompt,\n                plan_types,\n                model_input_context_length,\n                data_exceeds_limit=True,\n            )\n\n        # For data within limits, use staged evaluation\n        return self._staged_evaluation(\n            op_config,\n            input_data,\n            evaluation_samples,\n            validator_prompt,\n            plan_types,\n            no_change_runtime,\n            model_input_context_length,\n        )\n\n    def _select_best_plan(\n        self,\n        results: dict[str, tuple[float, float, list[dict[str, Any]]]],\n        op_config: dict[str, Any],\n        evaluation_samples: list[dict[str, Any]],\n        validator_prompt: str,\n        candidate_plans: dict[str, list[dict[str, Any]]],\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], str, dict[str, int]]:\n        \"\"\"\n        Select the best plan from evaluation results using top-k comparison.\n\n        Returns:\n            Tuple of (best plan, best output, best plan name, pairwise rankings)\n        \"\"\"\n        # Sort results by score in descending order\n        sorted_results = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n\n        # Take the top k plans\n        top_plans = sorted_results[: self.k_to_pairwise_compare]\n\n        # Check if there are no top plans\n        if len(top_plans) == 0:\n            raise ValueError(\n                \"No valid plans were generated. Unable to proceed with optimization.\"\n            )\n\n        # Include any additional plans that are tied with the last plan\n        tail_score = (\n            top_plans[-1][1][0]\n            if len(top_plans) == self.k_to_pairwise_compare\n            else float(\"-inf\")\n        )\n        filtered_results = dict(\n            top_plans\n            + [\n                item\n                for item in sorted_results[len(top_plans) :]\n                if item[1][0] == tail_score\n            ]\n        )\n\n        # Perform pairwise comparisons on filtered plans\n        if len(filtered_results) &gt; 1:\n            pairwise_rankings = self.evaluator._pairwise_compare_plans(\n                filtered_results, validator_prompt, op_config, evaluation_samples\n            )\n            best_plan_name = max(pairwise_rankings, key=pairwise_rankings.get)\n        else:\n            pairwise_rankings = {k: 0 for k in results.keys()}\n            best_plan_name = next(iter(filtered_results))\n\n        # Display results table\n        self.console.log(\n            f\"\\n[bold]Plan Evaluation Results for {op_config['name']} ({op_config['type']}, {len(results)} plans, {len(evaluation_samples)} samples):[/bold]\"\n        )\n        table = Table(show_header=True, header_style=\"bold magenta\")\n        table.add_column(\"Plan\", style=\"dim\")\n        table.add_column(\"Score\", justify=\"right\", width=10)\n        table.add_column(\"Runtime\", justify=\"right\", width=10)\n        table.add_column(\"Pairwise Wins\", justify=\"right\", width=10)\n\n        for plan_name, (score, runtime, _) in sorted_results:\n            table.add_row(\n                plan_name,\n                f\"{score:.2f}\",\n                f\"{runtime:.2f}s\",\n                f\"{pairwise_rankings.get(plan_name, 0)}\",\n            )\n\n        self.console.log(table)\n        self.console.log(\"\\n\")\n\n        try:\n            best_plan = candidate_plans[best_plan_name]\n            best_output = results[best_plan_name][2]\n        except KeyError:\n            raise ValueError(\n                f\"Best plan name {best_plan_name} not found in candidate plans. Candidate plan names: {candidate_plans.keys()}\"\n            )\n\n        self.console.log(\n            f\"[green]Current best plan: {best_plan_name} for operation {op_config['name']} \"\n            f\"(Score: {results[best_plan_name][0]:.2f}, \"\n            f\"Runtime: {results[best_plan_name][1]:.2f}s)[/green]\"\n        )\n\n        return best_plan, best_output, best_plan_name, pairwise_rankings\n\n    def _staged_evaluation(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        evaluation_samples: list[dict[str, Any]],\n        validator_prompt: str,\n        plan_types: list[str],\n        no_change_runtime: float,\n        model_input_context_length: int,\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"Stage 1: Try gleaning and proj synthesis plans first\"\"\"\n        candidate_plans = {\"no_change\": [op_config]}\n\n        # Generate initial plans (gleaning and proj synthesis)\n        if \"glean\" in plan_types:\n            self.console.log(\n                \"[bold magenta]Generating gleaning plans...[/bold magenta]\"\n            )\n            gleaning_plans = self.plan_generator._generate_gleaning_plans(\n                op_config, validator_prompt\n            )\n            candidate_plans.update(gleaning_plans)\n\n        if \"proj_synthesis\" in plan_types and not self.is_filter:\n            self.console.log(\n                \"[bold magenta]Generating independent projection synthesis plans...[/bold magenta]\"\n            )\n            parallel_plans = self.plan_generator._generate_parallel_plans(\n                op_config, input_data\n            )\n            candidate_plans.update(parallel_plans)\n\n            self.console.log(\n                \"[bold magenta]Generating chain projection synthesis plans...[/bold magenta]\"\n            )\n            chain_plans = self.plan_generator._generate_chain_plans(\n                op_config, input_data\n            )\n            candidate_plans.update(chain_plans)\n\n        # Evaluate initial plans\n        initial_results = self._evaluate_plans(\n            candidate_plans,\n            op_config,\n            evaluation_samples,\n            validator_prompt,\n            no_change_runtime,\n        )\n\n        # Get best initial plan\n        best_plan, best_output, best_plan_name, pairwise_rankings = (\n            self._select_best_plan(\n                initial_results,\n                op_config,\n                evaluation_samples,\n                validator_prompt,\n                candidate_plans,\n            )\n        )\n        best_is_better_than_baseline = best_plan_name != \"no_change\"\n\n        # Stage 2: Decide whether/how to try chunking plans\n        if \"chunk\" in plan_types:\n            if best_is_better_than_baseline:\n                # Try 2 random chunking plans first\n                self.console.log(\n                    \"[bold magenta]Trying sample of chunking plans...[/bold magenta]\"\n                )\n                chunk_plans = self.plan_generator._generate_chunk_size_plans(\n                    op_config, input_data, validator_prompt, model_input_context_length\n                )\n\n                if chunk_plans:\n                    # Sample 2 random plans\n                    chunk_items = list(chunk_plans.items())\n                    sample_plans = dict(\n                        random.sample(chunk_items, min(2, len(chunk_items)))\n                    )\n                    sample_results = self._evaluate_plans(\n                        sample_plans, op_config, evaluation_samples, validator_prompt\n                    )\n\n                    # Do pairwise comparison between sampled plans and current best\n                    current_best = {best_plan_name: initial_results[best_plan_name]}\n                    current_best.update(sample_results)\n\n                    _, _, new_best_name, new_pairwise_rankings = self._select_best_plan(\n                        current_best,\n                        op_config,\n                        evaluation_samples,\n                        validator_prompt,\n                        {**{best_plan_name: best_plan}, **sample_plans},\n                    )\n\n                    if new_best_name == best_plan_name:\n                        self.console.log(\n                            \"[yellow]Sample chunking plans did not improve results. Keeping current best plan.[/yellow]\"\n                        )\n                        return (\n                            best_plan,\n                            best_output,\n                            self.plan_generator.subplan_optimizer_cost,\n                        )\n\n                    # If a sampled plan wins, evaluate all chunking plans\n                    self.console.log(\n                        \"[bold magenta]Generating all chunking plans...[/bold magenta]\"\n                    )\n                    chunk_results = self._evaluate_plans(\n                        chunk_plans, op_config, evaluation_samples, validator_prompt\n                    )\n                    initial_results.update(chunk_results)\n                    candidate_plans.update(chunk_plans)\n            else:\n                # Try all chunking plans since no improvement found yet\n                self.console.log(\n                    \"[bold magenta]Generating chunking plans...[/bold magenta]\"\n                )\n                chunk_plans = self.plan_generator._generate_chunk_size_plans(\n                    op_config, input_data, validator_prompt, model_input_context_length\n                )\n                chunk_results = self._evaluate_plans(\n                    chunk_plans, op_config, evaluation_samples, validator_prompt\n                )\n                initial_results.update(chunk_results)\n                candidate_plans.update(chunk_plans)\n\n        # Final selection of best plan\n        best_plan, best_output, _, final_pairwise_rankings = self._select_best_plan(\n            initial_results,\n            op_config,\n            evaluation_samples,\n            validator_prompt,\n            candidate_plans,\n        )\n\n        # Capture evaluation results with pairwise rankings\n        ratings = {k: v[0] for k, v in initial_results.items()}\n        runtime = {k: v[1] for k, v in initial_results.items()}\n        sample_outputs = {k: v[2] for k, v in initial_results.items()}\n        self.runner.optimizer.captured_output.save_optimizer_output(\n            stage_type=StageType.EVALUATION_RESULTS,\n            output={\n                \"input_data\": evaluation_samples,\n                \"all_plan_ratings\": ratings,\n                \"all_plan_runtimes\": runtime,\n                \"all_plan_sample_outputs\": sample_outputs,\n                \"all_plan_pairwise_rankings\": final_pairwise_rankings,\n            },\n        )\n\n        self.console.post_optimizer_status(StageType.END)\n        return best_plan, best_output, self.plan_generator.subplan_optimizer_cost\n\n    def _evaluate_plans(\n        self,\n        plans: dict[str, list[dict[str, Any]]],\n        op_config: dict[str, Any],\n        evaluation_samples: list[dict[str, Any]],\n        validator_prompt: str,\n        no_change_runtime: float | None = None,\n    ) -&gt; dict[str, tuple[float, float, list[dict[str, Any]]]]:\n        \"\"\"Helper method to evaluate a set of plans in parallel\"\"\"\n        results = {}\n        plans_list = list(plans.items())\n\n        for i in range(0, len(plans_list), self._num_plans_to_evaluate_in_parallel):\n            batch = plans_list[i : i + self._num_plans_to_evaluate_in_parallel]\n            with ThreadPoolExecutor(\n                max_workers=self._num_plans_to_evaluate_in_parallel\n            ) as executor:\n                futures = {\n                    executor.submit(\n                        self.evaluator._evaluate_plan,\n                        plan_name,\n                        op_config,\n                        plan,\n                        copy.deepcopy(evaluation_samples),\n                        validator_prompt,\n                    ): plan_name\n                    for plan_name, plan in batch\n                }\n                for future in as_completed(futures):\n                    plan_name = futures[future]\n                    try:\n                        score, runtime, output = future.result(timeout=self.timeout)\n                        results[plan_name] = (score, runtime, output)\n                    except concurrent.futures.TimeoutError:\n                        self.console.log(\n                            f\"[yellow]Plan {plan_name} timed out and will be skipped.[/yellow]\"\n                        )\n                    except Exception as e:\n                        self.console.log(\n                            f\"[red]Error in plan {plan_name}: {str(e)}[/red]\"\n                        )\n\n        if \"no_change\" in results and no_change_runtime is not None:\n            results[\"no_change\"] = (\n                results[\"no_change\"][0],\n                no_change_runtime,\n                results[\"no_change\"][2],\n            )\n\n        return results\n\n    def _evaluate_all_plans(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        evaluation_samples: list[dict[str, Any]],\n        validator_prompt: str,\n        plan_types: list[str],\n        model_input_context_length: int,\n        data_exceeds_limit: bool,\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"\n        Evaluate all plans for a given operation configuration.\n        \"\"\"\n        candidate_plans = {}\n\n        # Generate all plans\n        self.console.post_optimizer_status(StageType.CANDIDATE_PLANS)\n        self.console.log(\n            f\"[bold magenta]Generating {len(plan_types)} plans...[/bold magenta]\"\n        )\n        for plan_type in plan_types:\n            if plan_type == \"chunk\":\n                self.console.log(\n                    \"[bold magenta]Generating chunking plans...[/bold magenta]\"\n                )\n                chunk_size_plans = self.plan_generator._generate_chunk_size_plans(\n                    op_config, input_data, validator_prompt, model_input_context_length\n                )\n                candidate_plans.update(chunk_size_plans)\n            elif plan_type == \"proj_synthesis\":\n                if not self.is_filter:\n                    self.console.log(\n                        \"[bold magenta]Generating independent projection synthesis plans...[/bold magenta]\"\n                    )\n                    parallel_plans = self.plan_generator._generate_parallel_plans(\n                        op_config, input_data\n                    )\n                    candidate_plans.update(parallel_plans)\n\n                    self.console.log(\n                        \"[bold magenta]Generating chain projection synthesis plans...[/bold magenta]\"\n                    )\n                    chain_plans = self.plan_generator._generate_chain_plans(\n                        op_config, input_data\n                    )\n                    candidate_plans.update(chain_plans)\n            elif plan_type == \"glean\":\n                self.console.log(\n                    \"[bold magenta]Generating gleaning plans...[/bold magenta]\"\n                )\n                gleaning_plans = self.plan_generator._generate_gleaning_plans(\n                    op_config, validator_prompt\n                )\n                candidate_plans.update(gleaning_plans)\n\n        # Capture candidate plans\n        self.runner.optimizer.captured_output.save_optimizer_output(\n            stage_type=StageType.CANDIDATE_PLANS,\n            output=candidate_plans,\n        )\n\n        self.console.post_optimizer_status(StageType.EVALUATION_RESULTS)\n        self.console.log(\n            f\"[bold magenta]Evaluating {len(candidate_plans)} plans...[/bold magenta]\"\n        )\n\n        results = self._evaluate_plans(\n            candidate_plans, op_config, evaluation_samples, validator_prompt\n        )\n\n        # Select best plan using the centralized method\n        best_plan, best_output, _, pairwise_rankings = self._select_best_plan(\n            results, op_config, evaluation_samples, validator_prompt, candidate_plans\n        )\n\n        # Capture evaluation results with pairwise rankings\n        ratings = {k: v[0] for k, v in results.items()}\n        runtime = {k: v[1] for k, v in results.items()}\n        sample_outputs = {k: v[2] for k, v in results.items()}\n        self.runner.optimizer.captured_output.save_optimizer_output(\n            stage_type=StageType.EVALUATION_RESULTS,\n            output={\n                \"input_data\": evaluation_samples,\n                \"all_plan_ratings\": ratings,\n                \"all_plan_runtimes\": runtime,\n                \"all_plan_sample_outputs\": sample_outputs,\n                \"all_plan_pairwise_rankings\": pairwise_rankings,\n            },\n        )\n\n        self.console.post_optimizer_status(StageType.END)\n        return best_plan, best_output, self.plan_generator.subplan_optimizer_cost\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer.__init__","title":"<code>__init__(runner, run_operation, timeout=10, is_filter=False, depth=1)</code>","text":"<p>Initialize the MapOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>runner</code> <code>Runner</code> <p>The runner object.</p> required <code>run_operation</code> <code>Callable</code> <p>A function to execute operations.</p> required <code>timeout</code> <code>int</code> <p>The timeout in seconds for operation execution. Defaults to 10.</p> <code>10</code> <code>is_filter</code> <code>bool</code> <p>If True, the operation is a filter operation. Defaults to False.</p> <code>False</code> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>def __init__(\n    self,\n    runner,\n    run_operation: Callable,\n    timeout: int = 10,\n    is_filter: bool = False,\n    depth: int = 1,\n):\n    \"\"\"\n    Initialize the MapOptimizer.\n\n    Args:\n        runner (Runner): The runner object.\n        run_operation (Callable): A function to execute operations.\n        timeout (int, optional): The timeout in seconds for operation execution. Defaults to 10.\n        is_filter (bool, optional): If True, the operation is a filter operation. Defaults to False.\n    \"\"\"\n    self.runner = runner\n    self.config = runner.config\n    self.console = runner.console\n    self.llm_client = runner.optimizer.llm_client\n    self._run_operation = run_operation\n    self.max_threads = runner.max_threads\n    self.timeout = runner.optimizer.timeout\n    self._num_plans_to_evaluate_in_parallel = 5\n    self.is_filter = is_filter\n    self.k_to_pairwise_compare = 6\n\n    self.plan_generator = PlanGenerator(\n        runner,\n        self.llm_client,\n        self.console,\n        self.config,\n        run_operation,\n        self.max_threads,\n        is_filter,\n        depth,\n    )\n    self.evaluator = Evaluator(\n        self.llm_client,\n        self.console,\n        self._run_operation,\n        self.timeout,\n        self._num_plans_to_evaluate_in_parallel,\n        self.is_filter,\n    )\n    self.prompt_generator = PromptGenerator(\n        self.runner,\n        self.llm_client,\n        self.console,\n        self.config,\n        self.max_threads,\n        self.is_filter,\n    )\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer.optimize","title":"<code>optimize(op_config, input_data, plan_types=['chunk', 'proj_synthesis', 'glean'])</code>","text":"<p>Optimize the given operation configuration for the input data. Uses a staged evaluation approach: 1. For data exceeding limits: Try all plan types at once 2. For data within limits:     - First try gleaning/proj synthesis     - Compare with baseline     - Selectively try chunking plans based on initial results</p> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>def optimize(\n    self,\n    op_config: dict[str, Any],\n    input_data: list[dict[str, Any]],\n    plan_types: list[str] | None = [\"chunk\", \"proj_synthesis\", \"glean\"],\n) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n    \"\"\"\n    Optimize the given operation configuration for the input data.\n    Uses a staged evaluation approach:\n    1. For data exceeding limits: Try all plan types at once\n    2. For data within limits:\n        - First try gleaning/proj synthesis\n        - Compare with baseline\n        - Selectively try chunking plans based on initial results\n    \"\"\"\n    # Verify that the plan types are valid\n    for plan_type in plan_types:\n        if plan_type not in [\"chunk\", \"proj_synthesis\", \"glean\"]:\n            raise ValueError(\n                f\"Invalid plan type: {plan_type}. Valid plan types are: chunk, proj_synthesis, glean.\"\n            )\n\n    (\n        input_data,\n        output_data,\n        model_input_context_length,\n        no_change_runtime,\n        validator_prompt,\n        assessment,\n        data_exceeds_limit,\n    ) = self._should_optimize_helper(op_config, input_data)\n\n    if not self.config.get(\"optimizer_config\", {}).get(\"force_decompose\", False):\n        if not data_exceeds_limit and not assessment.get(\"needs_improvement\", True):\n            self.console.log(\n                f\"[green]No improvement needed for operation {op_config['name']}[/green]\"\n            )\n            return (\n                [op_config],\n                output_data,\n                self.plan_generator.subplan_optimizer_cost,\n            )\n\n    # Select consistent evaluation samples\n    num_evaluations = min(5, len(input_data))\n    evaluation_samples = select_evaluation_samples(input_data, num_evaluations)\n\n    if data_exceeds_limit:\n        # For data exceeding limits, try all plan types at once\n        return self._evaluate_all_plans(\n            op_config,\n            input_data,\n            evaluation_samples,\n            validator_prompt,\n            plan_types,\n            model_input_context_length,\n            data_exceeds_limit=True,\n        )\n\n    # For data within limits, use staged evaluation\n    return self._staged_evaluation(\n        op_config,\n        input_data,\n        evaluation_samples,\n        validator_prompt,\n        plan_types,\n        no_change_runtime,\n        model_input_context_length,\n    )\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer.should_optimize","title":"<code>should_optimize(op_config, input_data)</code>","text":"<p>Determine if the given operation configuration should be optimized.</p> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>def should_optimize(\n    self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n) -&gt; tuple[str, list[dict[str, Any]], list[dict[str, Any]]]:\n    \"\"\"\n    Determine if the given operation configuration should be optimized.\n    \"\"\"\n    (\n        input_data,\n        output_data,\n        _,\n        _,\n        validator_prompt,\n        assessment,\n        data_exceeds_limit,\n    ) = self._should_optimize_helper(op_config, input_data)\n    if data_exceeds_limit or assessment.get(\"needs_improvement\", True):\n        assessment_str = (\n            \"\\n\".join(assessment.get(\"reasons\", []))\n            + \"\\n\\nHere are some improvements that may help:\\n\"\n            + \"\\n\".join(assessment.get(\"improvements\", []))\n        )\n        if data_exceeds_limit:\n            assessment_str += \"\\nAlso, the input data exceeds the token limit.\"\n        return assessment_str, input_data, output_data\n    else:\n        return \"\", input_data, output_data\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.reduce_optimizer.ReduceOptimizer","title":"<code>docetl.optimizers.reduce_optimizer.ReduceOptimizer</code>","text":"<p>A class that optimizes reduce operations in data processing pipelines.</p> <p>This optimizer analyzes the input and output of a reduce operation, creates and evaluates multiple reduce plans, and selects the best plan for optimizing the operation's performance.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the optimizer.</p> <code>console</code> <code>Console</code> <p>Rich console object for pretty printing.</p> <code>llm_client</code> <code>LLMClient</code> <p>Client for interacting with a language model.</p> <code>_run_operation</code> <code>Callable</code> <p>Function to run an operation.</p> <code>max_threads</code> <code>int</code> <p>Maximum number of threads to use for parallel processing.</p> <code>num_fold_prompts</code> <code>int</code> <p>Number of fold prompts to generate.</p> <code>num_samples_in_validation</code> <code>int</code> <p>Number of samples to use in validation.</p> Source code in <code>docetl/optimizers/reduce_optimizer.py</code> <pre><code>class ReduceOptimizer:\n    \"\"\"\n    A class that optimizes reduce operations in data processing pipelines.\n\n    This optimizer analyzes the input and output of a reduce operation, creates and evaluates\n    multiple reduce plans, and selects the best plan for optimizing the operation's performance.\n\n    Attributes:\n        config (dict[str, Any]): Configuration dictionary for the optimizer.\n        console (Console): Rich console object for pretty printing.\n        llm_client (LLMClient): Client for interacting with a language model.\n        _run_operation (Callable): Function to run an operation.\n        max_threads (int): Maximum number of threads to use for parallel processing.\n        num_fold_prompts (int): Number of fold prompts to generate.\n        num_samples_in_validation (int): Number of samples to use in validation.\n    \"\"\"\n\n    def __init__(\n        self,\n        runner,\n        run_operation: Callable,\n        num_fold_prompts: int = 1,\n        num_samples_in_validation: int = 10,\n    ):\n        \"\"\"\n        Initialize the ReduceOptimizer.\n\n        Args:\n            config (dict[str, Any]): Configuration dictionary for the optimizer.\n            console (Console): Rich console object for pretty printing.\n            llm_client (LLMClient): Client for interacting with a language model.\n            max_threads (int): Maximum number of threads to use for parallel processing.\n            run_operation (Callable): Function to run an operation.\n            num_fold_prompts (int, optional): Number of fold prompts to generate. Defaults to 1.\n            num_samples_in_validation (int, optional): Number of samples to use in validation. Defaults to 10.\n        \"\"\"\n        self.runner = runner\n        self.config = self.runner.config\n        self.console = self.runner.console\n        self.llm_client = self.runner.optimizer.llm_client\n        self._run_operation = run_operation\n        self.max_threads = self.runner.max_threads\n        self.num_fold_prompts = num_fold_prompts\n        self.num_samples_in_validation = num_samples_in_validation\n        self.status = self.runner.status\n\n    def should_optimize_helper(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; str:\n        # Check if we're running out of token limits for the reduce prompt\n        model = op_config.get(\"model\", self.config.get(\"default_model\", \"gpt-4o-mini\"))\n        model_input_context_length = model_cost.get(model, {}).get(\n            \"max_input_tokens\", 4096\n        )\n\n        # Find the key with the longest value\n        if op_config[\"reduce_key\"] == [\"_all\"]:\n            sample_key = tuple([\"_all\"])\n        else:\n            longest_key = max(\n                op_config[\"reduce_key\"], key=lambda k: len(str(input_data[0][k]))\n            )\n            sample_key = tuple(\n                input_data[0][k] if k == longest_key else input_data[0][k]\n                for k in op_config[\"reduce_key\"]\n            )\n\n        # Render the prompt with a sample input\n        prompt_template = Template(op_config[\"prompt\"])\n        sample_prompt = prompt_template.render(\n            reduce_key=dict(zip(op_config[\"reduce_key\"], sample_key)),\n            inputs=[input_data[0]],\n        )\n\n        # Count tokens in the sample prompt\n        prompt_tokens = count_tokens(sample_prompt, model)\n\n        self.console.post_optimizer_status(StageType.SAMPLE_RUN)\n        original_output = self._run_operation(op_config, input_data)\n\n        # Step 1: Synthesize a validator prompt\n        self.console.post_optimizer_status(StageType.SHOULD_OPTIMIZE)\n        validator_prompt = self._generate_validator_prompt(\n            op_config, input_data, original_output\n        )\n\n        # Log the validator prompt\n        self.console.log(\"[bold]Validator Prompt:[/bold]\")\n        self.console.log(validator_prompt)\n        self.console.log(\"\\n\")  # Add a newline for better readability\n\n        # Step 2: validate the output\n        validator_inputs = self._create_validation_inputs(\n            input_data, op_config[\"reduce_key\"]\n        )\n        validation_results = self._validate_reduce_output(\n            op_config, validator_inputs, original_output, validator_prompt\n        )\n\n        return (\n            validation_results,\n            prompt_tokens,\n            model_input_context_length,\n            model,\n            validator_prompt,\n            original_output,\n        )\n\n    def should_optimize(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; tuple[str, list[dict[str, Any]], list[dict[str, Any]]]:\n        (\n            validation_results,\n            prompt_tokens,\n            model_input_context_length,\n            model,\n            validator_prompt,\n            original_output,\n        ) = self.should_optimize_helper(op_config, input_data)\n        if prompt_tokens * 1.5 &gt; model_input_context_length:\n            return (\n                \"The reduce prompt is likely to exceed the token limit for model {model}.\",\n                input_data,\n                original_output,\n            )\n\n        if validation_results.get(\"needs_improvement\", False):\n            return (\n                \"\\n\".join(\n                    [\n                        f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                        for result in validation_results[\"validation_results\"]\n                    ]\n                ),\n                input_data,\n                original_output,\n            )\n        else:\n            return \"\", input_data, original_output\n\n    def optimize(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        level: int = 1,\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"\n        Optimize the reduce operation based on the given configuration and input data.\n\n        This method performs the following steps:\n        1. Run the original operation\n        2. Generate a validator prompt\n        3. Validate the output\n        4. If improvement is needed:\n           a. Evaluate if decomposition is beneficial\n           b. If decomposition is beneficial, recursively optimize each sub-operation\n           c. If not, proceed with single operation optimization\n        5. Run the optimized operation(s)\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n\n        Returns:\n            tuple[list[dict[str, Any]], list[dict[str, Any]], float]: A tuple containing the list of optimized configurations\n            and the list of outputs from the optimized operation(s), and the cost of the operation due to synthesizing any resolve operations.\n        \"\"\"\n        (\n            validation_results,\n            prompt_tokens,\n            model_input_context_length,\n            model,\n            validator_prompt,\n            original_output,\n        ) = self.should_optimize_helper(op_config, input_data)\n\n        # add_map_op = False\n        if prompt_tokens * 2 &gt; model_input_context_length:\n            # add_map_op = True\n            self.console.log(\n                f\"[yellow]Warning: The reduce prompt exceeds the token limit for model {model}. \"\n                f\"Token count: {prompt_tokens}, Limit: {model_input_context_length}. \"\n                f\"Add a map operation to the pipeline.[/yellow]\"\n            )\n\n        # # Also query an agent to look at a sample of the inputs and see if they think a map operation would be helpful\n        # preprocessing_steps = \"\"\n        # should_use_map, preprocessing_steps = self._should_use_map(\n        #     op_config, input_data\n        # )\n        # if should_use_map or add_map_op:\n        #     # Synthesize a map operation\n        #     map_prompt, map_output_schema = self._synthesize_map_operation(\n        #         op_config, preprocessing_steps, input_data\n        #     )\n        #     # Change the reduce operation prompt to use the map schema\n        #     new_reduce_prompt = self._change_reduce_prompt_to_use_map_schema(\n        #         op_config[\"prompt\"], map_output_schema\n        #     )\n        #     op_config[\"prompt\"] = new_reduce_prompt\n\n        #     # Return unoptimized map and reduce operations\n        #     return [map_prompt, op_config], input_data, 0.0\n\n        # Print the validation results\n        self.console.log(\"[bold]Validation Results on Initial Sample:[/bold]\")\n        if validation_results[\"needs_improvement\"] or self.config.get(\n            \"optimizer_config\", {}\n        ).get(\"force_decompose\", False):\n            self.console.post_optimizer_rationale(\n                should_optimize=True,\n                rationale=\"\\n\".join(\n                    [\n                        f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                        for result in validation_results[\"validation_results\"]\n                    ]\n                ),\n                validator_prompt=validator_prompt,\n            )\n            self.console.log(\n                \"\\n\".join(\n                    [\n                        f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                        for result in validation_results[\"validation_results\"]\n                    ]\n                )\n            )\n\n            # Step 3: Evaluate if decomposition is beneficial\n            decomposition_result = self._evaluate_decomposition(\n                op_config, input_data, level\n            )\n\n            if decomposition_result[\"should_decompose\"]:\n                return self._optimize_decomposed_reduce(\n                    decomposition_result, op_config, input_data, level\n                )\n\n            return self._optimize_single_reduce(op_config, input_data, validator_prompt)\n        else:\n            self.console.log(f\"No improvements identified; {validation_results}.\")\n            self.console.post_optimizer_rationale(\n                should_optimize=False,\n                rationale=\"No improvements identified; no optimization recommended.\",\n                validator_prompt=validator_prompt,\n            )\n            return [op_config], original_output, 0.0\n\n    def _should_use_map(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; tuple[bool, str]:\n        \"\"\"\n        Determine if a map operation should be used based on the input data.\n        \"\"\"\n        # Sample a random input item\n        sample_input = random.choice(input_data)\n\n        # Format the prompt with the sample input\n        prompt_template = Template(op_config[\"prompt\"])\n        formatted_prompt = prompt_template.render(\n            reduce_key=dict(\n                zip(op_config[\"reduce_key\"], sample_input[op_config[\"reduce_key\"]])\n            ),\n            inputs=[sample_input],\n        )\n\n        # Prepare the message for the LLM\n        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n\n        # Truncate the messages to fit the model's context window\n        truncated_messages = truncate_messages(\n            messages, self.config.get(\"model\", self.default_model)\n        )\n\n        # Query the LLM for preprocessing suggestions\n        preprocessing_prompt = (\n            \"Based on the following reduce operation prompt, should we do any preprocessing on the input data? \"\n            \"Consider if we need to remove unnecessary context, or logically construct an output that will help in the task. \"\n            \"If preprocessing would be beneficial, explain why and suggest specific steps. If not, explain why preprocessing isn't necessary.\\n\\n\"\n            f\"Reduce operation prompt:\\n{truncated_messages[0]['content']}\"\n        )\n\n        preprocessing_response = self.llm_client.generate_rewrite(\n            model=self.config.get(\"model\", self.default_model),\n            messages=[{\"role\": \"user\", \"content\": preprocessing_prompt}],\n            response_format={\n                \"type\": \"json_object\",\n                \"schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"preprocessing_needed\": {\"type\": \"boolean\"},\n                        \"rationale\": {\"type\": \"string\"},\n                        \"suggested_steps\": {\"type\": \"string\"},\n                    },\n                    \"required\": [\n                        \"preprocessing_needed\",\n                        \"rationale\",\n                        \"suggested_steps\",\n                    ],\n                },\n            },\n        )\n\n        preprocessing_result = preprocessing_response.choices[0].message.content\n\n        should_preprocess = preprocessing_result[\"preprocessing_needed\"]\n        preprocessing_rationale = preprocessing_result[\"rationale\"]\n\n        self.console.log(\"[bold]Map-Reduce Decomposition Analysis:[/bold]\")\n        self.console.log(f\"Should write a map operation: {should_preprocess}\")\n        self.console.log(f\"Rationale: {preprocessing_rationale}\")\n\n        if should_preprocess:\n            self.console.log(\n                f\"Suggested steps: {preprocessing_result['suggested_steps']}\"\n            )\n\n        return should_preprocess, preprocessing_result[\"suggested_steps\"]\n\n    def _optimize_single_reduce(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        validator_prompt: str,\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"\n        Optimize a single reduce operation.\n\n        This method performs the following steps:\n        1. Determine and configure value sampling\n        2. Determine if the reduce operation is associative\n        3. Create and evaluate multiple reduce plans\n        4. Run the best reduce plan\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n            validator_prompt (str): The validator prompt for evaluating reduce plans.\n\n        Returns:\n            tuple[list[dict[str, Any]], list[dict[str, Any]], float]: A tuple containing a single-item list with the optimized configuration\n            and a single-item list with the output from the optimized operation, and the cost of the operation due to synthesizing any resolve operations.\n        \"\"\"\n        # Step 1: Determine and configure value sampling (TODO: re-enable this when the agent is more reliable)\n        # value_sampling_config = self._determine_value_sampling(op_config, input_data)\n        # if value_sampling_config[\"enabled\"]:\n        #     op_config[\"value_sampling\"] = value_sampling_config\n        #     self.console.log(\"[bold]Value Sampling Configuration:[/bold]\")\n        #     self.console.log(json.dumps(value_sampling_config, indent=2))\n\n        # Step 2: Determine if the reduce operation is associative\n        is_associative = self._is_associative(op_config, input_data)\n\n        # Step 3: Create and evaluate multiple reduce plans\n        self.console.post_optimizer_status(StageType.CANDIDATE_PLANS)\n        self.console.log(\"[bold magenta]Generating batched plans...[/bold magenta]\")\n        reduce_plans = self._create_reduce_plans(op_config, input_data, is_associative)\n\n        # Create gleaning plans\n        self.console.log(\"[bold magenta]Generating gleaning plans...[/bold magenta]\")\n        gleaning_plans = self._generate_gleaning_plans(reduce_plans, validator_prompt)\n\n        self.console.log(\"[bold magenta]Evaluating plans...[/bold magenta]\")\n        self.console.post_optimizer_status(StageType.EVALUATION_RESULTS)\n        best_plan = self._evaluate_reduce_plans(\n            op_config, reduce_plans + gleaning_plans, input_data, validator_prompt\n        )\n\n        # Step 4: Run the best reduce plan\n        optimized_output = self._run_operation(best_plan, input_data)\n        self.console.post_optimizer_status(StageType.END)\n\n        return [best_plan], optimized_output, 0.0\n\n    def _generate_gleaning_plans(\n        self,\n        plans: list[dict[str, Any]],\n        validation_prompt: str,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Generate plans that use gleaning for the given operation.\n\n        Gleaning involves iteratively refining the output of an operation\n        based on validation feedback. This method creates plans with different\n        numbers of gleaning rounds.\n\n        Args:\n            plans (list[dict[str, Any]]): The list of plans to use for gleaning.\n            validation_prompt (str): The prompt used for validating the operation's output.\n\n        Returns:\n            dict[str, list[dict[str, Any]]]: A dictionary of gleaning plans, where each key\n            is a plan name and each value is a list containing a single operation configuration\n            with gleaning parameters.\n\n        \"\"\"\n        # Generate an op with gleaning num_rounds and validation_prompt\n        gleaning_plans = []\n        gleaning_rounds = [1]\n        biggest_batch_size = max([plan[\"fold_batch_size\"] for plan in plans])\n        for plan in plans:\n            if plan[\"fold_batch_size\"] != biggest_batch_size:\n                continue\n            for gleaning_round in gleaning_rounds:\n                plan_copy = copy.deepcopy(plan)\n                plan_copy[\"gleaning\"] = {\n                    \"num_rounds\": gleaning_round,\n                    \"validation_prompt\": validation_prompt,\n                }\n                plan_name = f\"gleaning_{gleaning_round}_rounds_{plan['name']}\"\n                plan_copy[\"name\"] = plan_name\n                gleaning_plans.append(plan_copy)\n        return gleaning_plans\n\n    def _optimize_decomposed_reduce(\n        self,\n        decomposition_result: dict[str, Any],\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        level: int,\n    ) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n        \"\"\"\n        Optimize a decomposed reduce operation.\n\n        This method performs the following steps:\n        1. Group the input data by the sub-group key.\n        2. Optimize the first reduce operation.\n        3. Run the optimized first reduce operation on all groups.\n        4. Optimize the second reduce operation using the results of the first.\n        5. Run the optimized second reduce operation.\n\n        Args:\n            decomposition_result (dict[str, Any]): The result of the decomposition evaluation.\n            op_config (dict[str, Any]): The original reduce operation configuration.\n            input_data (list[dict[str, Any]]): The input data for the reduce operation.\n            level (int): The current level of decomposition.\n        Returns:\n            tuple[list[dict[str, Any]], list[dict[str, Any]], float]: A tuple containing the list of optimized configurations\n            for both reduce operations and the final output of the second reduce operation, and the cost of the operation due to synthesizing any resolve operations.\n        \"\"\"\n        sub_group_key = decomposition_result[\"sub_group_key\"]\n        first_reduce_prompt = decomposition_result[\"first_reduce_prompt\"]\n        second_reduce_prompt = decomposition_result[\"second_reduce_prompt\"]\n        pipeline = []\n        all_cost = 0.0\n\n        first_reduce_config = op_config.copy()\n        first_reduce_config[\"prompt\"] = first_reduce_prompt\n        if isinstance(op_config[\"reduce_key\"], list):\n            first_reduce_config[\"reduce_key\"] = [sub_group_key] + op_config[\n                \"reduce_key\"\n            ]\n        else:\n            first_reduce_config[\"reduce_key\"] = [sub_group_key, op_config[\"reduce_key\"]]\n        first_reduce_config[\"pass_through\"] = True\n\n        if first_reduce_config.get(\"synthesize_resolve\", True):\n            resolve_config = {\n                \"name\": f\"synthesized_resolve_{uuid.uuid4().hex[:8]}\",\n                \"type\": \"resolve\",\n                \"empty\": True,\n                \"embedding_model\": \"text-embedding-3-small\",\n                \"resolution_model\": self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                \"comparison_model\": self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                \"_intermediates\": {\n                    \"map_prompt\": op_config.get(\"_intermediates\", {}).get(\n                        \"last_map_prompt\"\n                    ),\n                    \"reduce_key\": first_reduce_config[\"reduce_key\"],\n                },\n            }\n            optimized_resolve_config, resolve_cost = JoinOptimizer(\n                self.runner,\n                self.config,\n                resolve_config,\n                self.console,\n                self.llm_client,\n                self.max_threads,\n            ).optimize_resolve(input_data)\n            all_cost += resolve_cost\n\n            if not optimized_resolve_config.get(\"empty\", False):\n                # Add this to the pipeline\n                pipeline += [optimized_resolve_config]\n\n                # Run the resolver\n                optimized_output = self._run_operation(\n                    optimized_resolve_config, input_data\n                )\n                input_data = optimized_output\n\n        first_optimized_configs, first_outputs, first_cost = self.optimize(\n            first_reduce_config, input_data, level + 1\n        )\n        pipeline += first_optimized_configs\n        all_cost += first_cost\n\n        # Optimize second reduce operation\n        second_reduce_config = op_config.copy()\n        second_reduce_config[\"prompt\"] = second_reduce_prompt\n        second_reduce_config[\"pass_through\"] = True\n\n        second_optimized_configs, second_outputs, second_cost = self.optimize(\n            second_reduce_config, first_outputs, level + 1\n        )\n\n        # Combine optimized configs and return with final output\n        pipeline += second_optimized_configs\n        all_cost += second_cost\n\n        return pipeline, second_outputs, all_cost\n\n    def _evaluate_decomposition(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        level: int = 1,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Evaluate whether decomposing the reduce operation would be beneficial.\n\n        This method first determines if decomposition would be helpful, and if so,\n        it then determines the sub-group key and prompts for the decomposed operations.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n            level (int): The current level of decomposition.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the decomposition decision and details.\n        \"\"\"\n        should_decompose = self._should_decompose(op_config, input_data, level)\n\n        # Log the decomposition decision\n        if should_decompose[\"should_decompose\"]:\n            self.console.log(\n                f\"[bold green]Decomposition recommended:[/bold green] {should_decompose['explanation']}\"\n            )\n        else:\n            self.console.log(\n                f\"[bold yellow]Decomposition not recommended:[/bold yellow] {should_decompose['explanation']}\"\n            )\n\n        # Return early if decomposition is not recommended\n        if not should_decompose[\"should_decompose\"]:\n            return should_decompose\n\n        # Temporarily stop the status\n        if self.status:\n            self.status.stop()\n\n        # Ask user if they agree with the decomposition assessment\n        user_agrees = Confirm.ask(\n            f\"Do you agree with the decomposition assessment? \"\n            f\"[bold]{'Recommended' if should_decompose['should_decompose'] else 'Not recommended'}[/bold]\",\n            console=self.console,\n        )\n\n        # If user disagrees, invert the decomposition decision\n        if not user_agrees:\n            should_decompose[\"should_decompose\"] = not should_decompose[\n                \"should_decompose\"\n            ]\n            should_decompose[\"explanation\"] = (\n                \"User disagreed with the initial assessment.\"\n            )\n\n        # Restart the status\n        if self.status:\n            self.status.start()\n\n        # Return if decomposition is not recommended\n        if not should_decompose[\"should_decompose\"]:\n            return should_decompose\n\n        decomposition_details = self._get_decomposition_details(op_config, input_data)\n        result = {**should_decompose, **decomposition_details}\n        if decomposition_details[\"sub_group_key\"] in op_config[\"reduce_key\"]:\n            result[\"should_decompose\"] = False\n            result[\n                \"explanation\"\n            ] += \" However, the suggested sub-group key is already part of the current reduce key(s), so decomposition is not recommended.\"\n            result[\"sub_group_key\"] = \"\"\n\n        return result\n\n    def _should_decompose(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        level: int = 1,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Determine if decomposing the reduce operation would be beneficial.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n            level (int): The current level of decomposition.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the decomposition decision and explanation.\n        \"\"\"\n        # TODO: we have not enabled recursive decomposition yet\n        if level &gt; 1 and not op_config.get(\"recursively_optimize\", False):\n            return {\n                \"should_decompose\": False,\n                \"explanation\": \"Recursive decomposition is not enabled.\",\n            }\n\n        system_prompt = (\n            \"You are an AI assistant tasked with optimizing data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(10, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        # Get all keys from the input data\n        all_keys = set().union(*(item.keys() for item in sample_input))\n        reduce_key = op_config[\"reduce_key\"]\n        reduce_keys = [reduce_key] if isinstance(reduce_key, str) else reduce_key\n        other_keys = [key for key in all_keys if key not in reduce_keys]\n\n        # See if there's an input schema and constrain the sample_input to that schema\n        input_schema = op_config.get(\"input\", {}).get(\"schema\", {})\n        if input_schema:\n            sample_input = [\n                {key: item[key] for key in input_schema} for item in sample_input\n            ]\n\n        # Create a sample of values for other keys\n        sample_values = {\n            key: list(set(str(item.get(key))[:50] for item in sample_input))[:5]\n            for key in other_keys\n        }\n\n        prompt = f\"\"\"Analyze the following reduce operation and determine if it should be decomposed into two reduce operations chained together:\n\n        Reduce Operation Prompt:\n        ```\n        {op_config['prompt']}\n        ```\n\n        Current Reduce Key(s): {reduce_keys}\n        Other Available Keys: {', '.join(other_keys)}\n\n        Sample values for other keys:\n        {json.dumps(sample_values, indent=2)}\n\n        Based on this information, determine if it would be beneficial to decompose this reduce operation into a sub-reduce operation followed by a final reduce operation. Consider ALL of the following:\n\n        1. Is there a natural hierarchy in the data (e.g., country -&gt; state -&gt; city) among the other available keys, with a key at a finer level of granularity than the current reduce key(s)?\n        2. Are the current reduce key(s) some form of ID, and are there many different types of inputs for that ID among the other available keys?\n        3. Does the prompt implicitly ask for sub-grouping based on the other available keys (e.g., \"summarize policies by state, then by country\")?\n        4. Would splitting the operation improve accuracy (i.e., make sure information isn't lost when reducing)?\n        5. Are all the keys of the potential hierarchy provided in the other available keys? If not, we should not decompose.\n        6. Importantly, do not suggest decomposition using any key that is already part of the current reduce key(s). We are looking for a new key from the other available keys to use for sub-grouping.\n        7. Do not suggest keys that don't contain meaningful information (e.g., id-related keys).\n\n        Provide your analysis in the following format:\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"should_decompose\": {\"type\": \"boolean\"},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"should_decompose\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)\n\n    def _get_decomposition_details(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Determine the sub-group key and prompts for decomposed reduce operations.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the sub-group key and prompts for decomposed operations.\n        \"\"\"\n        system_prompt = (\n            \"You are an AI assistant tasked with optimizing data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(10, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        # Get all keys from the input data\n        all_keys = set().union(*(item.keys() for item in sample_input))\n        reduce_key = op_config[\"reduce_key\"]\n        reduce_keys = [reduce_key] if isinstance(reduce_key, str) else reduce_key\n        other_keys = [key for key in all_keys if key not in reduce_keys]\n\n        prompt = f\"\"\"Given that we've decided to decompose the following reduce operation, suggest a two-step reduce process:\n\n        Reduce Operation Prompt:\n        ```\n        {op_config['prompt']}\n        ```\n\n        Reduce Key(s): {reduce_key}\n        Other Keys: {', '.join(other_keys)}\n\n        Provide the following:\n        1. A sub-group key to use for the first reduce operation\n        2. A prompt for the first reduce operation\n        3. A prompt for the second (final) reduce operation\n\n        For the reduce operation prompts, you should only minimally modify the original prompt. The prompts should be Jinja templates, and the only variables they can access are the `reduce_key` and `inputs` variables.\n\n        Provide your suggestions in the following format:\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sub_group_key\": {\"type\": \"string\"},\n                \"first_reduce_prompt\": {\"type\": \"string\"},\n                \"second_reduce_prompt\": {\"type\": \"string\"},\n            },\n            \"required\": [\n                \"sub_group_key\",\n                \"first_reduce_prompt\",\n                \"second_reduce_prompt\",\n            ],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)\n\n    def _determine_value_sampling(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Determine whether value sampling should be enabled and configure its parameters.\n        \"\"\"\n        system_prompt = (\n            \"You are an AI assistant helping to optimize data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(100, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        prompt = f\"\"\"\n        Analyze the following reduce operation and determine if value sampling should be enabled:\n\n        Reduce Operation Prompt:\n        {op_config['prompt']}\n\n        Sample Input Data (first 2 items):\n        {json.dumps(sample_input[:2], indent=2)}\n\n        Value sampling is appropriate for reduce operations that don't need to look at all the values for each key to produce a good result, such as generic summarization tasks.\n\n        Based on the reduce operation prompt and the sample input data, determine if value sampling should be enabled.\n        Answer with 'yes' if value sampling should be enabled or 'no' if it should not be enabled. Explain your reasoning briefly.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"enable_sampling\": {\"type\": \"boolean\"},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"enable_sampling\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        result = json.loads(response.choices[0].message.content)\n\n        if not result[\"enable_sampling\"]:\n            return {\"enabled\": False}\n\n        # Print the explanation for enabling value sampling\n        self.console.log(f\"Value sampling enabled: {result['explanation']}\")\n\n        # Determine sampling method\n        prompt = f\"\"\"\n        We are optimizing a reduce operation in a data processing pipeline. The reduce operation is defined by the following prompt:\n\n        Reduce Operation Prompt:\n        {op_config['prompt']}\n\n        Sample Input Data (first 2 items):\n        {json.dumps(sample_input[:2], indent=2)}\n\n        We have determined that value sampling should be enabled for this reduce operation. Value sampling is a technique used to process only a subset of the input data for each reduce key, rather than processing all items. This can significantly reduce processing time and costs for very large datasets, especially when the reduce operation doesn't require looking at every single item to produce a good result (e.g., summarization tasks).\n\n        Now we need to choose the most appropriate sampling method. The available methods are:\n\n        1. \"random\": Randomly select a subset of values.\n        Example: In a customer review analysis task, randomly selecting a subset of reviews to summarize the overall sentiment.\n\n        2. \"cluster\": Use K-means clustering to select representative samples.\n        Example: In a document categorization task, clustering documents based on their content and selecting representative documents from each cluster to determine the overall categories.\n\n        3. \"sem_sim\": Use semantic similarity to select the most relevant samples to a query text.\n        Example: In a news article summarization task, selecting articles that are semantically similar to a query like \"Major economic events of {{reduce_key}}\" to produce a focused summary.\n\n        Based on the reduce operation prompt, the nature of the task, and the sample input data, which sampling method would be most appropriate?\n\n        Provide your answer as either \"random\", \"cluster\", or \"sem_sim\", and explain your reasoning in detail. Consider the following in your explanation:\n        - The nature of the reduce task (e.g., summarization, aggregation, analysis)\n        - The structure and content of the input data\n        - The potential benefits and drawbacks of each sampling method for this specific task\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"method\": {\"type\": \"string\", \"enum\": [\"random\", \"cluster\", \"sem_sim\"]},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"method\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        result = json.loads(response.choices[0].message.content)\n        method = result[\"method\"]\n\n        value_sampling_config = {\n            \"enabled\": True,\n            \"method\": method,\n            \"sample_size\": 100,  # Default sample size\n            \"embedding_model\": \"text-embedding-3-small\",\n        }\n\n        if method in [\"cluster\", \"sem_sim\"]:\n            # Determine embedding keys\n            prompt = f\"\"\"\n            For the {method} sampling method, we need to determine which keys from the input data should be used for generating embeddings.\n\n            Input data keys:\n            {', '.join(sample_input[0].keys())}\n\n            Sample Input Data:\n            {json.dumps(sample_input[0], indent=2)[:1000]}...\n\n            Based on the reduce operation prompt and the sample input data, which keys should be used for generating embeddings? Use keys that will create meaningful embeddings (i.e., not id-related keys).\n            Provide your answer as a list of key names that is a subset of the input data keys. You should pick only the 1-3 keys that are necessary for generating meaningful embeddings, that have relatively short values.\n            \"\"\"\n\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"embedding_keys\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    \"explanation\": {\"type\": \"string\"},\n                },\n                \"required\": [\"embedding_keys\", \"explanation\"],\n            }\n\n            response = self.llm_client.generate_rewrite(\n                [{\"role\": \"user\", \"content\": prompt}],\n                system_prompt,\n                parameters,\n            )\n            result = json.loads(response.choices[0].message.content)\n            # TODO: validate that these exist\n            embedding_keys = result[\"embedding_keys\"]\n            for key in result[\"embedding_keys\"]:\n                if key not in sample_input[0]:\n                    embedding_keys.remove(key)\n\n            if not embedding_keys:\n                # Select the reduce key\n                self.console.log(\n                    \"No embedding keys found, selecting reduce key for embedding key\"\n                )\n                embedding_keys = (\n                    op_config[\"reduce_key\"]\n                    if isinstance(op_config[\"reduce_key\"], list)\n                    else [op_config[\"reduce_key\"]]\n                )\n\n            value_sampling_config[\"embedding_keys\"] = embedding_keys\n\n        if method == \"sem_sim\":\n            # Determine query text\n            prompt = f\"\"\"\n            For the semantic similarity (sem_sim) sampling method, we need to determine the query text to compare against when selecting samples.\n\n            Reduce Operation Prompt:\n            {op_config['prompt']}\n\n            The query text should be a Jinja template with access to the `reduce_key` variable.\n            Based on the reduce operation prompt, what would be an appropriate query text for selecting relevant samples?\n            \"\"\"\n\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query_text\": {\"type\": \"string\"},\n                    \"explanation\": {\"type\": \"string\"},\n                },\n                \"required\": [\"query_text\", \"explanation\"],\n            }\n\n            response = self.llm_client.generate_rewrite(\n                [{\"role\": \"user\", \"content\": prompt}],\n                system_prompt,\n                parameters,\n            )\n            result = json.loads(response.choices[0].message.content)\n            value_sampling_config[\"query_text\"] = result[\"query_text\"]\n\n        return value_sampling_config\n\n    def _is_associative(\n        self, op_config: dict[str, Any], input_data: list[dict[str, Any]]\n    ) -&gt; bool:\n        \"\"\"\n        Determine if the reduce operation is associative.\n\n        This method analyzes the reduce operation configuration and a sample of the input data\n        to determine if the operation is associative (i.e., the order of combining elements\n        doesn't affect the final result).\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n\n        Returns:\n            bool: True if the operation is determined to be associative, False otherwise.\n        \"\"\"\n        system_prompt = (\n            \"You are an AI assistant helping to optimize data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(5, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        prompt = f\"\"\"\n        Analyze the following reduce operation and determine if it is associative:\n\n        Reduce Operation Prompt:\n        {op_config['prompt']}\n\n        Sample Input Data:\n        {json.dumps(sample_input, indent=2)[:1000]}...\n\n        Based on the reduce operation prompt, determine whether the order in which we process data matters.\n        Answer with 'yes' if order matters or 'no' if order doesn't matter.\n        Explain your reasoning briefly.\n\n        For example:\n        - Merging extracted key-value pairs from documents does not require order: combining {{\"name\": \"John\", \"age\": 30}} with {{\"city\": \"New York\", \"job\": \"Engineer\"}} yields the same result regardless of order\n        - Generating a timeline of events requires order: the order of events matters for maintaining chronological accuracy.\n\n        Consider these examples when determining whether the order in which we process data matters. You might also have to consider the specific data.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"order_matters\": {\"type\": \"boolean\"},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"order_matters\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        result = json.loads(response.choices[0].message.content)\n        result[\"is_associative\"] = not result[\"order_matters\"]\n\n        self.console.log(\n            f\"[yellow]Reduce operation {'is associative' if result['is_associative'] else 'is not associative'}.[/yellow] Analysis: {result['explanation']}\"\n        )\n        return result[\"is_associative\"]\n\n    def _generate_validator_prompt(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        original_output: list[dict[str, Any]],\n    ) -&gt; str:\n        \"\"\"\n        Generate a custom validator prompt for assessing the quality of the reduce operation output.\n\n        This method creates a prompt that will be used to validate the output of the reduce operation.\n        It includes specific questions about the quality and completeness of the output.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n            original_output (list[dict[str, Any]]): Original output of the reduce operation.\n\n        Returns:\n            str: A custom validator prompt as a string.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with creating custom validation prompts for reduce operations in data processing pipelines.\"\n\n        sample_input = random.choice(input_data)\n        input_keys = op_config.get(\"input\", {}).get(\"schema\", {})\n        if input_keys:\n            sample_input = {k: sample_input[k] for k in input_keys}\n\n        reduce_key = op_config.get(\"reduce_key\")\n        if reduce_key and original_output:\n            if isinstance(reduce_key, list):\n                key = next(\n                    (\n                        tuple(item[k] for k in reduce_key)\n                        for item in original_output\n                        if all(k in item for k in reduce_key)\n                    ),\n                    tuple(None for _ in reduce_key),\n                )\n                sample_output = next(\n                    (\n                        item\n                        for item in original_output\n                        if all(item.get(k) == v for k, v in zip(reduce_key, key))\n                    ),\n                    {},\n                )\n            else:\n                key = next(\n                    (\n                        item[reduce_key]\n                        for item in original_output\n                        if reduce_key in item\n                    ),\n                    None,\n                )\n                sample_output = next(\n                    (item for item in original_output if item.get(reduce_key) == key),\n                    {},\n                )\n        else:\n            sample_output = original_output[0] if original_output else {}\n\n        output_keys = op_config.get(\"output\", {}).get(\"schema\", {})\n        sample_output = {k: sample_output[k] for k in output_keys}\n\n        prompt = f\"\"\"\n        Analyze the following reduce operation and its input/output:\n\n        Reduce Operation Prompt:\n        {op_config[\"prompt\"]}\n\n        Sample Input (just one item):\n        {json.dumps(sample_input, indent=2)}\n\n        Sample Output:\n        {json.dumps(sample_output, indent=2)}\n\n        Create a custom validator prompt that will assess how well the reduce operation performed its intended task. The prompt should ask specific 2-3 questions about the quality of the output, such as:\n        1. Does the output accurately reflect the aggregation method specified in the task? For example, if finding anomalies, are the identified anomalies actually anomalies?\n        2. Are there any missing fields, unexpected null values, or data type mismatches in the output compared to the expected schema?\n        3. Does the output maintain the key information from the input while appropriately condensing or summarizing it? For instance, in a text summarization task, are the main points preserved?\n        4. How well does the output adhere to any specific formatting requirements mentioned in the original prompt, such as character limits for summaries or specific data types for aggregated values?\n\n        Note that the output may reflect more than just the input provided, since we only provide a one-item sample input. Provide your response as a single string containing the custom validator prompt. The prompt should be tailored to the task and avoid generic criteria. The prompt should not reference a specific value in the sample input, but rather a general property.\n\n        Your prompt should not have any placeholders like {{ reduce_key }} or {{ input_key }}. It should just be a string.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\"validator_prompt\": {\"type\": \"string\"}},\n            \"required\": [\"validator_prompt\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)[\"validator_prompt\"]\n\n    def _validate_reduce_output(\n        self,\n        op_config: dict[str, Any],\n        validation_inputs: dict[Any, list[dict[str, Any]]],\n        output_data: list[dict[str, Any]],\n        validator_prompt: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Validate the output of the reduce operation using the generated validator prompt.\n\n        This method assesses the quality of the reduce operation output by applying the validator prompt\n        to multiple samples of the input and output data.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            validation_inputs (dict[Any, list[dict[str, Any]]]): Validation inputs for the reduce operation.\n            output_data (list[dict[str, Any]]): Output data from the reduce operation.\n            validator_prompt (str): The validator prompt generated earlier.\n\n        Returns:\n            dict[str, Any]: A dictionary containing validation results and a flag indicating if improvement is needed.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with validating the output of reduce operations in data processing pipelines.\"\n\n        validation_results = []\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = []\n            for reduce_key, inputs in validation_inputs.items():\n                if (\n                    op_config[\"reduce_key\"] == [\"_all\"]\n                    or op_config[\"reduce_key\"] == \"_all\"\n                ):\n                    sample_output = output_data[0]\n                elif isinstance(op_config[\"reduce_key\"], list):\n                    sample_output = next(\n                        (\n                            item\n                            for item in output_data\n                            if all(\n                                item[key] == reduce_key[i]\n                                for i, key in enumerate(op_config[\"reduce_key\"])\n                            )\n                        ),\n                        None,\n                    )\n                else:\n                    sample_output = next(\n                        (\n                            item\n                            for item in output_data\n                            if item[op_config[\"reduce_key\"]] == reduce_key\n                        ),\n                        None,\n                    )\n\n                if sample_output is None:\n                    self.console.log(\n                        f\"Warning: No output found for reduce key {reduce_key}\"\n                    )\n                    continue\n\n                input_str = json.dumps(inputs, indent=2)\n                # truncate input_str to 40,000 words\n                input_str = input_str.split()[:40000]\n                input_str = \" \".join(input_str) + \"...\"\n\n                prompt = f\"\"\"{validator_prompt}\n\n                Reduce Operation Task:\n                {op_config[\"prompt\"]}\n\n                Input Data Samples:\n                {input_str}\n\n                Output Data Sample:\n                {json.dumps(sample_output, indent=2)}\n\n                Based on the validator prompt and the input/output samples, assess the quality (e.g., correctness, completeness) of the reduce operation output.\n                Provide your assessment in the following format:\n                \"\"\"\n\n                parameters = {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"is_correct\": {\"type\": \"boolean\"},\n                        \"issues\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"suggestions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    },\n                    \"required\": [\"is_correct\", \"issues\", \"suggestions\"],\n                }\n\n                futures.append(\n                    executor.submit(\n                        self.llm_client.generate_judge,\n                        [{\"role\": \"user\", \"content\": prompt}],\n                        system_prompt,\n                        parameters,\n                    )\n                )\n\n            for future, (reduce_key, inputs) in zip(futures, validation_inputs.items()):\n                response = future.result()\n                result = json.loads(response.choices[0].message.content)\n                validation_results.append(result)\n\n        # Determine if optimization is needed based on validation results\n        invalid_count = sum(\n            1 for result in validation_results if not result[\"is_correct\"]\n        )\n        needs_improvement = invalid_count &gt; 1 or (\n            invalid_count == 1 and len(validation_results) == 1\n        )\n\n        return {\n            \"needs_improvement\": needs_improvement,\n            \"validation_results\": validation_results,\n        }\n\n    def _create_validation_inputs(\n        self, input_data: list[dict[str, Any]], reduce_key: str | list[str]\n    ) -&gt; dict[Any, list[dict[str, Any]]]:\n        # Group input data by reduce_key\n        grouped_data = {}\n        if reduce_key == [\"_all\"]:\n            # Put all data in one group under a single key\n            grouped_data[(\"_all\",)] = input_data\n        else:\n            # Group by reduce key(s) as before\n            for item in input_data:\n                if isinstance(reduce_key, list):\n                    key = tuple(item[k] for k in reduce_key)\n                else:\n                    key = item[reduce_key]\n                if key not in grouped_data:\n                    grouped_data[key] = []\n                grouped_data[key].append(item)\n\n        # Select a fixed number of reduce keys\n        selected_keys = random.sample(\n            list(grouped_data.keys()),\n            min(self.num_samples_in_validation, len(grouped_data)),\n        )\n\n        # Create a new dict with only the selected keys\n        validation_inputs = {key: grouped_data[key] for key in selected_keys}\n\n        return validation_inputs\n\n    def _create_reduce_plans(\n        self,\n        op_config: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        is_associative: bool,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Create multiple reduce plans based on the input data and operation configuration.\n\n        This method generates various reduce plans by varying batch sizes and fold prompts.\n        It takes into account the LLM's context window size to determine appropriate batch sizes.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            input_data (list[dict[str, Any]]): Input data for the reduce operation.\n            is_associative (bool): Flag indicating whether the reduce operation is associative.\n\n        Returns:\n            list[dict[str, Any]]: A list of reduce plans, each with different batch sizes and fold prompts.\n        \"\"\"\n        model = op_config.get(\"model\", \"gpt-4o-mini\")\n        model_input_context_length = model_cost.get(model, {}).get(\n            \"max_input_tokens\", 8192\n        )\n\n        # Estimate tokens for prompt, input, and output\n        prompt_tokens = count_tokens(op_config[\"prompt\"], model)\n        sample_input = input_data[:100]\n        sample_output = self._run_operation(op_config, input_data[:100])\n\n        prompt_vars = extract_jinja_variables(op_config[\"prompt\"])\n        prompt_vars = [var.split(\".\")[-1] for var in prompt_vars]\n        avg_input_tokens = mean(\n            [\n                count_tokens(\n                    json.dumps({k: item[k] for k in prompt_vars if k in item}), model\n                )\n                for item in sample_input\n            ]\n        )\n        avg_output_tokens = mean(\n            [\n                count_tokens(\n                    json.dumps({k: item[k] for k in prompt_vars if k in item}), model\n                )\n                for item in sample_output\n            ]\n        )\n\n        # Calculate max batch size that fits in context window\n        max_batch_size = (\n            model_input_context_length - prompt_tokens - avg_output_tokens\n        ) // avg_input_tokens\n\n        # Generate 6 candidate batch sizes\n        batch_sizes = [\n            max(1, int(max_batch_size * ratio))\n            for ratio in [0.1, 0.2, 0.4, 0.6, 0.75, 0.9]\n        ]\n        # Log the generated batch sizes\n        self.console.log(\"[cyan]Generating plans for batch sizes:[/cyan]\")\n        for size in batch_sizes:\n            self.console.log(f\"  - {size}\")\n        batch_sizes = sorted(set(batch_sizes))  # Remove duplicates and sort\n\n        plans = []\n\n        # Generate multiple fold prompts\n        max_retries = 5\n        retry_count = 0\n        fold_prompts = []\n\n        while retry_count &lt; max_retries and not fold_prompts:\n            try:\n                fold_prompts = self._synthesize_fold_prompts(\n                    op_config,\n                    sample_input,\n                    sample_output,\n                    num_prompts=self.num_fold_prompts,\n                )\n                fold_prompts = list(set(fold_prompts))\n                if not fold_prompts:\n                    raise ValueError(\"No fold prompts generated\")\n            except Exception as e:\n                retry_count += 1\n                if retry_count == max_retries:\n                    raise RuntimeError(\n                        f\"Failed to generate fold prompts after {max_retries} attempts: {str(e)}\"\n                    )\n                self.console.log(\n                    f\"Retry {retry_count}/{max_retries}: Failed to generate fold prompts. Retrying...\"\n                )\n\n        for batch_size in batch_sizes:\n            for fold_idx, fold_prompt in enumerate(fold_prompts):\n                plan = op_config.copy()\n                plan[\"fold_prompt\"] = fold_prompt\n                plan[\"fold_batch_size\"] = batch_size\n                plan[\"associative\"] = is_associative\n                plan[\"name\"] = f\"{op_config['name']}_bs_{batch_size}_fp_{fold_idx}\"\n                plans.append(plan)\n\n        return plans\n\n    def _calculate_compression_ratio(\n        self,\n        op_config: dict[str, Any],\n        sample_input: list[dict[str, Any]],\n        sample_output: list[dict[str, Any]],\n    ) -&gt; float:\n        \"\"\"\n        Calculate the compression ratio of the reduce operation.\n\n        This method compares the size of the input data to the size of the output data\n        to determine how much the data is being compressed by the reduce operation.\n\n        Args:\n            op_config (dict[str, Any]): Configuration for the reduce operation.\n            sample_input (list[dict[str, Any]]): Sample input data.\n            sample_output (list[dict[str, Any]]): Sample output data.\n\n        Returns:\n            float: The calculated compression ratio.\n        \"\"\"\n        reduce_key = op_config[\"reduce_key\"]\n        input_schema = op_config.get(\"input\", {}).get(\"schema\", {})\n        output_schema = op_config[\"output\"][\"schema\"]\n        model = op_config.get(\"model\", \"gpt-4o-mini\")\n\n        compression_ratios = {}\n\n        # Handle both single key and list of keys\n        if isinstance(reduce_key, list):\n            distinct_keys = set(\n                tuple(item[k] for k in reduce_key) for item in sample_input\n            )\n        else:\n            distinct_keys = set(item[reduce_key] for item in sample_input)\n\n        for key in distinct_keys:\n            if isinstance(reduce_key, list):\n                key_input = [\n                    item\n                    for item in sample_input\n                    if tuple(item[k] for k in reduce_key) == key\n                ]\n                key_output = [\n                    item\n                    for item in sample_output\n                    if tuple(item[k] for k in reduce_key) == key\n                ]\n            else:\n                key_input = [item for item in sample_input if item[reduce_key] == key]\n                key_output = [item for item in sample_output if item[reduce_key] == key]\n\n            if input_schema:\n                key_input_tokens = sum(\n                    count_tokens(\n                        json.dumps({k: item[k] for k in input_schema if k in item}),\n                        model,\n                    )\n                    for item in key_input\n                )\n            else:\n                key_input_tokens = sum(\n                    count_tokens(json.dumps(item), model) for item in key_input\n                )\n\n            key_output_tokens = sum(\n                count_tokens(\n                    json.dumps({k: item[k] for k in output_schema if k in item}), model\n                )\n                for item in key_output\n            )\n\n            compression_ratios[key] = (\n                key_output_tokens / key_input_tokens if key_input_tokens &gt; 0 else 1\n            )\n\n        if not compression_ratios:\n            return 1\n\n        # Calculate importance weights based on the number of items for each key\n        total_items = len(sample_input)\n        if isinstance(reduce_key, list):\n            importance_weights = {\n                key: len(\n                    [\n                        item\n                        for item in sample_input\n                        if tuple(item[k] for k in reduce_key) == key\n                    ]\n                )\n                / total_items\n                for key in compression_ratios\n            }\n        else:\n            importance_weights = {\n                key: len([item for item in sample_input if item[reduce_key] == key])\n                / total_items\n                for key in compression_ratios\n            }\n\n        # Calculate weighted average of compression ratios\n        weighted_sum = sum(\n            compression_ratios[key] * importance_weights[key]\n            for key in compression_ratios\n        )\n        return weighted_sum\n\n    def _synthesize_fold_prompts(\n        self,\n        op_config: dict[str, Any],\n        sample_input: list[dict[str, Any]],\n        sample_output: list[dict[str, Any]],\n        num_prompts: int = 2,\n    ) -&gt; list[str]:\n        \"\"\"\n        Synthesize fold prompts for the reduce operation. We generate multiple\n        fold prompts in case one is bad.\n\n        A fold operation is a higher-order function that iterates through a data structure,\n        accumulating the results of applying a given combining operation to its elements.\n        In the context of reduce operations, folding allows processing of data in batches,\n        which can significantly improve performance for large datasets.\n\n        This method generates multiple fold prompts that can be used to optimize the reduce operation\n        by allowing it to run on batches of inputs. It uses the language model to create prompts\n        that are variations of the original reduce prompt, adapted for folding operations.\n\n        Args:\n            op_config (dict[str, Any]): The configuration of the reduce operation.\n            sample_input (list[dict[str, Any]]): A sample of the input data.\n            sample_output (list[dict[str, Any]]): A sample of the output data.\n            num_prompts (int, optional): The number of fold prompts to generate. Defaults to 2.\n\n        Returns:\n            list[str]: A list of synthesized fold prompts.\n\n        The method performs the following steps:\n        1. Sets up the system prompt and parameters for the language model.\n        2. Defines a function to get random examples from the sample data.\n        3. Creates a prompt template for generating fold prompts.\n        4. Uses multi-threading to generate multiple fold prompts in parallel.\n        5. Returns the list of generated fold prompts.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with creating a fold prompt for reduce operations in data processing pipelines.\"\n        original_prompt = op_config[\"prompt\"]\n\n        input_schema = op_config.get(\"input\", {}).get(\"schema\", {})\n        output_schema = op_config[\"output\"][\"schema\"]\n\n        def get_random_examples():\n            reduce_key = op_config[\"reduce_key\"]\n            reduce_key = (\n                list(reduce_key) if not isinstance(reduce_key, list) else reduce_key\n            )\n\n            if reduce_key == [\"_all\"]:\n                # For _all case, just pick random input and output examples\n                input_example = random.choice(sample_input)\n                output_example = random.choice(sample_output)\n            elif isinstance(reduce_key, list):\n                random_key = tuple(\n                    random.choice(\n                        [\n                            tuple(item[k] for k in reduce_key if k in item)\n                            for item in sample_input\n                            if all(k in item for k in reduce_key)\n                        ]\n                    )\n                )\n                input_example = random.choice(\n                    [\n                        item\n                        for item in sample_input\n                        if all(item.get(k) == v for k, v in zip(reduce_key, random_key))\n                    ]\n                )\n                output_example = random.choice(\n                    [\n                        item\n                        for item in sample_output\n                        if all(item.get(k) == v for k, v in zip(reduce_key, random_key))\n                    ]\n                )\n\n            if input_schema:\n                input_example = {\n                    k: input_example[k] for k in input_schema if k in input_example\n                }\n            output_example = {\n                k: output_example[k] for k in output_schema if k in output_example\n            }\n            return input_example, output_example\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"fold_prompt\": {\n                    \"type\": \"string\",\n                }\n            },\n            \"required\": [\"fold_prompt\"],\n        }\n\n        def generate_single_prompt():\n            input_example, output_example = get_random_examples()\n            prompt = f\"\"\"\n            Original Reduce Operation Prompt:\n            {original_prompt}\n\n            Sample Input:\n            {json.dumps(input_example, indent=2)}\n\n            Sample Output:\n            {json.dumps(output_example, indent=2)}\n\n            Create a fold prompt for the reduce operation to run on batches of inputs. The fold prompt should:\n            1. Minimally modify the original reduce prompt\n            2. Describe how to combine the new values with the current reduced value\n            3. Be designed to work iteratively, allowing for multiple fold operations. The first iteration will use the original prompt, and all successive iterations will use the fold prompt.\n\n            The fold prompt should be a Jinja2 template with the following variables available:\n            - {{{{ output }}}}: The current reduced value (a dictionary with the current output schema)\n            - {{{{ inputs }}}}: A list of new values to be folded in\n            - {{{{ reduce_key }}}}: The key used for grouping in the reduce operation\n\n            Provide the fold prompt as a string.\n            \"\"\"\n            response = self.llm_client.generate_rewrite(\n                [{\"role\": \"user\", \"content\": prompt}],\n                system_prompt,\n                parameters,\n            )\n            fold_prompt = json.loads(response.choices[0].message.content)[\"fold_prompt\"]\n\n            # Run the operation with the fold prompt\n            # Create a temporary plan with the fold prompt\n            temp_plan = op_config.copy()\n            temp_plan[\"fold_prompt\"] = fold_prompt\n            temp_plan[\"fold_batch_size\"] = min(\n                len(sample_input), 2\n            )  # Use a small batch size for testing\n\n            # Run the operation with the fold prompt\n            try:\n                self._run_operation(\n                    temp_plan, sample_input[: temp_plan[\"fold_batch_size\"]]\n                )\n\n                return fold_prompt\n            except Exception as e:\n                self.console.log(\n                    f\"[red]Error in agent-generated fold prompt: {e}[/red]\"\n                )\n\n                # Create a default fold prompt that instructs folding new data into existing output\n                fold_prompt = f\"\"\"Analyze this batch of data using the following instructions:\n\n{original_prompt}\n\nHowever, instead of starting fresh, fold your analysis into the existing output that has already been generated. The existing output is provided in the 'output' variable below:\n\n{{{{ output }}}}\n\nRemember, you must fold the new data into the existing output, do not start fresh.\"\"\"\n                return fold_prompt\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            fold_prompts = list(\n                executor.map(lambda _: generate_single_prompt(), range(num_prompts))\n            )\n\n        return fold_prompts\n\n    def _evaluate_reduce_plans(\n        self,\n        op_config: dict[str, Any],\n        plans: list[dict[str, Any]],\n        input_data: list[dict[str, Any]],\n        validator_prompt: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Evaluate multiple reduce plans and select the best one.\n\n        This method takes a list of reduce plans, evaluates each one using the input data\n        and a validator prompt, and selects the best plan based on the evaluation scores.\n        It also attempts to create and evaluate a merged plan that enhances the runtime performance\n        of the best plan.\n\n        A merged plan is an optimization technique applied to the best-performing plan\n        that uses the fold operation. It allows the best plan to run even faster by\n        executing parallel folds and then merging the results of these individual folds\n        together. We default to a merge batch size of 2, but one can increase this.\n\n        Args:\n            op_config (dict[str, Any]): The configuration of the reduce operation.\n            plans (list[dict[str, Any]]): A list of reduce plans to evaluate.\n            input_data (list[dict[str, Any]]): The input data to use for evaluation.\n            validator_prompt (str): The prompt to use for validating the output of each plan.\n\n        Returns:\n            dict[str, Any]: The best reduce plan, either the top-performing original plan\n                            or a merged plan if it performs well enough.\n\n        The method performs the following steps:\n        1. Evaluates each plan using multi-threading.\n        2. Sorts the plans based on their evaluation scores.\n        3. Selects the best plan and attempts to create a merged plan.\n        4. Evaluates the merged plan and compares it to the best original plan.\n        5. Returns either the merged plan or the best original plan based on their scores.\n        \"\"\"\n        self.console.log(\"\\n[bold]Evaluating Reduce Plans:[/bold]\")\n        for i, plan in enumerate(plans):\n            self.console.log(f\"Plan {i+1} (batch size: {plan['fold_batch_size']})\")\n\n        plan_scores = []\n        plan_outputs = {}\n\n        # Create a fixed random sample for evaluation\n        sample_size = min(100, len(input_data))\n        evaluation_sample = random.sample(input_data, sample_size)\n\n        # Create a fixed set of validation samples\n        validation_inputs = self._create_validation_inputs(\n            evaluation_sample, plan[\"reduce_key\"]\n        )\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(\n                    self._evaluate_single_plan,\n                    plan,\n                    evaluation_sample,\n                    validator_prompt,\n                    validation_inputs,\n                )\n                for plan in plans\n            ]\n            for future in as_completed(futures):\n                plan, score, output = future.result()\n                plan_scores.append((plan, score))\n                plan_outputs[id(plan)] = output\n\n        # Sort plans by score in descending order, then by fold_batch_size in descending order\n        sorted_plans = sorted(\n            plan_scores, key=lambda x: (x[1], x[0][\"fold_batch_size\"]), reverse=True\n        )\n\n        self.console.log(\"\\n[bold]Reduce Plan Scores:[/bold]\")\n        for i, (plan, score) in enumerate(sorted_plans):\n            self.console.log(\n                f\"Plan {i+1} (batch size: {plan['fold_batch_size']}): {score:.2f}\"\n            )\n\n        best_plan, best_score = sorted_plans[0]\n        self.console.log(\n            f\"\\n[green]Selected best plan with score: {best_score:.2f} and batch size: {best_plan['fold_batch_size']}[/green]\"\n        )\n\n        if op_config.get(\"synthesize_merge\", False):\n            # Create a new plan with merge prompt and updated parameters\n            merged_plan = best_plan.copy()\n\n            # Synthesize merge prompt if it doesn't exist\n            if \"merge_prompt\" not in merged_plan:\n                merged_plan[\"merge_prompt\"] = self._synthesize_merge_prompt(\n                    merged_plan, plan_outputs[id(best_plan)]\n                )\n                # Print the synthesized merge prompt\n                self.console.log(\"\\n[bold]Synthesized Merge Prompt:[/bold]\")\n                self.console.log(merged_plan[\"merge_prompt\"])\n\n            # Set merge_batch_size to 2 and num_parallel_folds to 5\n            merged_plan[\"merge_batch_size\"] = 2\n\n            # Evaluate the merged plan\n            _, merged_plan_score, _, operation_instance = self._evaluate_single_plan(\n                merged_plan,\n                evaluation_sample,\n                validator_prompt,\n                validation_inputs,\n                return_instance=True,\n            )\n\n            # Get the merge and fold times from the operation instance\n            merge_times = operation_instance.merge_times\n            fold_times = operation_instance.fold_times\n            merge_avg_time = mean(merge_times) if merge_times else None\n            fold_avg_time = mean(fold_times) if fold_times else None\n\n            self.console.log(\"\\n[bold]Scores:[/bold]\")\n            self.console.log(f\"Original plan: {best_score:.2f}\")\n            self.console.log(f\"Merged plan: {merged_plan_score:.2f}\")\n\n            # Compare scores and decide which plan to use\n            if merged_plan_score &gt;= best_score * 0.75:\n                self.console.log(\n                    f\"\\n[green]Using merged plan with score: {merged_plan_score:.2f}[/green]\"\n                )\n                if merge_avg_time and fold_avg_time:\n                    merged_plan[\"merge_time\"] = merge_avg_time\n                    merged_plan[\"fold_time\"] = fold_avg_time\n                return merged_plan\n            else:\n                self.console.log(\n                    f\"\\n[yellow]Merged plan quality too low. Using original plan with score: {best_score:.2f}[/yellow]\"\n                )\n                return best_plan\n        else:\n            return best_plan\n\n    def _evaluate_single_plan(\n        self,\n        plan: dict[str, Any],\n        input_data: list[dict[str, Any]],\n        validator_prompt: str,\n        validation_inputs: list[dict[str, Any]],\n        return_instance: bool = False,\n    ) -&gt; (\n        tuple[dict[str, Any], float, list[dict[str, Any]]]\n        | tuple[dict[str, Any], float, list[dict[str, Any]], BaseOperation]\n    ):\n        \"\"\"\n        Evaluate a single reduce plan using the provided input data and validator prompt.\n\n        This method runs the reduce operation with the given plan, validates the output,\n        and calculates a score based on the validation results. The scoring works as follows:\n        1. It counts the number of valid results from the validation.\n        2. The score is calculated as the ratio of valid results to the total number of validation results.\n        3. This produces a score between 0 and 1, where 1 indicates all results were valid, and 0 indicates none were valid.\n\n        TODO: We should come up with a better scoring method here, maybe pairwise comparisons.\n\n        Args:\n            plan (dict[str, Any]): The reduce plan to evaluate.\n            input_data (list[dict[str, Any]]): The input data to use for evaluation.\n            validator_prompt (str): The prompt to use for validating the output.\n            return_instance (bool, optional): Whether to return the operation instance. Defaults to False.\n\n        Returns:\n            tuple[\n                tuple[dict[str, Any], float, list[dict[str, Any]]],\n                tuple[dict[str, Any], float, list[dict[str, Any]], BaseOperation],\n            ]: A tuple containing the plan, its score, the output data, and optionally the operation instance.\n\n        The method performs the following steps:\n        1. Runs the reduce operation with the given plan on the input data.\n        2. Validates the output using the validator prompt.\n        3. Calculates a score based on the validation results.\n        4. Returns the plan, score, output data, and optionally the operation instance.\n        \"\"\"\n        output = self._run_operation(plan, input_data, return_instance)\n        if return_instance:\n            output, operation_instance = output\n\n        validation_result = self._validate_reduce_output(\n            plan, validation_inputs, output, validator_prompt\n        )\n\n        # Calculate a score based on validation results\n        valid_count = sum(\n            1\n            for result in validation_result[\"validation_results\"]\n            if result[\"is_correct\"]\n        )\n        score = valid_count / len(validation_result[\"validation_results\"])\n\n        if return_instance:\n            return plan, score, output, operation_instance\n        else:\n            return plan, score, output\n\n    def _synthesize_merge_prompt(\n        self, plan: dict[str, Any], sample_outputs: list[dict[str, Any]]\n    ) -&gt; str:\n        \"\"\"\n        Synthesize a merge prompt for combining multiple folded outputs in a reduce operation.\n\n        This method generates a merge prompt that can be used to combine the results of multiple\n        parallel fold operations into a single output. It uses the language model to create a prompt\n        that is consistent with the original reduce and fold prompts while addressing the specific\n        requirements of merging multiple outputs.\n\n        Args:\n            plan (dict[str, Any]): The reduce plan containing the original prompt and fold prompt.\n            sample_outputs (list[dict[str, Any]]): Sample outputs from the fold operation to use as examples.\n\n        Returns:\n            str: The synthesized merge prompt as a string.\n\n        The method performs the following steps:\n        1. Sets up the system prompt for the language model.\n        2. Prepares a random sample output to use as an example.\n        3. Creates a detailed prompt for the language model, including the original reduce prompt,\n           fold prompt, sample output, and instructions for creating the merge prompt.\n        4. Uses the language model to generate the merge prompt.\n        5. Returns the generated merge prompt.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with creating a merge prompt for reduce operations in data processing pipelines. The pipeline has a reduce operation, and incrementally folds inputs into a single output. We want to optimize the pipeline for speed by running multiple folds on different inputs in parallel, and then merging the fold outputs into a single output.\"\n\n        output_schema = plan[\"output\"][\"schema\"]\n        random_output = random.choice(sample_outputs)\n        random_output = {\n            k: random_output[k] for k in output_schema if k in random_output\n        }\n\n        prompt = f\"\"\"Reduce Operation Prompt (runs on the first batch of inputs):\n        {plan[\"prompt\"]}\n\n        Fold Prompt (runs on the second and subsequent batches of inputs):\n        {plan[\"fold_prompt\"]}\n\n        Sample output of the fold operation (an input to the merge operation):\n        {json.dumps(random_output, indent=2)}\n\n        Create a merge prompt for the reduce operation to combine 2+ folded outputs. The merge prompt should:\n        1. Give context on the task &amp; fold operations, describing that the prompt will be used to combine multiple outputs from the fold operation (as if the original prompt was run on all inputs at once)\n        2. Describe how to combine multiple folded outputs into a single output\n        3. Minimally deviate from the reduce and fold prompts\n\n        The merge prompt should be a Jinja2 template with the following variables available:\n        - {{ outputs }}: A list of reduced outputs to be merged (each following the output schema). You can access the first output with {{ outputs[0] }} and the second with {{ outputs[1] }}\n\n        Output Schema:\n        {json.dumps(output_schema, indent=2)}\n\n        Provide the merge prompt as a string.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"merge_prompt\": {\n                    \"type\": \"string\",\n                }\n            },\n            \"required\": [\"merge_prompt\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)[\"merge_prompt\"]\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.reduce_optimizer.ReduceOptimizer.__init__","title":"<code>__init__(runner, run_operation, num_fold_prompts=1, num_samples_in_validation=10)</code>","text":"<p>Initialize the ReduceOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the optimizer.</p> required <code>console</code> <code>Console</code> <p>Rich console object for pretty printing.</p> required <code>llm_client</code> <code>LLMClient</code> <p>Client for interacting with a language model.</p> required <code>max_threads</code> <code>int</code> <p>Maximum number of threads to use for parallel processing.</p> required <code>run_operation</code> <code>Callable</code> <p>Function to run an operation.</p> required <code>num_fold_prompts</code> <code>int</code> <p>Number of fold prompts to generate. Defaults to 1.</p> <code>1</code> <code>num_samples_in_validation</code> <code>int</code> <p>Number of samples to use in validation. Defaults to 10.</p> <code>10</code> Source code in <code>docetl/optimizers/reduce_optimizer.py</code> <pre><code>def __init__(\n    self,\n    runner,\n    run_operation: Callable,\n    num_fold_prompts: int = 1,\n    num_samples_in_validation: int = 10,\n):\n    \"\"\"\n    Initialize the ReduceOptimizer.\n\n    Args:\n        config (dict[str, Any]): Configuration dictionary for the optimizer.\n        console (Console): Rich console object for pretty printing.\n        llm_client (LLMClient): Client for interacting with a language model.\n        max_threads (int): Maximum number of threads to use for parallel processing.\n        run_operation (Callable): Function to run an operation.\n        num_fold_prompts (int, optional): Number of fold prompts to generate. Defaults to 1.\n        num_samples_in_validation (int, optional): Number of samples to use in validation. Defaults to 10.\n    \"\"\"\n    self.runner = runner\n    self.config = self.runner.config\n    self.console = self.runner.console\n    self.llm_client = self.runner.optimizer.llm_client\n    self._run_operation = run_operation\n    self.max_threads = self.runner.max_threads\n    self.num_fold_prompts = num_fold_prompts\n    self.num_samples_in_validation = num_samples_in_validation\n    self.status = self.runner.status\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.reduce_optimizer.ReduceOptimizer.optimize","title":"<code>optimize(op_config, input_data, level=1)</code>","text":"<p>Optimize the reduce operation based on the given configuration and input data.</p> <p>This method performs the following steps: 1. Run the original operation 2. Generate a validator prompt 3. Validate the output 4. If improvement is needed:    a. Evaluate if decomposition is beneficial    b. If decomposition is beneficial, recursively optimize each sub-operation    c. If not, proceed with single operation optimization 5. Run the optimized operation(s)</p> <p>Parameters:</p> Name Type Description Default <code>op_config</code> <code>dict[str, Any]</code> <p>Configuration for the reduce operation.</p> required <code>input_data</code> <code>list[dict[str, Any]]</code> <p>Input data for the reduce operation.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>tuple[list[dict[str, Any]], list[dict[str, Any]], float]: A tuple containing the list of optimized configurations</p> <code>list[dict[str, Any]]</code> <p>and the list of outputs from the optimized operation(s), and the cost of the operation due to synthesizing any resolve operations.</p> Source code in <code>docetl/optimizers/reduce_optimizer.py</code> <pre><code>def optimize(\n    self,\n    op_config: dict[str, Any],\n    input_data: list[dict[str, Any]],\n    level: int = 1,\n) -&gt; tuple[list[dict[str, Any]], list[dict[str, Any]], float]:\n    \"\"\"\n    Optimize the reduce operation based on the given configuration and input data.\n\n    This method performs the following steps:\n    1. Run the original operation\n    2. Generate a validator prompt\n    3. Validate the output\n    4. If improvement is needed:\n       a. Evaluate if decomposition is beneficial\n       b. If decomposition is beneficial, recursively optimize each sub-operation\n       c. If not, proceed with single operation optimization\n    5. Run the optimized operation(s)\n\n    Args:\n        op_config (dict[str, Any]): Configuration for the reduce operation.\n        input_data (list[dict[str, Any]]): Input data for the reduce operation.\n\n    Returns:\n        tuple[list[dict[str, Any]], list[dict[str, Any]], float]: A tuple containing the list of optimized configurations\n        and the list of outputs from the optimized operation(s), and the cost of the operation due to synthesizing any resolve operations.\n    \"\"\"\n    (\n        validation_results,\n        prompt_tokens,\n        model_input_context_length,\n        model,\n        validator_prompt,\n        original_output,\n    ) = self.should_optimize_helper(op_config, input_data)\n\n    # add_map_op = False\n    if prompt_tokens * 2 &gt; model_input_context_length:\n        # add_map_op = True\n        self.console.log(\n            f\"[yellow]Warning: The reduce prompt exceeds the token limit for model {model}. \"\n            f\"Token count: {prompt_tokens}, Limit: {model_input_context_length}. \"\n            f\"Add a map operation to the pipeline.[/yellow]\"\n        )\n\n    # # Also query an agent to look at a sample of the inputs and see if they think a map operation would be helpful\n    # preprocessing_steps = \"\"\n    # should_use_map, preprocessing_steps = self._should_use_map(\n    #     op_config, input_data\n    # )\n    # if should_use_map or add_map_op:\n    #     # Synthesize a map operation\n    #     map_prompt, map_output_schema = self._synthesize_map_operation(\n    #         op_config, preprocessing_steps, input_data\n    #     )\n    #     # Change the reduce operation prompt to use the map schema\n    #     new_reduce_prompt = self._change_reduce_prompt_to_use_map_schema(\n    #         op_config[\"prompt\"], map_output_schema\n    #     )\n    #     op_config[\"prompt\"] = new_reduce_prompt\n\n    #     # Return unoptimized map and reduce operations\n    #     return [map_prompt, op_config], input_data, 0.0\n\n    # Print the validation results\n    self.console.log(\"[bold]Validation Results on Initial Sample:[/bold]\")\n    if validation_results[\"needs_improvement\"] or self.config.get(\n        \"optimizer_config\", {}\n    ).get(\"force_decompose\", False):\n        self.console.post_optimizer_rationale(\n            should_optimize=True,\n            rationale=\"\\n\".join(\n                [\n                    f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                    for result in validation_results[\"validation_results\"]\n                ]\n            ),\n            validator_prompt=validator_prompt,\n        )\n        self.console.log(\n            \"\\n\".join(\n                [\n                    f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                    for result in validation_results[\"validation_results\"]\n                ]\n            )\n        )\n\n        # Step 3: Evaluate if decomposition is beneficial\n        decomposition_result = self._evaluate_decomposition(\n            op_config, input_data, level\n        )\n\n        if decomposition_result[\"should_decompose\"]:\n            return self._optimize_decomposed_reduce(\n                decomposition_result, op_config, input_data, level\n            )\n\n        return self._optimize_single_reduce(op_config, input_data, validator_prompt)\n    else:\n        self.console.log(f\"No improvements identified; {validation_results}.\")\n        self.console.post_optimizer_rationale(\n            should_optimize=False,\n            rationale=\"No improvements identified; no optimization recommended.\",\n            validator_prompt=validator_prompt,\n        )\n        return [op_config], original_output, 0.0\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.join_optimizer.JoinOptimizer","title":"<code>docetl.optimizers.join_optimizer.JoinOptimizer</code>","text":"Source code in <code>docetl/optimizers/join_optimizer.py</code> <pre><code>class JoinOptimizer:\n    def __init__(\n        self,\n        runner,\n        op_config: dict[str, Any],\n        target_recall: float = 0.95,\n        sample_size: int = 500,\n        sampling_weight: float = 20,\n        agent_max_retries: int = 5,\n        estimated_selectivity: float | None = None,\n    ):\n        self.runner = runner\n        self.config = runner.config\n        self.op_config = op_config\n        self.llm_client = runner.optimizer.llm_client\n        self.max_threads = runner.max_threads\n        self.console = runner.console\n        self.target_recall = target_recall\n        self.sample_size = sample_size\n        self.sampling_weight = sampling_weight\n        self.agent_max_retries = agent_max_retries\n        self.estimated_selectivity = estimated_selectivity\n        self.console.log(f\"Target Recall: {self.target_recall}\")\n        self.status = self.runner.status\n        self.max_comparison_sampling_attempts = 5\n        self.synthesized_keys = []\n        # if self.estimated_selectivity is not None:\n        #     self.console.log(\n        #         f\"[yellow]Using estimated selectivity of {self.estimated_selectivity}[/yellow]\"\n        #     )\n\n    def _analyze_map_prompt_categorization(self, map_prompt: str) -&gt; tuple[bool, str]:\n        \"\"\"\n        Analyze the map prompt to determine if it's explicitly categorical.\n\n        Args:\n            map_prompt (str): The map prompt to analyze.\n\n        Returns:\n            bool: True if the prompt is explicitly categorical, False otherwise.\n        \"\"\"\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an AI assistant tasked with analyzing prompts for data processing operations.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following map operation prompt and determine if it is explicitly categorical,\n                meaning it details a specific set of possible outputs:\n\n                {map_prompt}\n\n                Respond with 'Yes' if the prompt is explicitly categorical, detailing a finite set of possible outputs.\n                Respond with 'No' if the prompt allows for open-ended or non-categorical responses.\n                Provide a brief explanation for your decision.\"\"\",\n            },\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            \"You are an expert in analyzing natural language prompts for data processing tasks.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"is_categorical\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"Yes\", \"No\"],\n                        \"description\": \"Whether the prompt is explicitly categorical\",\n                    },\n                    \"explanation\": {\n                        \"type\": \"string\",\n                        \"description\": \"Brief explanation for the decision\",\n                    },\n                },\n                \"required\": [\"is_categorical\", \"explanation\"],\n            },\n        )\n\n        analysis = json.loads(response.choices[0].message.content)\n\n        self.console.log(\"[bold]Map Prompt Analysis:[/bold]\")\n        self.console.log(f\"Is Categorical: {analysis['is_categorical']}\")\n        self.console.log(f\"Explanation: {analysis['explanation']}\")\n\n        return analysis[\"is_categorical\"].lower() == \"yes\", analysis[\"explanation\"]\n\n    def _determine_duplicate_keys(\n        self,\n        input_data: list[dict[str, Any]],\n        reduce_key: list[str],\n        map_prompt: str | None = None,\n    ) -&gt; tuple[bool, str]:\n        # Prepare a sample of the input data for analysis\n        sample_size = min(10, len(input_data))\n        data_sample = random.sample(\n            [{rk: item[rk] for rk in reduce_key} for item in input_data], sample_size\n        )\n\n        context_prefix = \"\"\n        if map_prompt:\n            context_prefix = f\"For context, these values came out of a pipeline with the following prompt:\\n\\n{map_prompt}\\n\\n\"\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"{context_prefix}I want to do a reduce operation on these values, and I need to determine if there are semantic duplicates in the data, where the strings are different but they technically belong in the same group. Note that exact string duplicates should not be considered here.\\n\\nHere's a sample of the data (showing the '{reduce_key}' field(s)): {data_sample}\\n\\nBased on this {'context and ' if map_prompt else ''}sample, are there likely to be such semantic duplicates (not exact string matches) in the dataset? Respond with 'yes' only if you think there are semantic duplicates, or 'no' if you don't see evidence of semantic duplicates or if you only see exact string duplicates.\",\n            },\n        ]\n        response = self.llm_client.generate_rewrite(\n            messages,\n            \"You are an expert data analyst. Analyze the given data sample and determine if there are likely to be semantic duplicate values that belong in the same group, even if the strings are different.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"likely_duplicates\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"Yes\", \"No\"],\n                        \"description\": \"Whether duplicates are likely to exist in the full dataset\",\n                    },\n                    \"explanation\": {\n                        \"type\": \"string\",\n                        \"description\": \"Brief explanation for the decision\",\n                    },\n                },\n                \"required\": [\"likely_duplicates\", \"explanation\"],\n            },\n        )\n\n        analysis = json.loads(response.choices[0].message.content)\n\n        self.console.log(f\"[bold]Duplicate Analysis for '{reduce_key}':[/bold]\")\n        self.console.log(f\"Likely Duplicates: {analysis['likely_duplicates']}\")\n        self.console.log(f\"Explanation: {analysis['explanation']}\")\n\n        if analysis[\"likely_duplicates\"].lower() == \"yes\":\n            self.console.log(\n                \"[yellow]Duplicates are likely. Consider using a deduplication strategy in the resolution step.[/yellow]\"\n            )\n            return True, analysis[\"explanation\"]\n        return False, \"\"\n\n    def _sample_random_pairs(\n        self, input_data: list[dict[str, Any]], n: int\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Sample random pairs of indices, excluding exact matches.\"\"\"\n        pairs = set()\n        max_attempts = n * 10  # Avoid infinite loop\n        attempts = 0\n\n        while len(pairs) &lt; n and attempts &lt; max_attempts:\n            i, j = random.sample(range(len(input_data)), 2)\n            if i != j and input_data[i] != input_data[j]:\n                pairs.add((min(i, j), max(i, j)))  # Ensure ordered pairs\n            attempts += 1\n\n        return list(pairs)\n\n    def _check_duplicates_with_llm(\n        self,\n        input_data: list[dict[str, Any]],\n        pairs: list[tuple[int, int]],\n        reduce_key: list[str],\n        map_prompt: str | None = None,\n    ) -&gt; tuple[bool, str]:\n        \"\"\"Use LLM to check if any pairs are duplicates.\"\"\"\n\n        content = \"Analyze the following pairs of entries and determine if any of them are likely duplicates. Respond with 'Yes' if you find any likely duplicates, or 'No' if none of the pairs seem to be duplicates. Provide a brief explanation for your decision.\\n\\n\"\n\n        if map_prompt:\n            content = (\n                f\"For reference, here is the map prompt used earlier in the pipeline: {map_prompt}\\n\\n\"\n                + content\n            )\n\n        for i, (idx1, idx2) in enumerate(pairs, 1):\n            content += f\"Pair {i}:\\n\"\n            content += \"Entry 1:\\n\"\n            for key in reduce_key:\n                content += f\"{key}: {json.dumps(input_data[idx1][key], indent=2)}\\n\"\n            content += \"\\nEntry 2:\\n\"\n            for key in reduce_key:\n                content += f\"{key}: {json.dumps(input_data[idx2][key], indent=2)}\\n\"\n            content += \"\\n\"\n\n        messages = [{\"role\": \"user\", \"content\": content}]\n\n        system_prompt = \"You are an AI assistant tasked with identifying potential duplicate entries in a dataset.\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"duplicates_found\": {\"type\": \"string\", \"enum\": [\"Yes\", \"No\"]},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"duplicates_found\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate_rewrite(\n            messages, system_prompt, response_schema\n        )\n\n        # Print the duplicates_found and explanation\n        self.console.log(\n            f\"[bold]Duplicates in keys found:[/bold] {response['duplicates_found']}\\n\"\n            f\"[bold]Explanation:[/bold] {response['explanation']}\"\n        )\n\n        return response[\"duplicates_found\"].lower() == \"yes\", response[\"explanation\"]\n\n    def synthesize_compare_prompt(\n        self, map_prompt: str | None, reduce_key: list[str]\n    ) -&gt; str:\n\n        system_prompt = f\"You are an AI assistant tasked with creating a comparison prompt for LLM-assisted entity resolution. Your task is to create a comparison prompt that will be used to compare two entities, referred to as input1 and input2, to see if they are likely the same entity based on the following reduce key(s): {', '.join(reduce_key)}.\"\n        if map_prompt:\n            system_prompt += f\"\\n\\nFor context, here is the prompt used earlier in the pipeline to create the inputs to resolve: {map_prompt}\"\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n    Create a comparison prompt for entity resolution: The prompt should:\n    1. Be tailored to the specific domain and type of data being compared ({reduce_key}), based on the context provided.\n    2. Instruct to compare two entities, referred to as input1 and input2.\n    3. Specifically mention comparing each reduce key in input1 and input2 (e.g., input1.{{key}} and input2.{{key}} for each key in {reduce_key}). You can reference other fields in the input as well, as long as they are short.\n    4. Include instructions to consider relevant attributes or characteristics for comparison.\n    5. Ask to respond with \"True\" if the entities are likely the same, or \"False\" if they are likely different.\n\n    Example structure:\n    ```\n    Compare the following two {reduce_key} from [entity or document type]:\n\n    [Entity 1]:\n    {{{{ input1.key1 }}}}\n    {{{{ input1.optional_key2 }}}}\n\n    [Entity 2]:\n    {{{{ input2.key1 }}}}\n    {{{{ input2.optional_key2 }}}}\n\n    Are these [entities] likely referring to the same [entity type]? Consider [list relevant attributes or characteristics to compare]. Respond with \"True\" if they are likely the same [entity type], or \"False\" if they are likely different [entity types].\n    ```\n\n    Please generate the comparison prompt, which should be a Jinja2 template:\n    \"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            system_prompt,\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"comparison_prompt\": {\n                        \"type\": \"string\",\n                        \"description\": \"Detailed comparison prompt for entity resolution\",\n                    }\n                },\n                \"required\": [\"comparison_prompt\"],\n            },\n        )\n\n        comparison_prompt = json.loads(response.choices[0].message.content)[\n            \"comparison_prompt\"\n        ]\n\n        # Log the synthesized comparison prompt\n        self.console.log(\"[green]Synthesized comparison prompt:[/green]\")\n        self.console.log(comparison_prompt)\n\n        if not comparison_prompt:\n            raise ValueError(\n                \"Could not synthesize a comparison prompt. Please provide a comparison prompt in the config.\"\n            )\n\n        return comparison_prompt\n\n    def synthesize_resolution_prompt(\n        self,\n        map_prompt: str | None,\n        reduce_key: list[str],\n        output_schema: dict[str, str],\n    ) -&gt; str:\n        system_prompt = f\"\"\"You are an AI assistant tasked with creating a resolution prompt for LLM-assisted entity resolution.\n        Your task is to create a prompt that will be used to merge multiple duplicate keys into a single, consolidated key.\n        The key(s) being resolved (known as the reduce_key) are {', '.join(reduce_key)}.\n        The duplicate keys will be provided in a list called 'inputs' in a Jinja2 template.\n        \"\"\"\n\n        if map_prompt:\n            system_prompt += f\"\\n\\nFor context, here is the prompt used earlier in the pipeline to create the inputs to resolve: {map_prompt}\"\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n    Create a resolution prompt for merging duplicate keys into a single key. The prompt should:\n    1. Be tailored to the specific domain and type of data being merged, based on the context provided.\n    2. Use a Jinja2 template to iterate over the duplicate keys (accessed as 'inputs', where each item is a dictionary containing the reduce_key fields, which you can access as entry.reduce_key for each reduce_key in {reduce_key}).\n    3. Instruct to create a single, consolidated key from the duplicate keys.\n    4. Include guidelines for resolving conflicts (e.g., choosing the most recent, most complete, or most reliable information).\n    5. Specify that the output of the resolution prompt should conform to the given output schema: {json.dumps(output_schema, indent=2)}\n\n    Example structure:\n    ```\n    Analyze the following duplicate entries for the {reduce_key} key:\n\n    {{% for key in inputs %}}\n    Entry {{{{ loop.index }}}}:\n    {{ % for key in reduce_key %}}\n    {{{{ key }}}}: {{{{ key[reduce_key] }}}}\n    {{% endfor %}}\n\n    {{% endfor %}}\n\n    Merge these into a single key.\n    When merging, follow these guidelines:\n    1. [Provide specific merging instructions relevant to the data type]\n    2. [Do not make the prompt too long]\n\n    Ensure that the merged key conforms to the following schema:\n    {json.dumps(output_schema, indent=2)}\n\n    Return the consolidated key as a single [appropriate data type] value.\n    ```\n\n    Please generate the resolution prompt:\n    \"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            system_prompt,\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"resolution_prompt\": {\n                        \"type\": \"string\",\n                        \"description\": \"Detailed resolution prompt for merging duplicate keys\",\n                    }\n                },\n                \"required\": [\"resolution_prompt\"],\n            },\n        )\n\n        resolution_prompt = json.loads(response.choices[0].message.content)[\n            \"resolution_prompt\"\n        ]\n\n        # Log the synthesized resolution prompt\n        self.console.log(\"[green]Synthesized resolution prompt:[/green]\")\n        self.console.log(resolution_prompt)\n\n        if not resolution_prompt:\n            raise ValueError(\n                \"Could not synthesize a resolution prompt. Please provide a resolution prompt in the config.\"\n            )\n\n        return resolution_prompt\n\n    def should_optimize(self, input_data: list[dict[str, Any]]) -&gt; tuple[bool, str]:\n        \"\"\"\n        Determine if the given operation configuration should be optimized.\n        \"\"\"\n        # If there are no blocking keys or embeddings, then we don't need to optimize\n        if not self.op_config.get(\"blocking_conditions\") or not self.op_config.get(\n            \"blocking_threshold\"\n        ):\n            return True, \"\"\n\n        # Check if the operation is marked as empty\n        elif self.op_config.get(\"empty\", False):\n            # Extract the map prompt from the intermediates\n            map_prompt = self.op_config[\"_intermediates\"][\"map_prompt\"]\n            reduce_key = self.op_config[\"_intermediates\"][\"reduce_key\"]\n\n            if reduce_key is None:\n                raise ValueError(\n                    \"[yellow]Warning: No reduce key found in intermediates for synthesized resolve operation.[/yellow]\"\n                )\n\n            dedup = True\n            explanation = \"There is a reduce operation that does not follow a resolve operation. Consider adding a resolve operation to deduplicate the data.\"\n\n            if map_prompt:\n                # Analyze the map prompt\n                analysis, explanation = self._analyze_map_prompt_categorization(\n                    map_prompt\n                )\n\n                if analysis:\n                    dedup = False\n            else:\n                self.console.log(\n                    \"[yellow]No map prompt found in intermediates for analysis.[/yellow]\"\n                )\n\n            # TODO: figure out why this would ever be the case\n            if not map_prompt:\n                map_prompt = \"N/A\"\n\n            if dedup is False:\n                dedup, explanation = self._determine_duplicate_keys(\n                    input_data, reduce_key, map_prompt\n                )\n\n            # Now do the last attempt of pairwise comparisons\n            if dedup is False:\n                # Sample up to 20 random pairs of keys for duplicate analysis\n                sampled_pairs = self._sample_random_pairs(input_data, 20)\n\n                # Use LLM to check for duplicates\n                duplicates_found, explanation = self._check_duplicates_with_llm(\n                    input_data, sampled_pairs, reduce_key, map_prompt\n                )\n\n                if duplicates_found:\n                    dedup = True\n\n            return dedup, explanation\n\n        return False, \"\"\n\n    def optimize_resolve(\n        self, input_data: list[dict[str, Any]]\n    ) -&gt; tuple[dict[str, Any], float]:\n        # Check if the operation is marked as empty\n        if self.op_config.get(\"empty\", False):\n            # Extract the map prompt from the intermediates\n            dedup, _ = self.should_optimize(input_data)\n            reduce_key = self.op_config[\"_intermediates\"][\"reduce_key\"]\n            map_prompt = self.op_config[\"_intermediates\"][\"map_prompt\"]\n\n            if dedup is False:\n                # If no deduplication is needed, return the same config with 0 cost\n                return self.op_config, 0.0\n\n            # Add the reduce key to the output schema in the config\n            self.op_config[\"output\"] = {\"schema\": {rk: \"string\" for rk in reduce_key}}\n            for attempt in range(2):  # Try up to 2 times\n                self.op_config[\"comparison_prompt\"] = self.synthesize_compare_prompt(\n                    map_prompt, reduce_key\n                )\n                if (\n                    \"input1\" in self.op_config[\"comparison_prompt\"]\n                    and \"input2\" in self.op_config[\"comparison_prompt\"]\n                ):\n                    break\n                elif attempt == 0:\n                    self.console.log(\n                        \"[yellow]Warning: 'input1' or 'input2' not found in comparison prompt. Retrying...[/yellow]\"\n                    )\n            if (\n                \"input1\" not in self.op_config[\"comparison_prompt\"]\n                or \"input2\" not in self.op_config[\"comparison_prompt\"]\n            ):\n                self.console.log(\n                    \"[red]Error: Failed to generate comparison prompt with 'input1' and 'input2'. Using last generated prompt.[/red]\"\n                )\n            for attempt in range(2):  # Try up to 2 times\n                self.op_config[\"resolution_prompt\"] = self.synthesize_resolution_prompt(\n                    map_prompt, reduce_key, self.op_config[\"output\"][\"schema\"]\n                )\n                if \"inputs\" in self.op_config[\"resolution_prompt\"]:\n                    break\n                elif attempt == 0:\n                    self.console.log(\n                        \"[yellow]Warning: 'inputs' not found in resolution prompt. Retrying...[/yellow]\"\n                    )\n            if \"inputs\" not in self.op_config[\"resolution_prompt\"]:\n                self.console.log(\n                    \"[red]Error: Failed to generate resolution prompt with 'inputs'. Using last generated prompt.[/red]\"\n                )\n\n            # Pop off the empty flag\n            self.op_config.pop(\"empty\")\n\n        embeddings, blocking_keys, embedding_cost = self._compute_embeddings(input_data)\n        self.console.log(\n            f\"[bold]Cost of creating embeddings on the sample: ${embedding_cost:.4f}[/bold]\"\n        )\n\n        similarities = self._calculate_cosine_similarities(embeddings)\n\n        sampled_pairs = self._sample_pairs(similarities)\n        comparison_results, comparison_cost = self._perform_comparisons_resolve(\n            input_data, sampled_pairs\n        )\n\n        self._print_similarity_histogram(similarities, comparison_results)\n\n        threshold, estimated_selectivity = self._find_optimal_threshold(\n            comparison_results, similarities\n        )\n\n        blocking_rules = self._generate_blocking_rules(\n            blocking_keys, input_data, comparison_results\n        )\n\n        if blocking_rules:\n            false_negatives, rule_selectivity = self._verify_blocking_rule(\n                input_data,\n                blocking_rules[0],\n                blocking_keys,\n                comparison_results,\n            )\n            # If more than 50% of the sample is false negatives, reject the blocking rule\n            if len(false_negatives) &gt; len(sampled_pairs) / 2:\n                if false_negatives:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. {len(false_negatives)} false negatives detected in the sample ({len(false_negatives) / len(sampled_pairs):.2f} of the sample).[/red]\"\n                    )\n                    for i, j in false_negatives[:5]:  # Show up to 5 examples\n                        self.console.log(\n                            f\"  Filtered pair: {{ {blocking_keys[0]}: {input_data[i][blocking_keys[0]]} }} and {{ {blocking_keys[0]}: {input_data[j][blocking_keys[0]]} }}\"\n                        )\n                    if len(false_negatives) &gt; 5:\n                        self.console.log(f\"  ... and {len(false_negatives) - 5} more.\")\n                blocking_rules = (\n                    []\n                )  # Clear the blocking rule if it introduces false negatives or is too selective\n            elif not false_negatives and rule_selectivity &gt; estimated_selectivity:\n                self.console.log(\n                    \"[green]Blocking rule verified. No false negatives detected in the sample and selectivity is within estimated selectivity.[/green]\"\n                )\n            else:\n                # TODO: ask user if they want to use the blocking rule, or come up with some good default behavior\n                blocking_rules = []\n\n        optimized_config = self._update_config(threshold, blocking_keys, blocking_rules)\n        return optimized_config, embedding_cost + comparison_cost\n\n    def optimize_equijoin(\n        self,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        skip_map_gen: bool = False,\n        skip_containment_gen: bool = False,\n    ) -&gt; tuple[dict[str, Any], float, dict[str, Any]]:\n        left_keys = self.op_config.get(\"blocking_keys\", {}).get(\"left\", [])\n        right_keys = self.op_config.get(\"blocking_keys\", {}).get(\"right\", [])\n\n        if not left_keys and not right_keys:\n            # Ask the LLM agent if it would be beneficial to do a map operation on\n            # one of the datasets before doing an equijoin\n            apply_transformation, dataset_to_transform, reason = (\n                (\n                    self._should_apply_map_transformation(\n                        left_keys, right_keys, left_data, right_data\n                    )\n                )\n                if not skip_map_gen\n                else (False, None, None)\n            )\n\n            if apply_transformation and not skip_map_gen:\n                self.console.log(\n                    f\"LLM agent suggested applying a map transformation to {dataset_to_transform} dataset because: {reason}\"\n                )\n                extraction_prompt, output_key, new_comparison_prompt = (\n                    self._generate_map_and_new_join_transformation(\n                        dataset_to_transform, reason, left_data, right_data\n                    )\n                )\n                self.console.log(\n                    f\"Generated map transformation prompt: {extraction_prompt}\"\n                )\n                self.console.log(f\"\\nNew output key: {output_key}\")\n                self.console.log(\n                    f\"\\nNew equijoin comparison prompt: {new_comparison_prompt}\"\n                )\n\n                # Update the comparison prompt\n                self.op_config[\"comparison_prompt\"] = new_comparison_prompt\n\n                # Add the output key to the left_keys or right_keys\n                if dataset_to_transform == \"left\":\n                    left_keys.append(output_key)\n                else:\n                    right_keys.append(output_key)\n\n                # Reset the blocking keys in the config\n                self.op_config[\"blocking_keys\"] = {\n                    \"left\": left_keys,\n                    \"right\": right_keys,\n                }\n\n                # Bubble up this config and return the transformation prompt, so we can optimize the map operation\n                return (\n                    self.op_config,\n                    0.0,\n                    {\n                        \"optimize_map\": True,\n                        \"map_prompt\": extraction_prompt,\n                        \"output_key\": output_key,\n                        \"dataset_to_transform\": dataset_to_transform,\n                    },\n                )\n\n            # Print the reason for not applying a map transformation\n            self.console.log(\n                f\"Reason for not synthesizing a map transformation for either left or right dataset: {reason}\"\n            )\n\n        # If there are no blocking keys, generate them\n        if not left_keys or not right_keys:\n            generated_left_keys, generated_right_keys = (\n                self._generate_blocking_keys_equijoin(left_data, right_data)\n            )\n            left_keys.extend(generated_left_keys)\n            right_keys.extend(generated_right_keys)\n            left_keys = list(set(left_keys))\n            right_keys = list(set(right_keys))\n\n            # Log the generated blocking keys\n            self.console.log(\n                \"[bold]Generated blocking keys (for embeddings-based blocking):[/bold]\"\n            )\n            self.console.log(f\"Left keys: {left_keys}\")\n            self.console.log(f\"Right keys: {right_keys}\")\n\n        left_embeddings, _, left_embedding_cost = self._compute_embeddings(\n            left_data, keys=left_keys\n        )\n        right_embeddings, _, right_embedding_cost = self._compute_embeddings(\n            right_data, keys=right_keys\n        )\n        self.console.log(\n            f\"[bold]Cost of creating embeddings on the sample: ${left_embedding_cost + right_embedding_cost:.4f}[/bold]\"\n        )\n\n        similarities = self._calculate_cross_similarities(\n            left_embeddings, right_embeddings\n        )\n\n        sampled_pairs = self._sample_pairs(similarities)\n        comparison_results, comparison_cost = self._perform_comparisons_equijoin(\n            left_data, right_data, sampled_pairs\n        )\n        self._print_similarity_histogram(similarities, comparison_results)\n        attempts = 0\n        while (\n            not any(result[2] for result in comparison_results)\n            and attempts &lt; self.max_comparison_sampling_attempts\n        ):\n            self.console.log(\n                \"[yellow]No matches found in the current sample. Resampling pairs to compare...[/yellow]\"\n            )\n            sampled_pairs = self._sample_pairs(similarities)\n            comparison_results, current_cost = self._perform_comparisons_equijoin(\n                left_data, right_data, sampled_pairs\n            )\n            comparison_cost += current_cost\n            self._print_similarity_histogram(similarities, comparison_results)\n            attempts += 1\n\n        if not any(result[2] for result in comparison_results):\n            # If still no matches after max_comparison_sampling_attempts attempts, use 99th percentile similarity as threshold\n            # This is a heuristic to avoid being in an infinite loop\n            # TODO: have a better plan for sampling pairs or avoiding getting into this situation\n            self.console.log(\n                f\"[yellow]No matches found after {self.max_comparison_sampling_attempts} attempts. Using 99th percentile similarity as threshold.[/yellow]\"\n            )\n            threshold = np.percentile([sim[2] for sim in similarities], 99)\n            # TODO: figure out how to estimate selectivity\n            estimated_selectivity = 0.0\n            self.estimated_selectivity = estimated_selectivity\n\n        else:\n            threshold, estimated_selectivity = self._find_optimal_threshold(\n                comparison_results, similarities\n            )\n            self.estimated_selectivity = estimated_selectivity\n\n        blocking_rules = self._generate_blocking_rules_equijoin(\n            left_keys, right_keys, left_data, right_data, comparison_results\n        )\n\n        if blocking_rules:\n            false_negatives, rule_selectivity = self._verify_blocking_rule_equijoin(\n                left_data,\n                right_data,\n                blocking_rules[0],\n                left_keys,\n                right_keys,\n                comparison_results,\n            )\n            if not false_negatives and rule_selectivity &lt;= estimated_selectivity:\n                self.console.log(\n                    \"[green]Blocking rule verified. No false negatives detected in the sample and selectivity is within bounds.[/green]\"\n                )\n            else:\n                if false_negatives:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. {len(false_negatives)} false negatives detected in the sample.[/red]\"\n                    )\n                    for i, j in false_negatives[:5]:  # Show up to 5 examples\n                        self.console.log(\n                            f\"  Filtered pair: Left: {{{', '.join(f'{key}: {left_data[i][key]}' for key in left_keys)}}} and Right: {{{', '.join(f'{key}: {right_data[j][key]}' for key in right_keys)}}}\"\n                        )\n                    if len(false_negatives) &gt; 5:\n                        self.console.log(f\"  ... and {len(false_negatives) - 5} more.\")\n                if rule_selectivity &gt; estimated_selectivity:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. Rule selectivity ({rule_selectivity:.4f}) is higher than the estimated selectivity ({estimated_selectivity:.4f}).[/red]\"\n                    )\n                blocking_rules = (\n                    []\n                )  # Clear the blocking rule if it introduces false negatives or is too selective\n\n        containment_rules = self._generate_containment_rules_equijoin(\n            left_data, right_data\n        )\n        if not skip_containment_gen:\n            self.console.log(\n                f\"[bold]Generated {len(containment_rules)} containment rules. Please select which ones to use as blocking conditions:[/bold]\"\n            )\n            selected_containment_rules = []\n            for rule in containment_rules:\n                self.console.log(f\"[green]{rule}[/green]\")\n                # Temporarily stop the status\n                if self.status:\n                    self.status.stop()\n                # Use Rich's Confirm for input\n                if Confirm.ask(\"Use this rule?\", console=self.console):\n                    selected_containment_rules.append(rule)\n                # Restart the status\n                if self.status:\n                    self.status.start()\n        else:\n            # Take first 2\n            selected_containment_rules = containment_rules[:2]\n\n        if len(containment_rules) &gt; 0:\n            self.console.log(\n                f\"[bold]Selected {len(selected_containment_rules)} containment rules for blocking.[/bold]\"\n            )\n        blocking_rules.extend(selected_containment_rules)\n\n        optimized_config = self._update_config_equijoin(\n            threshold, left_keys, right_keys, blocking_rules\n        )\n        return (\n            optimized_config,\n            left_embedding_cost + right_embedding_cost + comparison_cost,\n            {},\n        )\n\n    def _should_apply_map_transformation(\n        self,\n        left_keys: list[str],\n        right_keys: list[str],\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        sample_size: int = 5,\n    ) -&gt; tuple[bool, str, str]:\n        # Sample data\n        left_sample = random.sample(left_data, min(sample_size, len(left_data)))\n        right_sample = random.sample(right_data, min(sample_size, len(right_data)))\n\n        # Get keys and their average lengths\n        all_left_keys = {\n            k: sum(len(str(d[k])) for d in left_sample) / len(left_sample)\n            for k in left_sample[0].keys()\n        }\n        all_right_keys = {\n            k: sum(len(str(d[k])) for d in right_sample) / len(right_sample)\n            for k in right_sample[0].keys()\n        }\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following datasets and determine if an additional LLM transformation should be applied to generate a new key-value pair for easier joining:\n\n                Comparison prompt for the join operation: {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n                Left dataset keys and average lengths: {json.dumps(all_left_keys, indent=2)}\n                Right dataset keys and average lengths: {json.dumps(all_right_keys, indent=2)}\n\n                Left dataset sample:\n                {json.dumps(left_sample, indent=2)}\n\n                Right dataset sample:\n                {json.dumps(right_sample, indent=2)}\n\n                Current keys used for embedding-based ranking of likely matches:\n                Left keys: {left_keys}\n                Right keys: {right_keys}\n\n                Consider the following:\n                1. Are the current keys sufficient for accurate embedding-based ranking of likely matches? We don't want to use too many keys, or keys with too much information, as this will dilute the signal in the embeddings.\n                2. Are there any keys particularly long (e.g., full text fields), containing information that is not relevant for the join operation? The dataset with the longer keys should be transformed.\n                3. Would a summary or extraction of important information from long key-value pairs be beneficial? If so, the dataset with the longer keys should be transformed.\n                4. Is there a mismatch in information representation between the datasets?\n                5. Could an additional LLM-generated field improve the accuracy of embeddings or join comparisons?\n\n                If you believe an additional LLM transformation would be beneficial, specify which dataset (left or right) should be transformed and explain why. Otherwise, indicate that no additional transformation is needed and explain why the current blocking keys are sufficient.\"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            \"You are an AI expert in data analysis and entity matching.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"apply_transformation\": {\"type\": \"boolean\"},\n                    \"dataset_to_transform\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"left\", \"right\", \"none\"],\n                    },\n                    \"reason\": {\"type\": \"string\"},\n                },\n                \"required\": [\"apply_transformation\", \"dataset_to_transform\", \"reason\"],\n            },\n        )\n\n        result = json.loads(response.choices[0].message.content)\n\n        return (\n            result[\"apply_transformation\"],\n            result[\"dataset_to_transform\"],\n            result[\"reason\"],\n        )\n\n    def _generate_map_and_new_join_transformation(\n        self,\n        dataset_to_transform: str,\n        reason: str,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        sample_size: int = 5,\n    ) -&gt; tuple[str, str, str]:\n        # Sample data\n        left_sample = random.sample(left_data, min(sample_size, len(left_data)))\n        right_sample = random.sample(right_data, min(sample_size, len(right_data)))\n\n        target_data = left_sample if dataset_to_transform == \"left\" else right_sample\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Generate an LLM prompt to transform the {dataset_to_transform} dataset for easier joining. The transformation should create a new key-value pair.\n\n                Current comparison prompt for the join operation: {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n                Target ({dataset_to_transform}) dataset sample:\n                {json.dumps(target_data, indent=2)}\n\n                Other ({'left' if dataset_to_transform == \"right\" else \"right\"}) dataset sample:\n                {json.dumps(right_sample if dataset_to_transform == \"left\" else left_sample, indent=2)}\n\n                Reason for transforming {dataset_to_transform} dataset: {reason}\n\n                Please provide:\n                1. An LLM prompt to extract a smaller representation of what is relevant to the join task. The prompt should be a Jinja2 template, referring to any fields in the input data as {{{{ input.field_name }}}}. The prompt should instruct the LLM to return some **non-empty** string-valued output. The transformation should be tailored to the join task if possible, not just a generic summary of the data.\n                2. A name for the new output key that will store the transformed data.\n                3. An edited comparison prompt that leverages the new attribute created by the transformation. This prompt should be a Jinja2 template, referring to any fields in the input data as {{{{ left.field_name }}}} and {{{{ right.field_name }}}}. The prompt should be the same as the current comparison prompt, but with a new instruction that leverages the new attribute created by the transformation (in addition to the other fields in the prompt). The prompt should instruct the LLM to return a boolean-valued output, like the current comparison prompt.\"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            \"You are an AI expert in data analysis and decomposing complex data processing pipelines.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"extraction_prompt\": {\"type\": \"string\"},\n                    \"output_key\": {\"type\": \"string\"},\n                    \"new_comparison_prompt\": {\"type\": \"string\"},\n                },\n                \"required\": [\n                    \"extraction_prompt\",\n                    \"output_key\",\n                    \"new_comparison_prompt\",\n                ],\n            },\n        )\n\n        result = json.loads(response.choices[0].message.content)\n\n        return (\n            result[\"extraction_prompt\"]\n            .replace(\"left.\", \"input.\")\n            .replace(\"right.\", \"input.\"),\n            result[\"output_key\"],\n            result[\"new_comparison_prompt\"],\n        )\n\n    def _generate_blocking_keys_equijoin(\n        self,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        sample_size: int = 5,\n    ) -&gt; tuple[list[str], list[str]]:\n        # Sample data\n        left_sample = random.sample(left_data, min(sample_size, len(left_data)))\n        right_sample = random.sample(right_data, min(sample_size, len(right_data)))\n\n        # Prepare sample data for LLM\n        left_keys = list(left_sample[0].keys())\n        right_keys = list(right_sample[0].keys())\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following sample data from two datasets, select appropriate blocking keys for an equijoin operation.\n                The blocking process works as follows:\n                1. We create embeddings for the selected keys from both datasets.\n                2. We use cosine similarity between these embeddings to filter pairs for more detailed LLM comparison.\n                3. Pairs with high similarity will be passed to the LLM for final comparison.\n\n                The blocking keys should have relatively short values and be useful for generating embeddings that capture the essence of potential matches.\n\n                Left dataset keys: {left_keys}\n                Right dataset keys: {right_keys}\n\n                Sample from left dataset:\n                {json.dumps(left_sample, indent=2)}\n\n                Sample from right dataset:\n                {json.dumps(right_sample, indent=2)}\n\n                For context, here is the comparison prompt that will be used for the more detailed LLM comparison:\n                {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n                Please select one or more keys from each dataset that would be suitable for blocking. The keys should contain information that's likely to be similar in matching records and align with the comparison prompt's focus.\"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            \"You are an expert in entity matching and database operations.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"left_blocking_keys\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of selected blocking keys from the left dataset\",\n                    },\n                    \"right_blocking_keys\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of selected blocking keys from the right dataset\",\n                    },\n                },\n                \"required\": [\"left_blocking_keys\", \"right_blocking_keys\"],\n            },\n        )\n\n        result = json.loads(response.choices[0].message.content)\n        left_blocking_keys = result[\"left_blocking_keys\"]\n        right_blocking_keys = result[\"right_blocking_keys\"]\n\n        return left_blocking_keys, right_blocking_keys\n\n    def _compute_embeddings(\n        self,\n        input_data: list[dict[str, Any]],\n        keys: list[str] | None = None,\n        is_join: bool = True,\n    ) -&gt; tuple[list[list[float]], list[str], float]:\n        if keys is None:\n            keys = self.op_config.get(\"blocking_keys\", [])\n            if not keys:\n                prompt_template = self.op_config.get(\"comparison_prompt\", \"\")\n                prompt_vars = extract_jinja_variables(prompt_template)\n                # Get rid of input, input1, input2\n                prompt_vars = [\n                    var\n                    for var in prompt_vars\n                    if var not in [\"input\", \"input1\", \"input2\"]\n                ]\n\n                # strip all things before . in the prompt_vars\n                keys += list(set([var.split(\".\")[-1] for var in prompt_vars]))\n            if not keys:\n                self.console.log(\n                    \"[yellow]Warning: No blocking keys found. Using all keys for blocking.[/yellow]\"\n                )\n                keys = list(input_data[0].keys())\n\n        model_input_context_length = model_cost.get(\n            self.op_config.get(\"embedding_model\", \"text-embedding-3-small\"), {}\n        ).get(\"max_input_tokens\", 8192)\n        texts = [\n            \" \".join(str(item[key]) for key in keys if key in item)[\n                :model_input_context_length\n            ]\n            for item in input_data\n        ]\n\n        embeddings = []\n        total_cost = 0\n        batch_size = 2000\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i : i + batch_size]\n            self.console.log(\n                f\"[cyan]Processing batch {i//batch_size + 1} of {len(texts)//batch_size + 1}[/cyan]\"\n            )\n            response = self.runner.api.gen_embedding(\n                model=self.op_config.get(\"embedding_model\", \"text-embedding-3-small\"),\n                input=batch,\n            )\n            embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n            total_cost += completion_cost(response)\n        embeddings = [data[\"embedding\"] for data in response[\"data\"]]\n        cost = completion_cost(response)\n        return embeddings, keys, cost\n\n    def _calculate_cosine_similarities(\n        self, embeddings: list[list[float]]\n    ) -&gt; list[tuple[int, int, float]]:\n        embeddings_array = np.array(embeddings)\n        norms = np.linalg.norm(embeddings_array, axis=1)\n        dot_products = np.dot(embeddings_array, embeddings_array.T)\n        similarities_matrix = dot_products / np.outer(norms, norms)\n        i, j = np.triu_indices(len(embeddings), k=1)\n        similarities = list(\n            zip(i.tolist(), j.tolist(), similarities_matrix[i, j].tolist())\n        )\n        return similarities\n\n    def _print_similarity_histogram(\n        self,\n        similarities: list[tuple[int, int, float]],\n        comparison_results: list[tuple[int, int, bool]],\n    ):\n        flat_similarities = [sim[-1] for sim in similarities if sim[-1] != 1]\n        hist, bin_edges = np.histogram(flat_similarities, bins=20)\n        max_bar_width, max_count = 50, max(hist)\n        normalized_hist = [int(count / max_count * max_bar_width) for count in hist]\n\n        # Create a dictionary to store true labels\n        true_labels = {(i, j): is_match for i, j, is_match in comparison_results}\n\n        self.console.log(\"\\n[bold]Embedding Cosine Similarity Distribution:[/bold]\")\n        for i, count in enumerate(normalized_hist):\n            bar = \"\u2588\" * count\n            label = f\"{bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}\"\n\n            # Count true matches and not matches in this bin\n            true_matches = 0\n            not_matches = 0\n            labeled_count = 0\n            for sim in similarities:\n                if bin_edges[i] &lt;= sim[2] &lt; bin_edges[i + 1]:\n                    if (sim[0], sim[1]) in true_labels:\n                        labeled_count += 1\n                        if true_labels[(sim[0], sim[1])]:\n                            true_matches += 1\n                        else:\n                            not_matches += 1\n\n            # Calculate percentages of labeled pairs\n            if labeled_count &gt; 0:\n                true_match_percent = (true_matches / labeled_count) * 100\n                not_match_percent = (not_matches / labeled_count) * 100\n            else:\n                true_match_percent = 0\n                not_match_percent = 0\n\n            self.console.log(\n                f\"{label}: {bar} \"\n                f\"(Labeled: {labeled_count}/{hist[i]}, [green]{true_match_percent:.1f}% match[/green], [red]{not_match_percent:.1f}% not match[/red])\"\n            )\n        self.console.log(\"\\n\")\n\n    def _sample_pairs(\n        self, similarities: list[tuple[int, int, float]]\n    ) -&gt; list[tuple[int, int]]:\n        # Sort similarities in descending order\n        sorted_similarities = sorted(similarities, key=lambda x: x[2], reverse=True)\n\n        # Calculate weights using exponential weighting with self.sampling_weight\n        similarities_array = np.array([sim[2] for sim in sorted_similarities])\n        weights = np.exp(self.sampling_weight * similarities_array)\n        weights /= weights.sum()  # Normalize weights to sum to 1\n\n        # Sample pairs based on the calculated weights\n        sampled_indices = np.random.choice(\n            len(sorted_similarities),\n            size=min(self.sample_size, len(sorted_similarities)),\n            replace=False,\n            p=weights,\n        )\n\n        sampled_pairs = [\n            (sorted_similarities[i][0], sorted_similarities[i][1])\n            for i in sampled_indices\n        ]\n        return sampled_pairs\n\n    def _calculate_cross_similarities(\n        self, left_embeddings: list[list[float]], right_embeddings: list[list[float]]\n    ) -&gt; list[tuple[int, int, float]]:\n        left_array = np.array(left_embeddings)\n        right_array = np.array(right_embeddings)\n        dot_product = np.dot(left_array, right_array.T)\n        norm_left = np.linalg.norm(left_array, axis=1)\n        norm_right = np.linalg.norm(right_array, axis=1)\n        similarities = dot_product / np.outer(norm_left, norm_right)\n        return [\n            (i, j, sim)\n            for i, row in enumerate(similarities)\n            for j, sim in enumerate(row)\n        ]\n\n    def _perform_comparisons_resolve(\n        self, input_data: list[dict[str, Any]], pairs: list[tuple[int, int]]\n    ) -&gt; tuple[list[tuple[int, int, bool]], float]:\n        comparisons, total_cost = [], 0\n        op = ResolveOperation(\n            self.runner,\n            self.op_config,\n            self.runner.default_model,\n            self.max_threads,\n            self.console,\n            self.status,\n        )\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(\n                    op.compare_pair,\n                    self.op_config[\"comparison_prompt\"],\n                    self.op_config.get(\n                        \"comparison_model\", self.config.get(\"model\", \"gpt-4o-mini\")\n                    ),\n                    input_data[i],\n                    input_data[j],\n                )\n                for i, j in pairs\n            ]\n            for future, (i, j) in zip(futures, pairs):\n                is_match, cost, _ = future.result()\n                comparisons.append((i, j, is_match))\n                total_cost += cost\n\n        self.console.log(\n            f\"[bold]Cost of pairwise comparisons on the sample: ${total_cost:.4f}[/bold]\"\n        )\n        return comparisons, total_cost\n\n    def _perform_comparisons_equijoin(\n        self,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        pairs: list[tuple[int, int]],\n    ) -&gt; tuple[list[tuple[int, int, bool]], float]:\n        comparisons, total_cost = [], 0\n        op = EquijoinOperation(\n            self.runner,\n            self.op_config,\n            self.runner.default_model,\n            self.max_threads,\n            self.console,\n            self.status,\n        )\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(\n                    op.compare_pair,\n                    self.op_config[\"comparison_prompt\"],\n                    self.op_config.get(\n                        \"comparison_model\", self.config.get(\"model\", \"gpt-4o-mini\")\n                    ),\n                    left_data[i],\n                    right_data[j] if right_data else left_data[j],\n                )\n                for i, j in pairs\n            ]\n            for future, (i, j) in zip(futures, pairs):\n                is_match, cost = future.result()\n                comparisons.append((i, j, is_match))\n                total_cost += cost\n\n        self.console.log(\n            f\"[bold]Cost of pairwise comparisons on the sample: ${total_cost:.4f}[/bold]\"\n        )\n        return comparisons, total_cost\n\n    def _find_optimal_threshold(\n        self,\n        comparisons: list[tuple[int, int, bool]],\n        similarities: list[tuple[int, int, float]],\n    ) -&gt; tuple[float, float, float]:\n        true_labels = np.array([comp[2] for comp in comparisons])\n        sim_dict = {(i, j): sim for i, j, sim in similarities}\n        sim_scores = np.array([sim_dict[(i, j)] for i, j, _ in comparisons])\n\n        thresholds = np.linspace(0, 1, 100)\n        precisions, recalls = [], []\n\n        for threshold in thresholds:\n            predictions = sim_scores &gt;= threshold\n            tp = np.sum(predictions &amp; true_labels)\n            fp = np.sum(predictions &amp; ~true_labels)\n            fn = np.sum(~predictions &amp; true_labels)\n\n            precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n\n            precisions.append(precision)\n            recalls.append(recall)\n\n        valid_indices = [i for i, r in enumerate(recalls) if r &gt;= self.target_recall]\n        if not valid_indices:\n            optimal_threshold = float(thresholds[np.argmax(recalls)])\n        else:\n            optimal_threshold = float(thresholds[max(valid_indices)])\n\n        # Improved selectivity estimation\n        all_similarities = np.array([s[2] for s in similarities])\n        sampled_similarities = sim_scores\n\n        # Calculate sampling probabilities\n        sampling_probs = np.exp(self.sampling_weight * sampled_similarities)\n        sampling_probs /= sampling_probs.sum()\n\n        # Estimate selectivity using importance sampling\n        weights = 1 / (len(all_similarities) * sampling_probs)\n        numerator = np.sum(weights * true_labels)\n        denominator = np.sum(weights)\n        selectivity_estimate = numerator / denominator\n\n        self.console.log(\n            \"[bold cyan]\u250c\u2500 Estimated Self-Join Selectivity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510[/bold cyan]\"\n        )\n        self.console.log(\n            f\"[bold cyan]\u2502[/bold cyan] [yellow]Target Recall:[/yellow] {self.target_recall:.0%}\"\n        )\n        self.console.log(\n            f\"[bold cyan]\u2502[/bold cyan] [yellow]Estimate:[/yellow] {selectivity_estimate:.4f}\"\n        )\n        self.console.log(\n            \"[bold cyan]\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518[/bold cyan]\"\n        )\n        self.console.log(\n            f\"[bold]Chosen similarity threshold for blocking: {optimal_threshold:.4f}[/bold]\"\n        )\n\n        return round(optimal_threshold, 4), selectivity_estimate\n\n    def _generate_blocking_rules(\n        self,\n        blocking_keys: list[str],\n        input_data: list[dict[str, Any]],\n        comparisons: list[tuple[int, int, bool]],\n    ) -&gt; list[str]:\n        # Sample 2 true and 2 false comparisons\n        true_comparisons = [comp for comp in comparisons if comp[2]][:2]\n        false_comparisons = [comp for comp in comparisons if not comp[2]][:2]\n        sample_datas = [\n            (\n                {key: input_data[i][key] for key in blocking_keys},\n                {key: input_data[j][key] for key in blocking_keys},\n                is_match,\n            )\n            for i, j, is_match in true_comparisons + false_comparisons\n        ]\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following sample comparisons between entities, generate a single-line Python statement that acts as a blocking rule for entity resolution. This rule will be used in the form: `eval(blocking_rule, {{\"input1\": item1, \"input2\": item2}})`.\n\n    Sample comparisons (note: these are just a few examples and may not represent all possible cases):\n    {json.dumps(sample_datas, indent=2)}\n\n    For context, here is the comparison prompt that will be used for the more expensive, detailed comparison:\n    {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n    Please generate ONE one-line blocking rule that adheres to the following criteria:\n    1. The rule should evaluate to True if the entities are possibly a match and require further comparison.\n    2. The rule should evaluate to False ONLY if the entities are definitely not a match.\n    3. The rule must be a single Python expression that can be evaluated using the eval() function.\n    4. The rule should be much faster to evaluate than the full comparison prompt.\n    5. The rule should capture the essence of the comparison prompt but in a simplified manner.\n    6. The rule should be general enough to work well on the entire dataset, not just these specific examples.\n    7. The rule should handle inconsistent casing by using string methods like .lower() when comparing string values.\n    8. The rule should err on the side of inclusivity - it's better to have false positives than false negatives.\n\n    Example structure of a one-line blocking rule:\n    \"(condition1) or (condition2) or (condition3)\"\n\n    Where conditions could be comparisons like:\n    \"input1['field'].lower() == input2['field'].lower()\"\n    \"abs(len(input1['text']) - len(input2['text'])) &lt;= 5\"\n    \"any(word in input1['description'].lower() for word in input2['description'].lower().split())\"\n\n    If there's no clear rule that can be generated based on the given information, return the string \"True\" to ensure all pairs are compared.\n\n    Remember, the primary goal of the blocking rule is to safely reduce the number of comparisons by quickly identifying pairs that are definitely not matches, while keeping all potential matches for further evaluation.\"\"\",\n            }\n        ]\n\n        for attempt in range(self.agent_max_retries):  # Up to 3 attempts\n            # Generate blocking rule using the LLM\n            response = self.llm_client.generate_rewrite(\n                messages,\n                \"You are an expert in entity resolution and Python programming. Your task is to generate one efficient blocking rule based on the given sample comparisons and data structure.\",\n                {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"blocking_rule\": {\n                            \"type\": \"string\",\n                            \"description\": \"One-line Python statement acting as a blocking rule\",\n                        }\n                    },\n                    \"required\": [\"blocking_rule\"],\n                },\n            )\n\n            # Extract the blocking rule from the LLM response\n            blocking_rule = response.choices[0].message.content\n            blocking_rule = json.loads(blocking_rule).get(\"blocking_rule\")\n\n            if blocking_rule:\n                self.console.log(\"\")  # Print a newline\n\n                if blocking_rule.strip() == \"True\":\n                    self.console.log(\n                        \"[yellow]No suitable blocking rule could be found. Proceeding without a blocking rule.[/yellow]\"\n                    )\n                    return []\n\n                self.console.log(\n                    f\"[bold]Generated blocking rule (Attempt {attempt + 1}):[/bold] {blocking_rule}\"\n                )\n\n                # Test the blocking rule\n                filtered_pairs = self._test_blocking_rule(\n                    input_data, blocking_keys, blocking_rule, comparisons\n                )\n\n                if not filtered_pairs:\n                    self.console.log(\n                        \"[green]Blocking rule looks good! No known matches were filtered out.[/green]\"\n                    )\n                    return [blocking_rule]\n                else:\n                    feedback = f\"The previous rule incorrectly filtered out {len(filtered_pairs)} known matches. \"\n                    feedback += (\n                        \"Here are up to 3 examples of incorrectly filtered pairs:\\n\"\n                    )\n                    for i, j in filtered_pairs[:3]:\n                        feedback += f\"Item 1: {json.dumps({key: input_data[i][key] for key in blocking_keys})}\\nItem 2: {json.dumps({key: input_data[j][key] for key in blocking_keys})}\\n\"\n                        feedback += \"These pairs are known matches but were filtered out by the rule.\\n\"\n                    feedback += \"Please generate a new rule that doesn't filter out these matches.\"\n\n                    messages.append({\"role\": \"assistant\", \"content\": blocking_rule})\n                    messages.append({\"role\": \"user\", \"content\": feedback})\n            else:\n                self.console.log(\"[yellow]No blocking rule generated.[/yellow]\")\n                return []\n\n        self.console.log(\n            f\"[yellow]Failed to generate a suitable blocking rule after {self.agent_max_retries} attempts. Proceeding without a blocking rule.[/yellow]\"\n        )\n        return []\n\n    def _test_blocking_rule(\n        self,\n        input_data: list[dict[str, Any]],\n        blocking_keys: list[str],\n        blocking_rule: str,\n        comparisons: list[tuple[int, int, bool]],\n    ) -&gt; list[tuple[int, int]]:\n        def apply_blocking_rule(item1, item2):\n            try:\n                return eval(blocking_rule, {\"input1\": item1, \"input2\": item2})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        filtered_pairs = []\n\n        for i, j, is_match in comparisons:\n            if is_match:\n                item1 = {\n                    k: input_data[i][k] for k in blocking_keys if k in input_data[i]\n                }\n                item2 = {\n                    k: input_data[j][k] for k in blocking_keys if k in input_data[j]\n                }\n\n                if not apply_blocking_rule(item1, item2):\n                    filtered_pairs.append((i, j))\n\n        if filtered_pairs:\n            self.console.log(\n                f\"[yellow italic]LLM Correction: The blocking rule incorrectly filtered out {len(filtered_pairs)} known positive matches.[/yellow italic]\"\n            )\n            for i, j in filtered_pairs[:5]:  # Show up to 5 examples\n                self.console.log(\n                    f\"  Incorrectly filtered pair 1: {json.dumps({key: input_data[i][key] for key in blocking_keys})}  and pair 2: {json.dumps({key: input_data[j][key] for key in blocking_keys})}\"\n                )\n            if len(filtered_pairs) &gt; 5:\n                self.console.log(\n                    f\"  ... and {len(filtered_pairs) - 5} more incorrect pairs.\"\n                )\n\n        return filtered_pairs\n\n    def _generate_containment_rules_equijoin(\n        self,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n    ) -&gt; list[str]:\n        # Get all available keys from the sample data\n        left_keys = set(left_data[0].keys())\n        right_keys = set(right_data[0].keys())\n\n        # Find the keys that are in the config's prompt\n        try:\n            left_prompt_keys = set(\n                self.op_config.get(\"comparison_prompt\", \"\")\n                .split(\"{{ left.\")[1]\n                .split(\" }}\")[0]\n                .split(\".\")\n            )\n        except Exception as e:\n            self.console.log(f\"[red]Error parsing comparison prompt: {e}[/red]\")\n            left_prompt_keys = left_keys\n\n        try:\n            right_prompt_keys = set(\n                self.op_config.get(\"comparison_prompt\", \"\")\n                .split(\"{{ right.\")[1]\n                .split(\" }}\")[0]\n                .split(\".\")\n            )\n        except Exception as e:\n            self.console.log(f\"[red]Error parsing comparison prompt: {e}[/red]\")\n            right_prompt_keys = right_keys\n\n        # Sample a few records from each dataset\n        sample_left = random.sample(left_data, min(3, len(left_data)))\n        sample_right = random.sample(right_data, min(3, len(right_data)))\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an AI assistant tasked with generating containment-based blocking rules for an equijoin operation.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Generate multiple one-line Python statements that act as containment-based blocking rules for equijoin. These rules will be used in the form: `eval(blocking_rule, {{\"left\": item1, \"right\": item2}})`.\n\nAvailable keys in left dataset: {', '.join(left_keys)}\nAvailable keys in right dataset: {', '.join(right_keys)}\n\nSample data from left dataset:\n{json.dumps(sample_left, indent=2)}\n\nSample data from right dataset:\n{json.dumps(sample_right, indent=2)}\n\nComparison prompt used for detailed comparison:\n{self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\nPlease generate multiple one-line blocking rules that adhere to the following criteria:\n1. The rules should focus on containment relationships between fields in the left and right datasets. Containment can mean that the left field contains all the words in the right field, or the right field contains all the words in the left field.\n2. Each rule should evaluate to True if there's a potential match based on containment, False otherwise.\n3. Rules must be single Python expressions that can be evaluated using the eval() function.\n4. Rules should handle inconsistent casing by using string methods like .lower() when comparing string values.\n5. Consider the length of the fields when generating rules: for example, if the left field is much longer than the right field, it's more likely to contain all the words in the right field.\n\nExample structures of containment-based blocking rules:\n\"all(word in left['{{left_key}}'].lower() for word in right['{{right_key}}'].lower().split())\"\n\"any(word in right['{{right_key}}'].lower().split() for word in left['{{left_key}}'].lower().split())\"\n\nPlease provide 3-5 different containment-based blocking rules, based on the keys and sample data provided. Prioritize rules with the following keys: {', '.join(left_prompt_keys)} and {', '.join(right_prompt_keys)}.\"\"\",\n            },\n        ]\n\n        response = self.llm_client.generate_rewrite(\n            messages,\n            \"You are an expert in data matching and Python programming.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"containment_rules\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of containment-based blocking rules as Python expressions\",\n                    }\n                },\n                \"required\": [\"containment_rules\"],\n            },\n        )\n\n        containment_rules = response.choices[0].message.content\n        containment_rules = json.loads(containment_rules).get(\"containment_rules\")\n        return containment_rules\n\n    def _generate_blocking_rules_equijoin(\n        self,\n        left_keys: list[str],\n        right_keys: list[str],\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        comparisons: list[tuple[int, int, bool]],\n    ) -&gt; list[str]:\n        if not left_keys or not right_keys:\n            left_keys = list(left_data[0].keys())\n            right_keys = list(right_data[0].keys())\n\n        # Sample 2 true and 2 false comparisons\n        true_comparisons = [comp for comp in comparisons if comp[2]][:2]\n        false_comparisons = [comp for comp in comparisons if not comp[2]][:2]\n        sample_datas = [\n            (\n                {key: left_data[i][key] for key in left_keys if key in left_data[i]},\n                {key: right_data[j][key] for key in right_keys if key in right_data[j]},\n                is_match,\n            )\n            for i, j, is_match in true_comparisons + false_comparisons\n        ]\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following sample comparisons between entities, generate a single-line Python statement that acts as a blocking rule for equijoin. This rule will be used in the form: `eval(blocking_rule, {{\"left\": item1, \"right\": item2}})`.\n\n    Sample comparisons (note: these are just a few examples and may not represent all possible cases):\n    {json.dumps(sample_datas, indent=2)}\n\n    For context, here is the comparison prompt that will be used for the more expensive, detailed comparison:\n    {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n    Please generate ONE one-line blocking rule that adheres to the following criteria:\n    1. The rule should evaluate to True if the entities are possibly a match and require further comparison.\n    2. The rule should evaluate to False ONLY if the entities are definitely not a match.\n    3. The rule must be a single Python expression that can be evaluated using the eval() function.\n    4. The rule should be much faster to evaluate than the full comparison prompt.\n    5. The rule should capture the essence of the comparison prompt but in a simplified manner.\n    6. The rule should be general enough to work well on the entire dataset, not just these specific examples.\n    7. The rule should handle inconsistent casing by using string methods like .lower() when comparing string values.\n    8. The rule should err on the side of inclusivity - it's better to have false positives than false negatives.\n\n    Example structure of a one-line blocking rule:\n    \"(condition1) or (condition2) or (condition3)\"\n\n    Where conditions could be comparisons like:\n    \"left['{left_keys[0]}'].lower() == right['{right_keys[0]}'].lower()\"\n    \"abs(len(left['{left_keys[0]}']) - len(right['{right_keys[0]}'])) &lt;= 5\"\n    \"any(word in left['{left_keys[0]}'].lower() for word in right['{right_keys[0]}'].lower().split())\"\n\n    If there's no clear rule that can be generated based on the given information, return the string \"True\" to ensure all pairs are compared.\n\n    Remember, the primary goal of the blocking rule is to safely reduce the number of comparisons by quickly identifying pairs that are definitely not matches, while keeping all potential matches for further evaluation.\"\"\",\n            }\n        ]\n\n        for attempt in range(self.agent_max_retries):\n            response = self.llm_client.generate_rewrite(\n                messages,\n                \"You are an expert in entity resolution and Python programming. Your task is to generate one efficient blocking rule based on the given sample comparisons and data structure.\",\n                {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"blocking_rule\": {\n                            \"type\": \"string\",\n                            \"description\": \"One-line Python statement acting as a blocking rule\",\n                        }\n                    },\n                    \"required\": [\"blocking_rule\"],\n                },\n            )\n\n            blocking_rule = response.choices[0].message.content\n            blocking_rule = json.loads(blocking_rule).get(\"blocking_rule\")\n\n            if blocking_rule:\n                self.console.log(\"\")\n\n                if blocking_rule.strip() == \"True\":\n                    self.console.log(\n                        \"[yellow]No suitable blocking rule could be found. Proceeding without a blocking rule.[/yellow]\"\n                    )\n                    return []\n\n                self.console.log(\n                    f\"[bold]Generated blocking rule (Attempt {attempt + 1}):[/bold] {blocking_rule}\"\n                )\n\n                # Test the blocking rule\n                filtered_pairs = self._test_blocking_rule_equijoin(\n                    left_data,\n                    right_data,\n                    left_keys,\n                    right_keys,\n                    blocking_rule,\n                    comparisons,\n                )\n\n                if not filtered_pairs:\n                    self.console.log(\n                        \"[green]Blocking rule looks good! No known matches were filtered out.[/green]\"\n                    )\n                    return [blocking_rule]\n                else:\n                    feedback = f\"The previous rule incorrectly filtered out {len(filtered_pairs)} known matches. \"\n                    feedback += (\n                        \"Here are up to 3 examples of incorrectly filtered pairs:\\n\"\n                    )\n                    for i, j in filtered_pairs[:3]:\n                        feedback += f\"Left: {json.dumps({key: left_data[i][key] for key in left_keys})}\\n\"\n                        feedback += f\"Right: {json.dumps({key: right_data[j][key] for key in right_keys})}\\n\"\n                        feedback += \"These pairs are known matches but were filtered out by the rule.\\n\"\n                    feedback += \"Please generate a new rule that doesn't filter out these matches.\"\n\n                    messages.append({\"role\": \"assistant\", \"content\": blocking_rule})\n                    messages.append({\"role\": \"user\", \"content\": feedback})\n            else:\n                self.console.log(\"[yellow]No blocking rule generated.[/yellow]\")\n                return []\n\n        self.console.log(\n            f\"[yellow]Failed to generate a suitable blocking rule after {self.agent_max_retries} attempts. Proceeding without a blocking rule.[/yellow]\"\n        )\n        return []\n\n    def _test_blocking_rule_equijoin(\n        self,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        left_keys: list[str],\n        right_keys: list[str],\n        blocking_rule: str,\n        comparisons: list[tuple[int, int, bool]],\n    ) -&gt; list[tuple[int, int]]:\n        def apply_blocking_rule(left, right):\n            try:\n                return eval(blocking_rule, {\"left\": left, \"right\": right})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        filtered_pairs = []\n\n        for i, j, is_match in comparisons:\n            if is_match:\n                left = left_data[i]\n                right = right_data[j]\n                if not apply_blocking_rule(left, right):\n                    filtered_pairs.append((i, j))\n\n        if filtered_pairs:\n            self.console.log(\n                f\"[yellow italic]LLM Correction: The blocking rule incorrectly filtered out {len(filtered_pairs)} known positive matches.[/yellow italic]\"\n            )\n            for i, j in filtered_pairs[:5]:  # Show up to 5 examples\n                left_dict = {key: left_data[i][key] for key in left_keys}\n                right_dict = {key: right_data[j][key] for key in right_keys}\n                self.console.log(\n                    f\"  Incorrectly filtered pair - Left: {json.dumps(left_dict)}  Right: {json.dumps(right_dict)}\"\n                )\n            if len(filtered_pairs) &gt; 5:\n                self.console.log(\n                    f\"  ... and {len(filtered_pairs) - 5} more incorrect pairs.\"\n                )\n\n        return filtered_pairs\n\n    def _verify_blocking_rule_equijoin(\n        self,\n        left_data: list[dict[str, Any]],\n        right_data: list[dict[str, Any]],\n        blocking_rule: str,\n        left_keys: list[str],\n        right_keys: list[str],\n        comparison_results: list[tuple[int, int, bool]],\n    ) -&gt; tuple[list[tuple[int, int]], float]:\n        def apply_blocking_rule(left, right):\n            try:\n                return eval(blocking_rule, {\"left\": left, \"right\": right})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        false_negatives = []\n        total_pairs = 0\n        blocked_pairs = 0\n\n        for i, j, is_match in comparison_results:\n            total_pairs += 1\n            left = left_data[i]\n            right = right_data[j]\n            if apply_blocking_rule(left, right):\n                blocked_pairs += 1\n                if is_match:\n                    false_negatives.append((i, j))\n\n        rule_selectivity = blocked_pairs / total_pairs if total_pairs &gt; 0 else 0\n\n        return false_negatives, rule_selectivity\n\n    def _update_config_equijoin(\n        self,\n        threshold: float,\n        left_keys: list[str],\n        right_keys: list[str],\n        blocking_rules: list[str],\n    ) -&gt; dict[str, Any]:\n        optimized_config = self.op_config.copy()\n        optimized_config[\"blocking_keys\"] = {\n            \"left\": left_keys,\n            \"right\": right_keys,\n        }\n        optimized_config[\"blocking_threshold\"] = threshold\n        if blocking_rules:\n            optimized_config[\"blocking_conditions\"] = blocking_rules\n        if \"embedding_model\" not in optimized_config:\n            optimized_config[\"embedding_model\"] = \"text-embedding-3-small\"\n        return optimized_config\n\n    def _verify_blocking_rule(\n        self,\n        input_data: list[dict[str, Any]],\n        blocking_rule: str,\n        blocking_keys: list[str],\n        comparison_results: list[tuple[int, int, bool]],\n    ) -&gt; tuple[list[tuple[int, int]], float]:\n        def apply_blocking_rule(item1, item2):\n            try:\n                return eval(blocking_rule, {\"input1\": item1, \"input2\": item2})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        false_negatives = []\n        total_pairs = 0\n        blocked_pairs = 0\n\n        for i, j, is_match in comparison_results:\n            total_pairs += 1\n            item1 = {k: input_data[i][k] for k in blocking_keys if k in input_data[i]}\n            item2 = {k: input_data[j][k] for k in blocking_keys if k in input_data[j]}\n\n            if apply_blocking_rule(item1, item2):\n                blocked_pairs += 1\n                if is_match:\n                    false_negatives.append((i, j))\n\n        rule_selectivity = blocked_pairs / total_pairs if total_pairs &gt; 0 else 0\n\n        return false_negatives, rule_selectivity\n\n    def _update_config(\n        self, threshold: float, blocking_keys: list[str], blocking_rules: list[str]\n    ) -&gt; dict[str, Any]:\n        optimized_config = self.op_config.copy()\n        optimized_config[\"blocking_keys\"] = blocking_keys\n        optimized_config[\"blocking_threshold\"] = threshold\n        if blocking_rules:\n            optimized_config[\"blocking_conditions\"] = blocking_rules\n        if \"embedding_model\" not in optimized_config:\n            optimized_config[\"embedding_model\"] = \"text-embedding-3-small\"\n        return optimized_config\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.join_optimizer.JoinOptimizer.should_optimize","title":"<code>should_optimize(input_data)</code>","text":"<p>Determine if the given operation configuration should be optimized.</p> Source code in <code>docetl/optimizers/join_optimizer.py</code> <pre><code>def should_optimize(self, input_data: list[dict[str, Any]]) -&gt; tuple[bool, str]:\n    \"\"\"\n    Determine if the given operation configuration should be optimized.\n    \"\"\"\n    # If there are no blocking keys or embeddings, then we don't need to optimize\n    if not self.op_config.get(\"blocking_conditions\") or not self.op_config.get(\n        \"blocking_threshold\"\n    ):\n        return True, \"\"\n\n    # Check if the operation is marked as empty\n    elif self.op_config.get(\"empty\", False):\n        # Extract the map prompt from the intermediates\n        map_prompt = self.op_config[\"_intermediates\"][\"map_prompt\"]\n        reduce_key = self.op_config[\"_intermediates\"][\"reduce_key\"]\n\n        if reduce_key is None:\n            raise ValueError(\n                \"[yellow]Warning: No reduce key found in intermediates for synthesized resolve operation.[/yellow]\"\n            )\n\n        dedup = True\n        explanation = \"There is a reduce operation that does not follow a resolve operation. Consider adding a resolve operation to deduplicate the data.\"\n\n        if map_prompt:\n            # Analyze the map prompt\n            analysis, explanation = self._analyze_map_prompt_categorization(\n                map_prompt\n            )\n\n            if analysis:\n                dedup = False\n        else:\n            self.console.log(\n                \"[yellow]No map prompt found in intermediates for analysis.[/yellow]\"\n            )\n\n        # TODO: figure out why this would ever be the case\n        if not map_prompt:\n            map_prompt = \"N/A\"\n\n        if dedup is False:\n            dedup, explanation = self._determine_duplicate_keys(\n                input_data, reduce_key, map_prompt\n            )\n\n        # Now do the last attempt of pairwise comparisons\n        if dedup is False:\n            # Sample up to 20 random pairs of keys for duplicate analysis\n            sampled_pairs = self._sample_random_pairs(input_data, 20)\n\n            # Use LLM to check for duplicates\n            duplicates_found, explanation = self._check_duplicates_with_llm(\n                input_data, sampled_pairs, reduce_key, map_prompt\n            )\n\n            if duplicates_found:\n                dedup = True\n\n        return dedup, explanation\n\n    return False, \"\"\n</code></pre>"},{"location":"api-reference/python/","title":"Python API","text":""},{"location":"api-reference/python/#operations","title":"Operations","text":""},{"location":"api-reference/python/#docetl.schemas.MapOp","title":"<code>docetl.schemas.MapOp = map.MapOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.ResolveOp","title":"<code>docetl.schemas.ResolveOp = resolve.ResolveOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.ReduceOp","title":"<code>docetl.schemas.ReduceOp = reduce.ReduceOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.ParallelMapOp","title":"<code>docetl.schemas.ParallelMapOp = map.ParallelMapOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.FilterOp","title":"<code>docetl.schemas.FilterOp = filter.FilterOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.EquijoinOp","title":"<code>docetl.schemas.EquijoinOp = equijoin.EquijoinOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.SplitOp","title":"<code>docetl.schemas.SplitOp = split.SplitOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.GatherOp","title":"<code>docetl.schemas.GatherOp = gather.GatherOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.UnnestOp","title":"<code>docetl.schemas.UnnestOp = unnest.UnnestOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.SampleOp","title":"<code>docetl.schemas.SampleOp = sample.SampleOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.ClusterOp","title":"<code>docetl.schemas.ClusterOp = cluster.ClusterOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.CodeMapOp","title":"<code>docetl.schemas.CodeMapOp = code_operations.CodeMapOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.CodeReduceOp","title":"<code>docetl.schemas.CodeReduceOp = code_operations.CodeReduceOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.CodeFilterOp","title":"<code>docetl.schemas.CodeFilterOp = code_operations.CodeFilterOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.ExtractOp","title":"<code>docetl.schemas.ExtractOp = extract.ExtractOperation.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#callable-support-for-code-ops","title":"Callable support for code ops","text":"<p>Code operations (<code>code_map</code>, <code>code_reduce</code>, <code>code_filter</code>) accept either a string containing Python code that defines a <code>transform</code> function, or a regular Python function. When you pass a function, it does not need to be named <code>transform</code>; DocETL binds it internally.</p> <p>Example:</p> <pre><code>from docetl.api import CodeMapOp\n\ndef my_map(doc: dict) -&gt; dict:\n    return {\"double\": doc[\"x\"] * 2}\n\ncode_map = CodeMapOp(name=\"double_x\", type=\"code_map\", code=my_map)\n</code></pre> <ul> <li>Map: <code>fn(doc: dict) -&gt; dict</code></li> <li>Filter: <code>fn(doc: dict) -&gt; bool</code></li> <li>Reduce: <code>fn(group: list[dict]) -&gt; dict</code></li> </ul> <p>See also: Code Operators, Extract Operator</p>"},{"location":"api-reference/python/#dataset-and-pipeline","title":"Dataset and Pipeline","text":""},{"location":"api-reference/python/#docetl.schemas.Dataset","title":"<code>docetl.schemas.Dataset = dataset.Dataset.schema</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/python/#docetl.schemas.ParsingTool","title":"<code>docetl.schemas.ParsingTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a parsing tool used for custom data parsing in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the parsing tool. This should be unique within the pipeline configuration.</p> <code>function_code</code> <code>str</code> <p>The Python code defining the parsing function. This code will be executed                  to parse the input data according to the specified logic. It should return a list of strings, where each string is its own document.</p> Example <pre><code>parsing_tools:\n  - name: ocr_parser\n    function_code: |\n      import pytesseract\n      from pdf2image import convert_from_path\n      def ocr_parser(filename: str) -&gt; list[str]:\n          images = convert_from_path(filename)\n          text = \"\"\n          for image in images:\n              text += pytesseract.image_to_string(image)\n          return [text]\n</code></pre> Source code in <code>docetl/base_schemas.py</code> <pre><code>class ParsingTool(BaseModel):\n    \"\"\"\n    Represents a parsing tool used for custom data parsing in the pipeline.\n\n    Attributes:\n        name (str): The name of the parsing tool. This should be unique within the pipeline configuration.\n        function_code (str): The Python code defining the parsing function. This code will be executed\n                             to parse the input data according to the specified logic. It should return a list of strings, where each string is its own document.\n\n    Example:\n        ```yaml\n        parsing_tools:\n          - name: ocr_parser\n            function_code: |\n              import pytesseract\n              from pdf2image import convert_from_path\n              def ocr_parser(filename: str) -&gt; list[str]:\n                  images = convert_from_path(filename)\n                  text = \"\"\n                  for image in images:\n                      text += pytesseract.image_to_string(image)\n                  return [text]\n        ```\n    \"\"\"\n\n    name: str\n    function_code: str\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.PipelineStep","title":"<code>docetl.schemas.PipelineStep</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the step.</p> <code>operations</code> <code>list[dict[str, Any] | str]</code> <p>A list of operations to be applied in this step. Each operation can be either a string (the name of the operation) or a dictionary (for more complex configurations).</p> <code>input</code> <code>str | None</code> <p>The input for this step. It can be either the name of a dataset or the name of a previous step. If not provided, the step will use the output of the previous step as its input.</p> Example <pre><code># Simple step with a single operation\nprocess_step = PipelineStep(\n    name=\"process_step\",\n    input=\"my_dataset\",\n    operations=[\"process\"]\n)\n\n# Step with multiple operations\nsummarize_step = PipelineStep(\n    name=\"summarize_step\",\n    input=\"process_step\",\n    operations=[\"summarize\"]\n)\n\n# Step with a more complex operation configuration\ncustom_step = PipelineStep(\n    name=\"custom_step\",\n    input=\"previous_step\",\n    operations=[\n        {\n            \"custom_operation\": {\n                \"model\": \"gpt-4\",\n                \"prompt\": \"Perform a custom analysis on the following text:\"\n            }\n        }\n    ]\n)\n</code></pre> <p>These examples show different ways to configure pipeline steps, from simple single-operation steps to more complex configurations with custom parameters.</p> Source code in <code>docetl/base_schemas.py</code> <pre><code>class PipelineStep(BaseModel):\n    \"\"\"\n    Represents a step in the pipeline.\n\n    Attributes:\n        name (str): The name of the step.\n        operations (list[dict[str, Any] | str]): A list of operations to be applied in this step.\n            Each operation can be either a string (the name of the operation) or a dictionary\n            (for more complex configurations).\n        input (str | None): The input for this step. It can be either the name of a dataset\n            or the name of a previous step. If not provided, the step will use the output\n            of the previous step as its input.\n\n    Example:\n        ```python\n        # Simple step with a single operation\n        process_step = PipelineStep(\n            name=\"process_step\",\n            input=\"my_dataset\",\n            operations=[\"process\"]\n        )\n\n        # Step with multiple operations\n        summarize_step = PipelineStep(\n            name=\"summarize_step\",\n            input=\"process_step\",\n            operations=[\"summarize\"]\n        )\n\n        # Step with a more complex operation configuration\n        custom_step = PipelineStep(\n            name=\"custom_step\",\n            input=\"previous_step\",\n            operations=[\n                {\n                    \"custom_operation\": {\n                        \"model\": \"gpt-4\",\n                        \"prompt\": \"Perform a custom analysis on the following text:\"\n                    }\n                }\n            ]\n        )\n        ```\n\n    These examples show different ways to configure pipeline steps, from simple\n    single-operation steps to more complex configurations with custom parameters.\n    \"\"\"\n\n    name: str\n    operations: list[dict[str, Any] | str]\n    input: str | None = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.PipelineOutput","title":"<code>docetl.schemas.PipelineOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the output configuration for a pipeline.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of output. This could be 'file', 'database', etc.</p> <code>path</code> <code>str</code> <p>The path where the output will be stored. This could be a file path,         database connection string, etc., depending on the type.</p> <code>intermediate_dir</code> <code>str | None</code> <p>The directory to store intermediate results,                               if applicable. Defaults to None.</p> Example <pre><code>output = PipelineOutput(\n    type=\"file\",\n    path=\"/path/to/output.json\",\n    intermediate_dir=\"/path/to/intermediate/results\"\n)\n</code></pre> Source code in <code>docetl/base_schemas.py</code> <pre><code>class PipelineOutput(BaseModel):\n    \"\"\"\n    Represents the output configuration for a pipeline.\n\n    Attributes:\n        type (str): The type of output. This could be 'file', 'database', etc.\n        path (str): The path where the output will be stored. This could be a file path,\n                    database connection string, etc., depending on the type.\n        intermediate_dir (str | None): The directory to store intermediate results,\n                                          if applicable. Defaults to None.\n\n    Example:\n        ```python\n        output = PipelineOutput(\n            type=\"file\",\n            path=\"/path/to/output.json\",\n            intermediate_dir=\"/path/to/intermediate/results\"\n        )\n        ```\n    \"\"\"\n\n    type: str\n    path: str\n    intermediate_dir: str | None = None\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline","title":"<code>docetl.api.Pipeline</code>","text":"<p>Represents a complete document processing pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the pipeline.</p> <code>datasets</code> <code>dict[str, Dataset]</code> <p>A dictionary of datasets used in the pipeline,                            where keys are dataset names and values are Dataset objects.</p> <code>operations</code> <code>list[OpType]</code> <p>A list of operations to be performed in the pipeline.</p> <code>steps</code> <code>list[PipelineStep]</code> <p>A list of steps that make up the pipeline.</p> <code>output</code> <code>PipelineOutput</code> <p>The output configuration for the pipeline.</p> <code>parsing_tools</code> <code>list[ParsingTool]</code> <p>A list of parsing tools used in the pipeline.                                Defaults to an empty list.</p> <code>default_model</code> <code>str | None</code> <p>The default language model to use for operations                            that require one. Defaults to None.</p> Example <pre><code>def custom_parser(text: str) -&gt; list[str]:\n    # this will convert the text in the column to uppercase\n    # You should return a list of strings, where each string is a separate document\n    return [text.upper()]\n\npipeline = Pipeline(\n    name=\"document_processing_pipeline\",\n    datasets={\n        \"input_data\": Dataset(type=\"file\", path=\"/path/to/input.json\", parsing=[{\"name\": \"custom_parser\", \"input_key\": \"content\", \"output_key\": \"uppercase_content\"}]),\n    },\n    parsing_tools=[custom_parser],\n    operations=[\n        MapOp(\n            name=\"process\",\n            type=\"map\",\n            prompt=\"Determine what type of document this is: {{ input.uppercase_content }}\",\n            output={\"schema\": {\"document_type\": \"string\"}}\n        ),\n        ReduceOp(\n            name=\"summarize\",\n            type=\"reduce\",\n            reduce_key=\"document_type\",\n            prompt=\"Summarize the processed contents: {% for item in inputs %}{{ item.uppercase_content }} {% endfor %}\",\n            output={\"schema\": {\"summary\": \"string\"}}\n        )\n    ],\n    steps=[\n        PipelineStep(name=\"process_step\", input=\"input_data\", operations=[\"process\"]),\n        PipelineStep(name=\"summarize_step\", input=\"process_step\", operations=[\"summarize\"])\n    ],\n    output=PipelineOutput(type=\"file\", path=\"/path/to/output.json\"),\n    default_model=\"gpt-4o-mini\"\n)\n</code></pre> <p>This example shows a complete pipeline configuration with datasets, operations, steps, and output settings.</p> Source code in <code>docetl/api.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Represents a complete document processing pipeline.\n\n    Attributes:\n        name (str): The name of the pipeline.\n        datasets (dict[str, Dataset]): A dictionary of datasets used in the pipeline,\n                                       where keys are dataset names and values are Dataset objects.\n        operations (list[OpType]): A list of operations to be performed in the pipeline.\n        steps (list[PipelineStep]): A list of steps that make up the pipeline.\n        output (PipelineOutput): The output configuration for the pipeline.\n        parsing_tools (list[ParsingTool]): A list of parsing tools used in the pipeline.\n                                           Defaults to an empty list.\n        default_model (str | None): The default language model to use for operations\n                                       that require one. Defaults to None.\n\n    Example:\n        ```python\n        def custom_parser(text: str) -&gt; list[str]:\n            # this will convert the text in the column to uppercase\n            # You should return a list of strings, where each string is a separate document\n            return [text.upper()]\n\n        pipeline = Pipeline(\n            name=\"document_processing_pipeline\",\n            datasets={\n                \"input_data\": Dataset(type=\"file\", path=\"/path/to/input.json\", parsing=[{\"name\": \"custom_parser\", \"input_key\": \"content\", \"output_key\": \"uppercase_content\"}]),\n            },\n            parsing_tools=[custom_parser],\n            operations=[\n                MapOp(\n                    name=\"process\",\n                    type=\"map\",\n                    prompt=\"Determine what type of document this is: {{ input.uppercase_content }}\",\n                    output={\"schema\": {\"document_type\": \"string\"}}\n                ),\n                ReduceOp(\n                    name=\"summarize\",\n                    type=\"reduce\",\n                    reduce_key=\"document_type\",\n                    prompt=\"Summarize the processed contents: {% for item in inputs %}{{ item.uppercase_content }} {% endfor %}\",\n                    output={\"schema\": {\"summary\": \"string\"}}\n                )\n            ],\n            steps=[\n                PipelineStep(name=\"process_step\", input=\"input_data\", operations=[\"process\"]),\n                PipelineStep(name=\"summarize_step\", input=\"process_step\", operations=[\"summarize\"])\n            ],\n            output=PipelineOutput(type=\"file\", path=\"/path/to/output.json\"),\n            default_model=\"gpt-4o-mini\"\n        )\n        ```\n\n    This example shows a complete pipeline configuration with datasets, operations,\n    steps, and output settings.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        datasets: dict[str, Dataset],\n        operations: list[OpType],\n        steps: list[PipelineStep],\n        output: PipelineOutput,\n        parsing_tools: list[ParsingTool | Callable] = [],\n        default_model: str | None = None,\n        rate_limits: dict[str, int] | None = None,\n        optimizer_config: dict[str, Any] = {},\n        **kwargs,\n    ):\n        self.name = name\n        self.datasets = datasets\n        self.operations = operations\n        self.steps = steps\n        self.output = output\n        self.parsing_tools = [\n            (\n                tool\n                if isinstance(tool, ParsingTool)\n                else ParsingTool(\n                    name=tool.__name__, function_code=inspect.getsource(tool)\n                )\n            )\n            for tool in parsing_tools\n        ]\n        self.default_model = default_model\n        self.rate_limits = rate_limits\n        self.optimizer_config = optimizer_config\n\n        # Add other kwargs to self.other_config\n        self.other_config = kwargs\n\n        self._load_env()\n\n    def _load_env(self):\n        import os\n\n        from dotenv import load_dotenv\n\n        # Get the current working directory\n        cwd = os.getcwd()\n\n        # Load .env file from the current working directory if it exists\n        env_file = os.path.join(cwd, \".env\")\n        if os.path.exists(env_file):\n            load_dotenv(env_file)\n\n    def optimize(\n        self,\n        max_threads: int | None = None,\n        resume: bool = False,\n        save_path: str | None = None,\n    ) -&gt; \"Pipeline\":\n        \"\"\"\n        Optimize the pipeline using the Optimizer.\n\n        Args:\n            max_threads (int | None): Maximum number of threads to use for optimization.\n            model (str): The model to use for optimization. Defaults to \"gpt-4o\".\n            resume (bool): Whether to resume optimization from a previous state. Defaults to False.\n            timeout (int): Timeout for optimization in seconds. Defaults to 60.\n\n        Returns:\n            Pipeline: An optimized version of the pipeline.\n        \"\"\"\n        config = self._to_dict()\n        runner = DSLRunner(\n            config,\n            base_name=os.path.join(os.getcwd(), self.name),\n            yaml_file_suffix=self.name,\n            max_threads=max_threads,\n        )\n        optimized_config, _ = runner.optimize(\n            resume=resume,\n            return_pipeline=False,\n            save_path=save_path,\n        )\n\n        updated_pipeline = Pipeline(\n            name=self.name,\n            datasets=self.datasets,\n            operations=self.operations,\n            steps=self.steps,\n            output=self.output,\n            default_model=self.default_model,\n            parsing_tools=self.parsing_tools,\n            optimizer_config=self.optimizer_config,\n        )\n        updated_pipeline._update_from_dict(optimized_config)\n        return updated_pipeline\n\n    def run(self, max_threads: int | None = None) -&gt; float:\n        \"\"\"\n        Run the pipeline using the DSLRunner.\n\n        Args:\n            max_threads (int | None): Maximum number of threads to use for execution.\n\n        Returns:\n            float: The total cost of running the pipeline.\n        \"\"\"\n        config = self._to_dict()\n        runner = DSLRunner(\n            config,\n            base_name=os.path.join(os.getcwd(), self.name),\n            yaml_file_suffix=self.name,\n            max_threads=max_threads,\n        )\n        result = runner.load_run_save()\n        return result\n\n    def to_yaml(self, path: str) -&gt; None:\n        \"\"\"\n        Convert the Pipeline object to a YAML string and save it to a file.\n\n        Args:\n            path (str): Path to save the YAML file.\n\n        Returns:\n            None\n        \"\"\"\n        config = self._to_dict()\n        with open(path, \"w\") as f:\n            yaml.safe_dump(config, f)\n\n        print(f\"[green]Pipeline saved to {path}[/green]\")\n\n    def _to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert the Pipeline object to a dictionary representation.\n\n        Returns:\n            dict[str, Any]: Dictionary representation of the Pipeline.\n        \"\"\"\n        d = {\n            \"datasets\": {\n                name: dataset.dict() for name, dataset in self.datasets.items()\n            },\n            \"operations\": [\n                {k: v for k, v in op.dict().items() if v is not None}\n                for op in self.operations\n            ],\n            \"pipeline\": {\n                \"steps\": [\n                    {k: v for k, v in step.dict().items() if v is not None}\n                    for step in self.steps\n                ],\n                \"output\": self.output.dict(),\n            },\n            \"default_model\": self.default_model,\n            \"parsing_tools\": (\n                [tool.dict() for tool in self.parsing_tools]\n                if self.parsing_tools\n                else None\n            ),\n            \"optimizer_config\": self.optimizer_config,\n            **self.other_config,\n        }\n        if self.rate_limits:\n            d[\"rate_limits\"] = self.rate_limits\n        return d\n\n    def _update_from_dict(self, config: dict[str, Any]):\n        \"\"\"\n        Update the Pipeline object from a dictionary representation.\n\n        Args:\n            config (dict[str, Any]): Dictionary representation of the Pipeline.\n        \"\"\"\n        self.datasets = {\n            name: Dataset(\n                type=dataset[\"type\"],\n                source=dataset[\"source\"],\n                path=dataset[\"path\"],\n                parsing=dataset.get(\"parsing\"),\n            )\n            for name, dataset in config[\"datasets\"].items()\n        }\n        self.operations = []\n        for op in config[\"operations\"]:\n            op_type = op.pop(\"type\")\n            if op_type == \"map\":\n                self.operations.append(MapOp(**op, type=op_type))\n            elif op_type == \"resolve\":\n                self.operations.append(ResolveOp(**op, type=op_type))\n            elif op_type == \"reduce\":\n                self.operations.append(ReduceOp(**op, type=op_type))\n            elif op_type == \"parallel_map\":\n                self.operations.append(ParallelMapOp(**op, type=op_type))\n            elif op_type == \"filter\":\n                self.operations.append(FilterOp(**op, type=op_type))\n            elif op_type == \"equijoin\":\n                self.operations.append(EquijoinOp(**op, type=op_type))\n            elif op_type == \"split\":\n                self.operations.append(SplitOp(**op, type=op_type))\n            elif op_type == \"gather\":\n                self.operations.append(GatherOp(**op, type=op_type))\n            elif op_type == \"unnest\":\n                self.operations.append(UnnestOp(**op, type=op_type))\n            elif op_type == \"cluster\":\n                self.operations.append(ClusterOp(**op, type=op_type))\n            elif op_type == \"sample\":\n                self.operations.append(SampleOp(**op, type=op_type))\n            elif op_type == \"code_map\":\n                self.operations.append(CodeMapOp(**op, type=op_type))\n            elif op_type == \"code_reduce\":\n                self.operations.append(CodeReduceOp(**op, type=op_type))\n            elif op_type == \"code_filter\":\n                self.operations.append(CodeFilterOp(**op, type=op_type))\n            elif op_type == \"extract\":\n                self.operations.append(ExtractOp(**op, type=op_type))\n        self.steps = [PipelineStep(**step) for step in config[\"pipeline\"][\"steps\"]]\n        self.output = PipelineOutput(**config[\"pipeline\"][\"output\"])\n        self.default_model = config.get(\"default_model\")\n        self.parsing_tools = (\n            [ParsingTool(**tool) for tool in config.get(\"parsing_tools\", [])]\n            if config.get(\"parsing_tools\")\n            else []\n        )\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline.optimize","title":"<code>optimize(max_threads=None, resume=False, save_path=None)</code>","text":"<p>Optimize the pipeline using the Optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>max_threads</code> <code>int | None</code> <p>Maximum number of threads to use for optimization.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for optimization. Defaults to \"gpt-4o\".</p> required <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous state. Defaults to False.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout for optimization in seconds. Defaults to 60.</p> required <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>An optimized version of the pipeline.</p> Source code in <code>docetl/api.py</code> <pre><code>def optimize(\n    self,\n    max_threads: int | None = None,\n    resume: bool = False,\n    save_path: str | None = None,\n) -&gt; \"Pipeline\":\n    \"\"\"\n    Optimize the pipeline using the Optimizer.\n\n    Args:\n        max_threads (int | None): Maximum number of threads to use for optimization.\n        model (str): The model to use for optimization. Defaults to \"gpt-4o\".\n        resume (bool): Whether to resume optimization from a previous state. Defaults to False.\n        timeout (int): Timeout for optimization in seconds. Defaults to 60.\n\n    Returns:\n        Pipeline: An optimized version of the pipeline.\n    \"\"\"\n    config = self._to_dict()\n    runner = DSLRunner(\n        config,\n        base_name=os.path.join(os.getcwd(), self.name),\n        yaml_file_suffix=self.name,\n        max_threads=max_threads,\n    )\n    optimized_config, _ = runner.optimize(\n        resume=resume,\n        return_pipeline=False,\n        save_path=save_path,\n    )\n\n    updated_pipeline = Pipeline(\n        name=self.name,\n        datasets=self.datasets,\n        operations=self.operations,\n        steps=self.steps,\n        output=self.output,\n        default_model=self.default_model,\n        parsing_tools=self.parsing_tools,\n        optimizer_config=self.optimizer_config,\n    )\n    updated_pipeline._update_from_dict(optimized_config)\n    return updated_pipeline\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline.run","title":"<code>run(max_threads=None)</code>","text":"<p>Run the pipeline using the DSLRunner.</p> <p>Parameters:</p> Name Type Description Default <code>max_threads</code> <code>int | None</code> <p>Maximum number of threads to use for execution.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The total cost of running the pipeline.</p> Source code in <code>docetl/api.py</code> <pre><code>def run(self, max_threads: int | None = None) -&gt; float:\n    \"\"\"\n    Run the pipeline using the DSLRunner.\n\n    Args:\n        max_threads (int | None): Maximum number of threads to use for execution.\n\n    Returns:\n        float: The total cost of running the pipeline.\n    \"\"\"\n    config = self._to_dict()\n    runner = DSLRunner(\n        config,\n        base_name=os.path.join(os.getcwd(), self.name),\n        yaml_file_suffix=self.name,\n        max_threads=max_threads,\n    )\n    result = runner.load_run_save()\n    return result\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline.to_yaml","title":"<code>to_yaml(path)</code>","text":"<p>Convert the Pipeline object to a YAML string and save it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the YAML file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>docetl/api.py</code> <pre><code>def to_yaml(self, path: str) -&gt; None:\n    \"\"\"\n    Convert the Pipeline object to a YAML string and save it to a file.\n\n    Args:\n        path (str): Path to save the YAML file.\n\n    Returns:\n        None\n    \"\"\"\n    config = self._to_dict()\n    with open(path, \"w\") as f:\n        yaml.safe_dump(config, f)\n\n    print(f\"[green]Pipeline saved to {path}[/green]\")\n</code></pre>"},{"location":"community/","title":"Community","text":"<p>Welcome to the DocETL community! We're excited to have you join us in exploring and improving document extraction and transformation workflows. We are committed to fostering an inclusive community for all people, regardless of technical background.</p>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>While we don't have a formal code of conduct page, we expect all community members to treat each other with respect and kindness. We do not tolerate harassment or discrimination of any kind. If you experience any issues, please reach out to the project maintainers immediately.</p>"},{"location":"community/#contributions","title":"Contributions","text":"<p>We welcome contributions from everyone who is interested in improving DocETL. Here's how you can get involved:</p> <ol> <li> <p>Report Issues: If you encounter a bug or have a feature request, open an issue on our GitHub repository.</p> </li> <li> <p>Join Discussions: Have a question or want to discuss ideas? Post on our Discord server.</p> </li> <li> <p>Contribute Code: Look for issues tagged with \"help wanted\" or \"good first issue\" on GitHub. These are great starting points for new contributors.</p> </li> <li> <p>Join Working Groups: We will create working groups in Discord focused on different project areas as discussed in our roadmap. Join the group(s) that interests you most!</p> </li> </ol> <p>To contribute code:</p> <ol> <li>Fork the repository on GitHub.</li> <li>Create a new branch for your changes.</li> <li>Make your changes in your branch.</li> <li>Submit a pull request with your changes.</li> </ol>"},{"location":"community/#connect-with-us","title":"Connect with Us","text":"<ul> <li>GitHub Repository: Contribute to the project or report issues on our GitHub repo.</li> <li>Discord Community: Join our Discord server; we're looking to build a vibrant community of people interested in intelligent document processing.</li> <li>Lab Webpages: We are affiliated with the EPIC Lab at UC Berkeley. Visit our Lab Page for a description of our research. We are also affiliated with the Data Systems and Foundations group at UC Berkeley--visit our DSF Page for more information.</li> </ul> <p>Request a Tutorial or Research Talk</p> <p>Interested in having us give a tutorial or research talk on DocETL? We'd love to connect! Please email shreyashankar@berkeley.edu to set up a time. Let us know what your team is interested in learning about (e.g., tutorial or research) so we can tailor the presentation to your interests.</p>"},{"location":"community/#frequently-encountered-issues","title":"Frequently Encountered Issues","text":""},{"location":"community/#keyerror-in-operations","title":"KeyError in Operations","text":"<p>If you're encountering a KeyError, it's often due to missing an unnest operation in your workflow. The unnest operation is crucial for flattening nested data structures.</p> <p>Solution: Add an unnest operation to your pipeline before accessing nested keys. If you're still having trouble, don't hesitate to open an issue on GitHub or ask for help on our Discord server.</p>"},{"location":"community/#browser-freezing-because-of-stale-client-storage","title":"Browser freezing because of stale client storage","text":"<p>Run the following script:</p> <p>Browser Storage Cleanup Script</p> <pre><code>// Function to delete all localStorage items with prefix 'docetl_'\nfunction cleanupDocETLStorage() {\n    const prefix = 'docetl_';\n    const itemsToDelete = [];\n\n    // First, collect all matching keys\n    for (let i = 0; i &lt; localStorage.length; i++) {\n        const key = localStorage.key(i);\n        if (key &amp;&amp; key.startsWith(prefix)) {\n            itemsToDelete.push(key);\n        }\n    }\n\n    // Then delete them and keep count\n    const deletedCount = itemsToDelete.length;\n    itemsToDelete.forEach(key =&gt; {\n        localStorage.removeItem(key);\n        console.log(`Deleted key: ${key}`);\n    });\n\n    console.log(`Cleanup complete. Deleted ${deletedCount} items with prefix \"${prefix}\"`);\n}\n\n// Execute the cleanup\ncleanupDocETLStorage();\n</code></pre>"},{"location":"community/roadmap/","title":"Roadmap","text":"<p>Join Our Working Groups</p> <p>Are you interested in contributing to any of these projects or have ideas for new areas of exploration? Join our Discord server to participate in our working groups and collaborate with the community!</p> <p>We're constantly working to improve DocETL and explore new possibilities in document processing. Our current ideas span both research and engineering problems, and are organized into the following categories:</p> <pre><code>mindmap\n  root((DocETL Roadmap))\n    User Interface and Interaction\n    Debugging and Optimization\n    Model and Tool Integrations\n    Agents and Planning</code></pre>"},{"location":"community/roadmap/#user-interface-and-interaction","title":"User Interface and Interaction","text":"<ul> <li>Natural Language to DocETL Pipeline: Building tools to generate DocETL pipelines from natural language descriptions.</li> <li>Interactive Pipeline Creation: Developing intuitive interfaces for creating and optimizing DocETL pipelines interactively.</li> </ul>"},{"location":"community/roadmap/#debugging-and-optimization","title":"Debugging and Optimization","text":"<ul> <li>DocETL Debugger: Creating a debugger with provenance tracking, allowing users to visualize all intermediates that contributed to a specific output.</li> <li>Plan Efficiency Optimization: Implementing strategies (and devising new strategies) to reduce latency and cost for the most accurate plans. This includes batching LLM calls, using model cascades, and fusing operators.</li> </ul>"},{"location":"community/roadmap/#model-and-tool-integrations","title":"Model and Tool Integrations","text":"<ul> <li>Model Diversity: Extending support beyond OpenAI to include a wider range of models, with a focus on local models.</li> <li>OCR and PDF Extraction: Improving integration with OCR technologies and PDF extraction tools for more robust document processing.</li> <li>Multimodal Data Processing: Enhancing DocETL to handle multimodal data, including text, images, audio, and video (as many of the LLMs support multimodal inputs, anyways).</li> </ul>"},{"location":"community/roadmap/#agents-and-planning","title":"Agents and Planning","text":"<ul> <li> <p>Smarter Agent and Planning Architectures: Optimizing plan exploration based on data characteristics. For instance, refining the optimizer to avoid unnecessary exploration of plans with the gather operator for tasks that don't require peripheral context when decomposing map operations for large documents.</p> </li> <li> <p>Context-Aware Sampling for Validation: Creating algorithms that can identify and extract the most representative samples from different parts of a document, including the beginning, middle, and end, to use in validaton prompts. This approach will help validation agents to verify that all sections of documents are adequately represented in the outputs, avoiding blind spots in the analysis due to truncation--as we currently naive truncate the middle of documents in validation prompts.</p> </li> <li> <p>Benchmarks: Developing a suite of benchmarks to evaluate the performance of different optimization strategies and agent architectures. These benchmarks will help us understand the trade-offs between accuracy, efficiency, and cost in different scenarios, guiding the development of more effective optimization techniques.</p> </li> </ul>"},{"location":"concepts/operators/","title":"Operators","text":"<p>Operators in DocETL are designed for semantically processing unstructured data. They form the building blocks of data processing pipelines, allowing you to transform, analyze, and manipulate datasets efficiently.</p>"},{"location":"concepts/operators/#overview","title":"Overview","text":"<ul> <li>Datasets contain items, where a item is an object in the JSON list, with fields and values. An item here could be simple text chunk or a document reference.</li> <li>DocETL provides several operators, each tailored for specific unstructured data processing tasks.</li> <li>By default, operations are parallelized on your data using multithreading for improved performance.</li> </ul> <p>Caching in DocETL</p> <p>DocETL employs caching for all LLM calls and partially-optimized plans. The cache is stored in the <code>.cache/docetl/general</code> and <code>.cache/docetl/llm</code> directories within your home directory. This caching mechanism helps to improve performance and reduce redundant API calls when running similar operations or reprocessing data.</p>"},{"location":"concepts/operators/#common-attributes","title":"Common Attributes","text":"<p>All operators share some common attributes:</p> <ul> <li><code>name</code>: A unique identifier for the operator.</li> <li><code>type</code>: Specifies the type of operation (e.g., \"map\", \"reduce\", \"filter\").</li> </ul> <p>LLM-based operators have additional attributes:</p> <ul> <li><code>prompt</code>: A Jinja2 template that defines the instruction for the language model.</li> <li><code>output</code>: Specifies the schema for the output from the LLM call.</li> <li><code>model</code> (optional): Allows specifying a different model from the pipeline default.</li> <li><code>litellm_completion_kwargs</code> (optional): Additional parameters to pass to LiteLLM completion calls.</li> </ul> <p>DocETL uses LiteLLM to execute all LLM calls, providing support for 100+ LLM providers including OpenAI, Anthropic, Azure, and more. You can pass any LiteLLM completion arguments using the <code>litellm_completion_kwargs</code> field.</p> <p>Example:</p> <pre><code>- name: extract_insights\n  type: map\n  model: gpt-4o-mini\n  litellm_completion_kwargs:\n    max_tokens: 500          # limit response length\n    temperature: 0.7         # control randomness\n    top_p: 0.9              # nucleus sampling parameter\n  prompt: |\n    Analyze the following user interaction log:\n    {{ input.log }}\n\n    Extract 2-3 main insights from this log, each being 1-2 words, to help inform future product development. Consider any difficulties or pain points the user may have had. Also provide 1-2 supporting actions for each insight.\n    Return the results as a list of dictionaries, each containing 'insight' and 'supporting_actions' keys.\n  output:\n    schema:\n      insights: \"list[{insight: string, supporting_actions: list[string]}]\"\n</code></pre>"},{"location":"concepts/operators/#input-and-output","title":"Input and Output","text":"<p>Prompts can reference any fields in the data, including:</p> <ul> <li>Original fields from the input data.</li> <li>Fields synthesized by previous operations in the pipeline.</li> </ul> <p>For map operations, you can only reference <code>input</code>, but in reduce operations, you can reference <code>inputs</code> (since it's a list of inputs).</p> <p>Example:</p> <pre><code>prompt: |\n  Summarize the user behavior insights for the country: {{ inputs[0].country }}\n\n  Insights and supporting actions:\n  {% for item in inputs %}\n  - Insight: {{ item.insight }}\n  Supporting actions:\n  {% for action in item.supporting_actions %}\n  - {{ action }}\n  {% endfor %}\n  {% endfor %}\n</code></pre> <p>What happens if the input is too long?</p> <p>When the input data exceeds the token limit of the LLM, DocETL automatically truncates tokens from the middle of the data to make it fit in the prompt. This approach preserves the beginning and end of the input, which often contain crucial context.</p> <p>A warning is displayed whenever truncation occurs, alerting you to potential loss of information:</p> <pre><code>WARNING: Input exceeded token limit. Truncated 500 tokens from the middle of the input.\n</code></pre> <p>If you frequently encounter this warning, consider using DocETL's optimizer or breaking down your input yourself into smaller chunks to handle large inputs more effectively.</p>"},{"location":"concepts/operators/#output-schema","title":"Output Schema","text":"<p>The <code>output</code> attribute defines the structure of the LLM's response. It supports various data types (see schemas for more details):</p> <ul> <li><code>string</code> (or <code>str</code>, <code>text</code>, <code>varchar</code>): For text data</li> <li><code>integer</code> (or <code>int</code>): For whole numbers</li> <li><code>number</code> (or <code>float</code>, <code>decimal</code>): For decimal numbers</li> <li><code>boolean</code> (or <code>bool</code>): For true/false values</li> <li><code>list</code>: For arrays or sequences of items</li> <li>objects: Using notation <code>{field: type}</code></li> <li><code>enum</code>: For a set of possible values</li> </ul> <p>Example:</p> <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, supporting_actions: string}]\"\n    detailed_summary: string\n</code></pre> <p>Keep Output Types Simple</p> <p>It's recommended to keep output types as simple as possible. Complex nested structures may be difficult for the LLM to consistently produce, potentially leading to parsing errors. The structured output feature works best with straightforward schemas. If you need complex data structures, consider breaking them down into multiple simpler operations.</p> <p>For example, instead of: <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, supporting_actions: list[{action: string, priority: integer}]}]\"\n</code></pre></p> <p>Consider: <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, supporting_actions: string}]\"\n</code></pre></p> <p>And then use a separate operation to further process the supporting actions if needed.</p> <p>Read more about schemas in the schemas section.</p>"},{"location":"concepts/operators/#validation","title":"Validation","text":"<p>Validation is a first-class citizen in DocETL, ensuring the quality and correctness of processed data.</p>"},{"location":"concepts/operators/#basic-validation","title":"Basic Validation","text":"<p>LLM-based operators can include a <code>validate</code> field, which accepts a list of Python statements:</p> <pre><code>validate:\n  - len(output[\"insights\"]) &gt;= 2\n  - all(len(insight[\"supporting_actions\"]) &gt;= 1 for insight in output[\"insights\"])\n</code></pre> <p>Access variables using dictionary syntax: <code>output[\"field\"]</code>. Note that you can't access <code>input</code> docs in validation, but the output docs should have all the fields from the input docs (for non-reduce operations), since fields pass through unchanged.</p> <p>The <code>num_retries_on_validate_failure</code> attribute specifies how many times to retry the LLM if any validation statements fail.</p>"},{"location":"concepts/operators/#advanced-validation-gleaning","title":"Advanced Validation: Gleaning","text":"<p>Gleaning is an advanced validation technique that uses LLM-based validators to refine outputs iteratively.</p> <p>To enable gleaning, specify:</p> <ul> <li><code>validation_prompt</code>: Instructions for the LLM to evaluate and improve the output.</li> <li><code>num_rounds</code>: The maximum number of refinement iterations.</li> <li><code>model</code> (optional): The model to use for the LLM executing the validation prompt. Defaults to the model specified for this operation. Note that if the validator LLM determines the output needs to be improved, the final output will be generated by the model specified for this operation.</li> <li><code>if</code> (optional): A Python boolean expression (evaluated with <code>safe_eval</code>) that refers to fields in the current <code>output</code>. If the expression evaluates to <code>False</code>, DocETL skips gleaning entirely.</li> </ul> <p>Example:</p> <pre><code>gleaning:\n  num_rounds: 1\n  validation_prompt: |\n    Evaluate the extraction for completeness and relevance:\n    1. Are all key user behaviors and pain points from the log addressed in the insights?\n    2. Are the supporting actions practical and relevant to the insights?\n    3. Is there any important information missing or any irrelevant information included?\n</code></pre> <p>This approach allows for context-aware validation and refinement of LLM outputs. Note that it is expensive, since it at least doubles the number of LLM calls required for each operator.</p> <p>Example map operation (with a different model for the validation prompt):</p> <pre><code>- name: extract_insights\n  type: map\n  model: gpt-4o\n  prompt: |\n    From the user log below, list 2-3 concise insights (1-2 words each) and 1-2 supporting actions per insight.\n    Return as a list of dictionaries with 'insight' and 'supporting_actions'.\n    Log: {{ input.log }}\n  output:\n    schema:\n      insights_summary: \"string\"\n  gleaning:\n    if: \"len(output['insights_summary']) &lt; 10\"  # Only refine if summary is too short\n    num_rounds: 2 # Will refine up to 2 times if needed\n    model: gpt-4o-mini\n    validation_prompt: |\n      There should be at least 2 insights, and each insight should have at least 1 supporting action.\n</code></pre> <p>Choosing a Different Model for Validation</p> <p>You may want to use a different model for the validation prompt. For example, you can use a more powerful (and expensive) model for generating outputs, but a cheaper model for validation\u2014especially if the validation only checks a single aspect. This approach helps reduce costs while still ensuring quality, since the final output is always produced by the more capable model.</p> <p>Conditional Gleaning</p> <p>You can also use the <code>if</code> field to conditionally skip gleaning. For example, if you only want to glean if the output is too short, you can use: <pre><code>gleaning:\n  if: \"len(output['insights_summary']) &lt; 10\"\n  num_rounds: 2\n</code></pre></p> <p>If the <code>if</code> field evaluates to <code>False</code>, DocETL skips gleaning entirely. Or, if the <code>if</code> field does not exist, DocETL will always glean.</p>"},{"location":"concepts/operators/#how-gleaning-works","title":"How Gleaning Works","text":"<p>Gleaning is an iterative process that refines LLM outputs using context-aware validation. Here's how it works:</p> <ol> <li> <p>Initial Operation: The LLM generates an initial output based on the original operation prompt.</p> </li> <li> <p>Validation: The validation prompt is appended to the chat thread, along with the original operation prompt and output. This is submitted to the LLM. Note that the validation prompt doesn't need any variables, since it's appended to the chat thread.</p> </li> <li> <p>Assessment: The LLM responds with an assessment of the output according to the validation prompt. The model used for this step is specified by the <code>model</code> field in the <code>gleaning</code> dictionary field, or defaults to the model specified for that operation.</p> </li> <li> <p>Decision: The system interprets the assessment:</p> <ul> <li>If there's no error or room for improvement, the current output is returned.</li> <li>If improvements are suggested, the process continues.</li> </ul> </li> <li> <p>Refinement: If improvements are needed:</p> <ul> <li>A new prompt is created, including the original operation prompt, the original output, and the validator feedback.</li> <li>This is submitted to the LLM to generate an improved output.</li> </ul> </li> <li> <p>Iteration: Steps 2-5 are repeated until either:</p> <ul> <li>The validator has no more feedback (i.e., the evaluation passes), or</li> <li>The number of iterations exceeds <code>num_rounds</code>.</li> </ul> </li> <li> <p>Final Output: The last refined output is returned.</p> </li> </ol> <p>Note that gleaning can significantly increase the number of LLM calls for each operator, potentially doubling it at minimum. While this increases cost and latency, it can lead to higher quality outputs for complex tasks.</p>"},{"location":"concepts/optimization/","title":"Optimization","text":"<p>Sometimes, finding the optimal pipeline for your task can be challenging. You might wonder:</p> <p>Questions</p> <ul> <li>Will a single LLM call suffice for your task?</li> <li>Do you need to decompose your task or data further for better results?</li> </ul> <p>To address these questions and improve your pipeline's performance, you can use DocETL to build an optimized version of your pipeline.</p>"},{"location":"concepts/optimization/#the-docetl-optimizer","title":"The DocETL Optimizer","text":"<p>The DocETL optimizer is designed to decompose operators (and sequences of operators) into their own subpipelines, potentially leading to higher accuracy.</p> <p>Example</p> <p>A map operation attempting to perform multiple tasks can be decomposed into separate map operations, ultimately improving overall accuracy. For example, consider a map operation on student survey responses that tries to:</p> <ol> <li>Extract actionable suggestions for course improvement</li> <li>Identify potential interdisciplinary connections</li> </ol> <p>This could be optimized into two separate map operations:</p> <ul> <li>Suggestion Extraction:   Focus solely on identifying concrete, actionable suggestions for improving the course.</li> </ul> <pre><code>prompt: |\n  From this student survey response, extract any specific, actionable suggestions\n  for improving the course. If no suggestions are present, output 'No suggestions found.':\n  '{{ input.response }}'\n</code></pre> <ul> <li>Interdisciplinary Connection Analysis:   Analyze the response for mentions of concepts or ideas that could connect to other disciplines or courses.</li> </ul> <pre><code>prompt: |\n  Identify any concepts or ideas in this student survey response that could have\n  interdisciplinary connections. For each connection, specify the related discipline or course:\n  '{{ input.response }}'\n</code></pre> <p>By breaking these tasks into separate operations, each LLM call can focus on a specific aspect of the analysis. This specialization might lead to more accurate results, depending on the LLM, data, and nature of the task!</p>"},{"location":"concepts/optimization/#how-it-works","title":"How It Works","text":"<p>The DocETL optimizer operates using the following mechanism:</p> <ol> <li> <p>Generation and Evaluation Agents: These agents generate different plans for the pipeline according to predefined rewrite rules. Evaluation agents then compare plans and outputs to determine the best approach.</p> </li> <li> <p>Operator Rewriting: The optimizer looks through operators in your pipeline where you've set optimize: true, and attempts to rewrite them using predefined rules.</p> </li> <li> <p>Output: After optimization, DocETL outputs a new YAML file representing the optimized pipeline.</p> </li> </ol>"},{"location":"concepts/optimization/#using-the-optimizer","title":"Using the Optimizer","text":"<p>DocETL provides two optimizer options:</p>"},{"location":"concepts/optimization/#moar-optimizer-recommended","title":"MOAR Optimizer (Recommended)","text":"<p>The MOAR optimizer uses Monte Carlo Tree Search to find Pareto-optimal solutions balancing accuracy and cost:</p> <pre><code>docetl build your_pipeline.yaml --optimizer moar\n</code></pre> <p>See the MOAR Optimizer Guide for detailed instructions.</p>"},{"location":"concepts/optimization/#v1-optimizer-deprecated","title":"V1 Optimizer (Deprecated)","text":"<p>Deprecated</p> <p>The V1 optimizer is deprecated and no longer recommended. Use MOAR instead.</p> <p>The V1 optimizer uses a greedy approach with validation. It's still available for backward compatibility:</p> <pre><code>docetl build your_pipeline.yaml --optimizer v1\n</code></pre> <p>Not Recommended</p> <p>The V1 optimizer should not be used for new projects. Use MOAR instead.</p>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>Pipelines in DocETL are the core structures that define the flow of data processing. They orchestrate the application of operators to datasets, creating a seamless workflow for complex chunk processing tasks.</p>"},{"location":"concepts/pipelines/#components-of-a-pipeline","title":"Components of a Pipeline","text":"<p>A pipeline in DocETL consists of five main components:</p> <ol> <li>Default Model: The language model to use for the pipeline.</li> <li>System Prompts: A description of your dataset and the \"persona\" you'd like the LLM to adopt when analyzing your data.</li> <li>Datasets: The input data sources for your pipeline.</li> <li>Operators: The processing steps that transform your data.</li> <li>Pipeline Specification: The sequence of steps and the output configuration.</li> </ol>"},{"location":"concepts/pipelines/#default-model","title":"Default Model","text":"<p>You can set the default model for a pipeline in the YAML configuration file. If no model is specified at the operation level, the default model will be used.</p> <p>You can also tell DocETL to skip the dataset-level cache for the entire pipeline by enabling <code>bypass_cache</code>. When set to <code>true</code>, DocETL will neither read from nor write to its cache for any operation in that pipeline\u2014which is helpful when you want to force fresh executions during development or debugging.</p> <pre><code>default_model: gpt-4o-mini\nbypass_cache: true  # optional \u2013 defaults to false\n</code></pre> <p><code>bypass_cache</code> can still be overridden at the operator level if required.</p> <p>You can also specify default API base URLs for language models and embeddings if you're hosting your own models with an OpenAI-compatible API:</p> <p>Note</p> <p>If you're hosting your own models with an OpenAI-compatible API, you can specify the base URLs:</p> <pre><code>default_lm_api_base: https://your-custom-llm-endpoint.com/v1\ndefault_embedding_api_base: https://your-custom-embedding-endpoint.com/v1\n</code></pre> <p>This is particularly useful when working with self-hosted models or services like Ollama, LM Studio, or other API-compatible LLM servers.</p>"},{"location":"concepts/pipelines/#system-prompts","title":"System Prompts","text":"<p>System prompts provide context to the language model about the data it's processing and the role it should adopt. This helps guide the model's responses to be more relevant and domain-appropriate. There are two key components to system prompts in DocETL:</p> <ol> <li>Dataset Description: A concise explanation of what kind of data the model will be processing.</li> <li>Persona: The role or perspective the model should adopt when analyzing the data.</li> </ol> <p>Here's an example of how to define system prompts in your pipeline configuration:</p> <pre><code>system_prompt:\n  dataset_description: a collection of transcripts of doctor visits\n  persona: a medical practitioner analyzing patient symptoms and reactions to medications\n</code></pre>"},{"location":"concepts/pipelines/#datasets","title":"Datasets","text":"<p>Datasets define the input data for your pipeline. They are collections of items/chunks, where each item/chunk is an object in a JSON list (or row in a CSV file). Datasets are typically specified in the YAML configuration file, indicating the type and path of the data source. For example:</p> <pre><code>datasets:\n  user_logs:\n    type: file\n    path: \"user_logs.json\"\n</code></pre>"},{"location":"concepts/pipelines/#dynamic-data-loading","title":"Dynamic Data Loading","text":"<p>DocETL supports dynamic data loading, allowing you to process various file types by specifying a key that points to a path or using a custom parsing function. This feature is particularly useful for handling diverse data sources, such as audio files, PDFs, or any other non-standard format.</p> <p>To implement dynamic data loading, you can use parsing tools in your dataset configuration. Here's an example:</p> <pre><code>datasets:\n  audio_transcripts:\n    type: file\n    source: local\n    path: \"audio_files/audio_paths.json\"\n    parsing_tools:\n      - input_key: audio_path\n        function: whisper_speech_to_text\n        output_key: transcript\n</code></pre> <p>In this example, the dataset configuration specifies a JSON file (audio_paths.json) that contains paths to audio files. The parsing_tools section defines how to process these files:</p> <ul> <li><code>input_key</code>: Specifies which key in the JSON contains the path to the audio file. In this example, each object in the dataset should have a \"audio_path\" key, that represents a path to an audio file or mp3.</li> <li><code>function</code>: Names the parsing function to use (in this case, the built-in whisper_speech_to_text function for audio transcription).</li> <li><code>output_key</code>: Defines the key where the processed data (transcript) will be stored. You can access this in the pipeline in any prompts with the <code>{{ input.transcipt }}</code> syntax.</li> </ul> <p>This approach allows DocETL to dynamically load and process various file types, extending its capabilities beyond standard JSON or CSV inputs. You can use built-in parsing tools or define custom ones to handle specific file formats or data processing needs. See the Custom Parsing documentation for more details.</p> <p>Note</p> <p>Currently, DocETL only supports JSON files or CSV files as input datasets. If you're interested in support for other data types or cloud-based datasets, please reach out to us or join our open-source community and contribute! We welcome new ideas and contributions to expand the capabilities of DocETL.</p>"},{"location":"concepts/pipelines/#dataset-description-and-persona","title":"Dataset Description and Persona","text":"<p>You can define a description of your dataset and persona you want the LLM to adopt when executing operations on your dataset. This is useful for providing context to the LLM and for optimizing the operations.</p> <pre><code>system_prompt: # This is optional, but recommended for better performance. It is applied to all operations in the pipeline.\n  dataset_description: a collection of transcripts of doctor visits\n  persona: a medical practitioner analyzing patient symptoms and reactions to medications\n</code></pre>"},{"location":"concepts/pipelines/#operators","title":"Operators","text":"<p>Operators are the building blocks of your pipeline, defining the transformations and analyses to be performed on your data. They are detailed in the Operators documentation. Operators can include map, reduce, filter, and other types of operations.</p>"},{"location":"concepts/pipelines/#pipeline-specification","title":"Pipeline Specification","text":"<p>The pipeline specification outlines the sequence of steps to be executed and the final output configuration. It typically includes:</p> <ul> <li>Steps: The sequence of operations to be applied to the data.</li> <li>Output: The configuration for the final output of the pipeline.</li> </ul> <p>For example:</p> <pre><code>pipeline:\n  steps:\n    - name: analyze_user_logs\n      input: user_logs\n      operations:\n        - extract_insights\n        - unnest_insights\n        - summarize_by_country\n  output:\n    type: file\n    path: \"country_summaries.json\"\n    intermediate_dir: \"intermediate_data\" # Optional: If you want to store intermediate outputs in a directory\n</code></pre> <p>For a practical example of how these components come together, refer to the Tutorial, which demonstrates a complete pipeline for analyzing user behavior data.</p>"},{"location":"concepts/schemas/","title":"Schemas","text":"<p>In DocETL, schemas play an important role in defining the structure of output from LLM operations. Every LLM call in DocETL is associated with an output schema, which specifies the expected format and types of the output data.</p>"},{"location":"concepts/schemas/#overview","title":"Overview","text":"<ul> <li>Schemas define the structure and types of output data from LLM operations.</li> <li>They help ensure consistency and facilitate downstream processing.</li> <li>DocETL uses structured outputs or tool API to enforce these schemas.</li> </ul> <p>Schema Simplicity</p> <p>We've observed that the more complex the output schema is, the worse the quality of the output tends to be. Keep your schemas as simple as possible for better results.</p>"},{"location":"concepts/schemas/#defining-schemas","title":"Defining Schemas","text":"<p>Schemas are defined in the <code>output</code> section of an operator. They support various data types:</p> Type Aliases Description <code>string</code> <code>str</code>, <code>text</code>, <code>varchar</code> For text data <code>integer</code> <code>int</code> For whole numbers <code>number</code> <code>float</code>, <code>decimal</code> For decimal numbers <code>boolean</code> <code>bool</code> For true/false values <code>enum</code> - For a set of possible values <code>list</code> - For arrays or sequences of items (must specify element type) Objects - Using notation <code>{field: type}</code> <p>Filter Operation Schemas</p> <p>Filter operation schemas must have a boolean type output field. This is used to determine whether each item should be included or excluded based on the filter criteria.</p>"},{"location":"concepts/schemas/#examples","title":"Examples","text":""},{"location":"concepts/schemas/#simple-schema","title":"Simple Schema","text":"<pre><code>output:\n  schema:\n    summary: string\n    sentiment: string\n    include_item: boolean # For filter operations\n</code></pre>"},{"location":"concepts/schemas/#complex-schema","title":"Complex Schema","text":"<pre><code>output:\n  schema:\n    insights: \"list[{insight: string, confidence: number}]\"\n    metadata: \"{timestamp: string, source: string}\"\n</code></pre>"},{"location":"concepts/schemas/#lists-and-objects","title":"Lists and Objects","text":"<p>Lists in schemas must specify their element type:</p> <ul> <li><code>list[string]</code>: A list of strings</li> <li><code>list[int]</code>: A list of integers</li> <li><code>list[{name: string, age: integer}]</code>: A list of objects</li> </ul> <p>Objects are defined using curly braces and must have typed fields:</p> <ul> <li><code>{name: string, age: integer, is_active: boolean}</code></li> </ul> <p>Complex List Example</p> <pre><code>output:\n  schema:\n    users: \"list[{name: string, age: integer, hobbies: list[string]}]\"\n</code></pre> <p>Make sure that you put the type in quotation marks, if it references an object type (i.e., has curly braces)! Otherwise the yaml won't compile!</p>"},{"location":"concepts/schemas/#enum-types","title":"Enum Types","text":"<p>You can also specify enum types, which will be validated against a set of possible values. Suppose we have an operation to extract sentiments from a document, and we want to ensure that the sentiment is one of the three possible values. Our schema would look like this:</p> <pre><code>output:\n  schema:\n    sentiment: \"enum[positive, negative, neutral]\"\n</code></pre> <p>You can also specify a list of enum types (say, if we wanted to extract multiple sentiments from a document):</p> <pre><code>output:\n  schema:\n    possible_sentiments: \"list[enum[positive, negative, neutral]]\"\n</code></pre>"},{"location":"concepts/schemas/#how-we-enforce-schemas","title":"How We Enforce Schemas","text":"<p>DocETL uses structured outputs or tool API to enforce schema typing. This ensures that the LLM outputs adhere to the specified schema, making the results more consistent and easier to process in subsequent operations.</p> <p>DocETL supports two output modes that determine how the LLM generates structured outputs:</p>"},{"location":"concepts/schemas/#tools-mode-default","title":"Tools Mode (Default)","text":"<p>Uses the OpenAI tools/function calling API to enforce schema structure. This is the default mode and provides robust schema validation.</p> <pre><code>output:\n  schema:\n    summary: string\n    sentiment: string\n  mode: \"tools\"  # Optional - this is the default\n</code></pre>"},{"location":"concepts/schemas/#structured-output-mode","title":"Structured Output Mode","text":"<p>Uses LiteLLM's structured output feature with JSON schema validation. This mode can provide more reliable schema adherence for complex outputs.</p> <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, confidence: number}]\"\n  mode: \"structured_output\"\n</code></pre> <p>When to Use Structured Output Mode</p> <p>Consider using <code>structured_output</code> mode when:</p> <ul> <li>You have complex nested schemas with lists and objects</li> <li>You need more consistent schema adherence</li> <li>You're experiencing schema validation issues with tools mode</li> </ul> <p>The tools mode remains the default and works well for most use cases.</p>"},{"location":"concepts/schemas/#mode-configuration","title":"Mode Configuration","text":"<p>The output mode can be configured in the <code>output</code> section of any operation:</p> <pre><code>operations:\n  - name: analyze_text\n    type: map\n    prompt: \"Analyze the following text...\"\n    output:\n      schema:\n        topics: \"list[{topic: string, relevance: number}]\"\n      mode: \"structured_output\"  # or \"tools\"\n    model: gpt-4o-mini\n</code></pre> <p>If no mode is specified, DocETL defaults to <code>\"tools\"</code> mode for backward compatibility.</p>"},{"location":"concepts/schemas/#best-practices","title":"Best Practices","text":"<ol> <li>Keep output fields simple and use string types whenever possible.</li> <li>Only use structured fields (like lists and objects) when necessary for downstream analysis or reduce operations.</li> <li>If you need to reference structured fields in downstream operations, consider breaking complex structures into multiple simpler operations.</li> </ol> <p>Schema Optimization</p> <p>If you find your schema becoming too complex, consider breaking it down into multiple operations. This can improve both the quality of LLM outputs and the manageability of your pipeline.</p> <p>Breaking Down Complex Schemas</p> <p>Instead of: <pre><code>output:\n  schema:\n    summary: string\n    key_points: \"list[{point: string, sentiment: string}]\"\n</code></pre></p> <p>Consider: <pre><code>output:\n  schema:\n    summary: string\n    key_points: \"string\"\n</code></pre></p> <p>Where in the prompt you can say something like: <code>In your key points, please include the sentiment of each point.</code></p> <p>The only reason to use the complex schema is if you need to do an operation at the point level, like resolve them and reduce on them.</p> <p>By following these guidelines and best practices, you can create effective schemas that enhance the performance and reliability of your DocETL operations.</p>"},{"location":"examples/annotating-legal-documents/","title":"Annotating legal documents","text":"<p>TODO</p>"},{"location":"examples/characterizing-troll-behavior/","title":"Characterizing troll behavior","text":"<p>TODO</p>"},{"location":"examples/custom-parsing/","title":"Pointing to External Data and Custom Parsing","text":"<p>In DocETL, you have full control over your dataset JSONs. These JSONs typically contain objects with key-value pairs, where you can reference external files that you want to process in your pipeline. This referencing mechanism, which we call \"pointing\", allows DocETL to locate and process external files that require special handling before they can be used in your main pipeline.</p> <p>Why Use Custom Parsing?</p> <p>Consider these scenarios where custom parsing of referenced files is beneficial:</p> <ul> <li>Your dataset JSON references Excel spreadsheets containing sales data.</li> <li>You have entries pointing to scanned receipts in PDF format that need OCR processing.</li> <li>You want to extract text from Word documents or PowerPoint presentations by referencing their file locations.</li> </ul> <p>In these cases, custom parsing enables you to transform your raw external data into a format that DocETL can process effectively within your pipeline. The pointing mechanism allows DocETL to locate these external files and apply custom parsing seamlessly. (Pointing in DocETL refers to the practice of including references or paths to external files within your dataset JSON. Instead of embedding the entire content of these files, you simply \"point\" to their locations, allowing DocETL to access and process them as needed during the pipeline execution.)</p>"},{"location":"examples/custom-parsing/#dataset-json-example","title":"Dataset JSON Example","text":"<p>Let's look at a typical dataset JSON file that you might create:</p> <pre><code>[\n  { \"id\": 1, \"excel_path\": \"sales_data/january_sales.xlsx\" },\n  { \"id\": 2, \"excel_path\": \"sales_data/february_sales.xlsx\" }\n]\n</code></pre> <p>In this example, you've specified paths to Excel files. DocETL will use these paths to locate and process the external files. However, without custom parsing, DocETL wouldn't know how to handle the contents of these files. This is where parsing tools come in handy.</p>"},{"location":"examples/custom-parsing/#custom-parsing-in-action","title":"Custom Parsing in Action","text":""},{"location":"examples/custom-parsing/#1-configuration","title":"1. Configuration","text":"<p>To use custom parsing, you need to define parsing tools in your DocETL configuration file. Here's an example:</p> <pre><code>parsing_tools:\n  - name: top_products_report\n    function_code: |\n      def top_products_report(document: Dict) -&gt; List[Dict]:\n          import pandas as pd\n\n          # Read the Excel file\n          filename = document[\"excel_path\"]\n          df = pd.read_excel(filename)\n\n          # Calculate total sales\n          total_sales = df['Sales'].sum()\n\n          # Find top 500 products by sales\n          top_products = df.groupby('Product')['Sales'].sum().nlargest(500)\n\n          # Calculate month-over-month growth\n          df['Date'] = pd.to_datetime(df['Date'])\n          monthly_sales = df.groupby(df['Date'].dt.to_period('M'))['Sales'].sum()\n          mom_growth = monthly_sales.pct_change().fillna(0)\n\n          # Prepare the analysis report\n          report = [\n              f\"Total Sales: ${total_sales:,.2f}\",\n              \"\\nTop 500 Products by Sales:\",\n              top_products.to_string(),\n              \"\\nMonth-over-Month Growth:\",\n              mom_growth.to_string()\n          ]\n\n          # Return a list of dicts representing the output\n          # The input document will be merged into each output doc,\n          # so we can access all original fields from the input doc.\n          return [{\"sales_analysis\": \"\\n\".join(report)}]\n\ndatasets:\n  sales_reports:\n    type: file\n    source: local\n    path: \"sales_data/sales_paths.json\"\n    parsing:\n      - function: top_products_report\n\n  receipts:\n    type: file\n    source: local\n    path: \"receipts/receipt_paths.json\"\n    parsing:\n      - input_key: pdf_path\n        function: paddleocr_pdf_to_string\n        output_key: receipt_text\n        ocr_enabled: true\n        lang: \"en\"\n</code></pre> <p>In this configuration:</p> <ul> <li>We define a custom <code>top_products_report</code> function for Excel files.</li> <li>We use the built-in <code>paddleocr_pdf_to_string</code> parser for PDF files.</li> <li>We apply these parsing tools to the external files referenced in the respective datasets.</li> </ul>"},{"location":"examples/custom-parsing/#2-pipeline-integration","title":"2. Pipeline Integration","text":"<p>Once you've defined your parsing tools and datasets, you can use the processed data in your pipeline:</p> <pre><code>pipeline:\n  steps:\n    - name: process_sales\n      input: sales_reports\n      operations:\n        - summarize_sales\n    - name: process_receipts\n      input: receipts\n      operations:\n        - extract_receipt_info\n</code></pre> <p>This pipeline will use the parsed data from both Excel files and PDFs for further processing.</p>"},{"location":"examples/custom-parsing/#how-data-gets-parsed-and-formatted","title":"How Data Gets Parsed and Formatted","text":"<p>When you run your DocETL pipeline, the parsing tools you've specified in your configuration file are applied to the external files referenced in your dataset JSONs. Here's what happens:</p> <ol> <li>DocETL reads your dataset JSON file.</li> <li>For each entry in the dataset, it looks at the parsing configuration you've specified.</li> <li>It applies the appropriate parsing function to the file path provided in the dataset JSON.</li> <li>The parsing function processes the file and returns the data in a format DocETL can work with (typically a list of strings).</li> </ol> <p>Let's look at how this works for our earlier examples:</p>"},{"location":"examples/custom-parsing/#excel-files-using-top_products_report","title":"Excel Files (using top_products_report)","text":"<p>For an Excel file like \"sales_data/january_sales.xlsx\":</p> <ul> <li>The top_products_report function reads the Excel file.</li> <li>It processes the sales data and generates a report of top-selling products.</li> <li>The output might look like this:</li> </ul> <pre><code>Top Products Report - January 2023\n\n1. Widget A - 1500 units sold\n2. Gadget B - 1200 units sold\n3. Gizmo C - 950 units sold\n4. Doohickey D - 800 units sold\n5. Thingamajig E - 650 units sold\n   ...\n\nTotal Revenue: $245,000\nBest Selling Category: Electronics\n</code></pre>"},{"location":"examples/custom-parsing/#pdf-files-using-paddleocr_pdf_to_string","title":"PDF Files (using paddleocr_pdf_to_string)","text":"<p>For a PDF file like \"receipts/receipt001.pdf\":</p> <ul> <li>The paddleocr_pdf_to_string function reads the PDF file.</li> <li>It uses PaddleOCR to perform optical character recognition on each page.</li> <li>The function combines the extracted text from all pages into a single string.   The output might look like this:</li> </ul> <pre><code>RECEIPT\nStore: Example Store\nDate: 2023-05-15\nItems:\n\n1. Product A - $10.99\n2. Product B - $15.50\n3. Product C - $7.25\n4. Product D - $22.00\n   Subtotal: $55.74\n   Tax (8%): $4.46\n   Total: $60.20\n\nPayment Method: Credit Card\nCard Number: \\***\\* \\*\\*** \\*\\*\\*\\* 1234\n\nThank you for your purchase!\n</code></pre> <p>This parsed and formatted data is then passed to the respective operations in your pipeline for further processing.</p>"},{"location":"examples/custom-parsing/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Once you've set up your pipeline configuration file with the appropriate parsing tools and dataset definitions, you can run your DocETL pipeline. Here's how:</p> <ol> <li>Ensure you have DocETL installed in your environment.</li> <li>Open a terminal or command prompt.</li> <li>Navigate to the directory containing your pipeline configuration file.</li> <li>Run the following command:</li> </ol> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>Replace <code>pipeline.yaml</code> with the name of your pipeline file if it's different.</p> <p>When you run this command:</p> <ol> <li>DocETL reads your pipeline file.</li> <li>It processes each dataset using the specified parsing tools.</li> <li>The pipeline steps are executed in the order you defined.</li> <li>Any operations you've specified (like <code>summarize_sales</code> or <code>extract_receipt_info</code>) are applied to the parsed data.</li> <li>The results are saved according to your output configuration.</li> </ol>"},{"location":"examples/custom-parsing/#built-in-parsing-tools","title":"Built-in Parsing Tools","text":"<p>DocETL provides several built-in parsing tools to handle common file formats and data processing tasks. These tools can be used directly in your configuration by specifying their names in the <code>function</code> field of your parsing tools configuration. Here's an overview of the available built-in parsing tools:</p> <p>Convert an Excel file to a string representation or a list of string representations.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the xlsx file.</p> required <code>orientation</code> <code>str</code> <p>Either \"row\" or \"col\" for cell arrangement.</p> <code>'col'</code> <code>col_order</code> <code>list[str] | None</code> <p>List of column names to specify the order.</p> <code>None</code> <code>doc_per_sheet</code> <code>bool</code> <p>If True, return a list of strings, one per sheet.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: String representation(s) of the Excel file content.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef xlsx_to_string(\n    filename: str,\n    orientation: str = \"col\",\n    col_order: list[str] | None = None,\n    doc_per_sheet: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Convert an Excel file to a string representation or a list of string representations.\n\n    Args:\n        filename (str): Path to the xlsx file.\n        orientation (str): Either \"row\" or \"col\" for cell arrangement.\n        col_order (list[str] | None): List of column names to specify the order.\n        doc_per_sheet (bool): If True, return a list of strings, one per sheet.\n\n    Returns:\n        list[str]: String representation(s) of the Excel file content.\n    \"\"\"\n    import openpyxl\n\n    wb = openpyxl.load_workbook(filename)\n\n    def process_sheet(sheet):\n        if col_order:\n            headers = [\n                col for col in col_order if col in sheet.iter_cols(1, sheet.max_column)\n            ]\n        else:\n            headers = [cell.value for cell in sheet[1]]\n\n        result = []\n        if orientation == \"col\":\n            for col_idx, header in enumerate(headers, start=1):\n                column = sheet.cell(1, col_idx).column_letter\n                column_values = [cell.value for cell in sheet[column][1:]]\n                result.append(f\"{header}: \" + \"\\n\".join(map(str, column_values)))\n                result.append(\"\")  # Empty line between columns\n        else:  # row\n            for row in sheet.iter_rows(min_row=2, values_only=True):\n                row_dict = {\n                    header: value for header, value in zip(headers, row) if header\n                }\n                result.append(\n                    \" | \".join(\n                        [f\"{header}: {value}\" for header, value in row_dict.items()]\n                    )\n                )\n\n        return \"\\n\".join(result)\n\n    if doc_per_sheet:\n        return [process_sheet(sheet) for sheet in wb.worksheets]\n    else:\n        return [process_sheet(wb.active)]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Read the content of a text file and return it as a list of strings (only one element).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the txt or md file.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Content of the file as a list of strings.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef txt_to_string(filename: str) -&gt; list[str]:\n    \"\"\"\n    Read the content of a text file and return it as a list of strings (only one element).\n\n    Args:\n        filename (str): Path to the txt or md file.\n\n    Returns:\n        list[str]: Content of the file as a list of strings.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        return [file.read()]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Extract text from a Word document.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the docx file.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Extracted text from the document.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef docx_to_string(filename: str) -&gt; list[str]:\n    \"\"\"\n    Extract text from a Word document.\n\n    Args:\n        filename (str): Path to the docx file.\n\n    Returns:\n        list[str]: Extracted text from the document.\n    \"\"\"\n    from docx import Document\n\n    doc = Document(filename)\n    return [\"\\n\".join([paragraph.text for paragraph in doc.paragraphs])]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Transcribe speech from an audio file to text using Whisper model via litellm. If the file is larger than 25 MB, it's split into 10-minute chunks with 30-second overlap.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the mp3 or mp4 file.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Transcribed text.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef whisper_speech_to_text(filename: str) -&gt; list[str]:\n    \"\"\"\n    Transcribe speech from an audio file to text using Whisper model via litellm.\n    If the file is larger than 25 MB, it's split into 10-minute chunks with 30-second overlap.\n\n    Args:\n        filename (str): Path to the mp3 or mp4 file.\n\n    Returns:\n        list[str]: Transcribed text.\n    \"\"\"\n\n    from litellm import transcription\n\n    file_size = os.path.getsize(filename)\n    if file_size &gt; 25 * 1024 * 1024:  # 25 MB in bytes\n        from pydub import AudioSegment\n\n        audio = AudioSegment.from_file(filename)\n        chunk_length = 10 * 60 * 1000  # 10 minutes in milliseconds\n        overlap = 30 * 1000  # 30 seconds in milliseconds\n\n        chunks = []\n        for i in range(0, len(audio), chunk_length - overlap):\n            chunk = audio[i : i + chunk_length]\n            chunks.append(chunk)\n\n        transcriptions = []\n\n        for i, chunk in enumerate(chunks):\n            buffer = io.BytesIO()\n            buffer.name = f\"temp_chunk_{i}_{os.path.basename(filename)}\"\n            chunk.export(buffer, format=\"mp3\")\n            buffer.seek(0)  # Reset buffer position to the beginning\n\n            response = transcription(model=\"whisper-1\", file=buffer)\n            transcriptions.append(response.text)\n\n        return transcriptions\n    else:\n        with open(filename, \"rb\") as audio_file:\n            response = transcription(model=\"whisper-1\", file=audio_file)\n\n        return [response.text]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Extract text from a PowerPoint presentation.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the pptx file.</p> required <code>doc_per_slide</code> <code>bool</code> <p>If True, return each slide as a separate document. If False, return the entire presentation as one document.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Extracted text from the presentation. If doc_per_slide is True, each string in the list represents a single slide. Otherwise, the list contains a single string with all slides' content.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef pptx_to_string(filename: str, doc_per_slide: bool = False) -&gt; list[str]:\n    \"\"\"\n    Extract text from a PowerPoint presentation.\n\n    Args:\n        filename (str): Path to the pptx file.\n        doc_per_slide (bool): If True, return each slide as a separate\n            document. If False, return the entire presentation as one document.\n\n    Returns:\n        list[str]: Extracted text from the presentation. If doc_per_slide\n            is True, each string in the list represents a single slide.\n            Otherwise, the list contains a single string with all slides'\n            content.\n    \"\"\"\n    from pptx import Presentation\n\n    prs = Presentation(filename)\n    result = []\n\n    for slide in prs.slides:\n        slide_content = []\n        for shape in slide.shapes:\n            if hasattr(shape, \"text\"):\n                slide_content.append(shape.text)\n\n        if doc_per_slide:\n            result.append(\"\\n\".join(slide_content))\n        else:\n            result.extend(slide_content)\n\n    if not doc_per_slide:\n        result = [\"\\n\".join(result)]\n\n    return result\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Note to developers: We used this documentation as a reference.</p> <p>This function uses Azure Document Intelligence to extract text from documents. To use this function, you need to set up an Azure Document Intelligence resource:</p> <ol> <li>Create an Azure account if you don't have one</li> <li>Set up a Document Intelligence resource in the Azure portal</li> <li>Once created, find the resource's endpoint and key in the Azure portal</li> <li>Set these as environment variables:</li> <li>DOCUMENTINTELLIGENCE_API_KEY: Your Azure Document Intelligence API key</li> <li>DOCUMENTINTELLIGENCE_ENDPOINT: Your Azure Document Intelligence endpoint URL</li> </ol> <p>The function will use these credentials to authenticate with the Azure service. If the environment variables are not set, the function will raise a ValueError.</p> <p>The Azure Document Intelligence client is then initialized with these credentials. It sends the document (either as a file or URL) to Azure for analysis. The service processes the document and returns structured information about its content.</p> <p>This function then extracts the text content from the returned data, applying any specified formatting options (like including line numbers or font styles). The extracted text is returned as a list of strings, with each string representing either a page (if doc_per_page is True) or the entire document.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file to be analyzed or URL of the document if use_url is True.</p> required <code>use_url</code> <code>bool</code> <p>If True, treat filename as a URL. Defaults to False.</p> <code>False</code> <code>include_line_numbers</code> <code>bool</code> <p>If True, include line numbers in the output. Defaults to False.</p> <code>False</code> <code>include_handwritten</code> <code>bool</code> <p>If True, include handwritten text in the output. Defaults to False.</p> <code>False</code> <code>include_font_styles</code> <code>bool</code> <p>If True, include font style information in the output. Defaults to False.</p> <code>False</code> <code>include_selection_marks</code> <code>bool</code> <p>If True, include selection marks in the output. Defaults to False.</p> <code>False</code> <code>doc_per_page</code> <code>bool</code> <p>If True, return each page as a separate document. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Extracted text from the document. If doc_per_page is True, each string in the list represents        a single page. Otherwise, the list contains a single string with all pages' content.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DOCUMENTINTELLIGENCE_API_KEY or DOCUMENTINTELLIGENCE_ENDPOINT environment variables are not set.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef azure_di_read(\n    filename: str,\n    use_url: bool = False,\n    include_line_numbers: bool = False,\n    include_handwritten: bool = False,\n    include_font_styles: bool = False,\n    include_selection_marks: bool = False,\n    doc_per_page: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    &gt; Note to developers: We used [this documentation](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/use-sdk-rest-api?view=doc-intel-4.0.0&amp;tabs=windows&amp;pivots=programming-language-python) as a reference.\n\n    This function uses Azure Document Intelligence to extract text from documents.\n    To use this function, you need to set up an Azure Document Intelligence resource:\n\n    1. [Create an Azure account](https://azure.microsoft.com/) if you don't have one\n    2. Set up a Document Intelligence resource in the [Azure portal](https://portal.azure.com/#create/Microsoft.CognitiveServicesFormRecognizer)\n    3. Once created, find the resource's endpoint and key in the Azure portal\n    4. Set these as environment variables:\n       - DOCUMENTINTELLIGENCE_API_KEY: Your Azure Document Intelligence API key\n       - DOCUMENTINTELLIGENCE_ENDPOINT: Your Azure Document Intelligence endpoint URL\n\n    The function will use these credentials to authenticate with the Azure service.\n    If the environment variables are not set, the function will raise a ValueError.\n\n    The Azure Document Intelligence client is then initialized with these credentials.\n    It sends the document (either as a file or URL) to Azure for analysis.\n    The service processes the document and returns structured information about its content.\n\n    This function then extracts the text content from the returned data,\n    applying any specified formatting options (like including line numbers or font styles).\n    The extracted text is returned as a list of strings, with each string\n    representing either a page (if doc_per_page is True) or the entire document.\n\n    Args:\n        filename (str): Path to the file to be analyzed or URL of the document if use_url is True.\n        use_url (bool, optional): If True, treat filename as a URL. Defaults to False.\n        include_line_numbers (bool, optional): If True, include line numbers in the output. Defaults to False.\n        include_handwritten (bool, optional): If True, include handwritten text in the output. Defaults to False.\n        include_font_styles (bool, optional): If True, include font style information in the output. Defaults to False.\n        include_selection_marks (bool, optional): If True, include selection marks in the output. Defaults to False.\n        doc_per_page (bool, optional): If True, return each page as a separate document. Defaults to False.\n\n    Returns:\n        list[str]: Extracted text from the document. If doc_per_page is True, each string in the list represents\n                   a single page. Otherwise, the list contains a single string with all pages' content.\n\n    Raises:\n        ValueError: If DOCUMENTINTELLIGENCE_API_KEY or DOCUMENTINTELLIGENCE_ENDPOINT environment variables are not set.\n    \"\"\"\n\n    from azure.ai.documentintelligence import DocumentIntelligenceClient\n    from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n    from azure.core.credentials import AzureKeyCredential\n\n    key = os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")\n    endpoint = os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\")\n\n    if key is None:\n        raise ValueError(\"DOCUMENTINTELLIGENCE_API_KEY environment variable is not set\")\n    if endpoint is None:\n        raise ValueError(\n            \"DOCUMENTINTELLIGENCE_ENDPOINT environment variable is not set\"\n        )\n\n    document_analysis_client = DocumentIntelligenceClient(\n        endpoint=endpoint, credential=AzureKeyCredential(key)\n    )\n\n    if use_url:\n        poller = document_analysis_client.begin_analyze_document(\n            \"prebuilt-read\", AnalyzeDocumentRequest(url_source=filename)\n        )\n    else:\n        with open(filename, \"rb\") as f:\n            poller = document_analysis_client.begin_analyze_document(\"prebuilt-read\", f)\n\n    result = poller.result()\n\n    style_content = []\n    content = []\n\n    if result.styles:\n        for style in result.styles:\n            if style.is_handwritten and include_handwritten:\n                handwritten_text = \",\".join(\n                    [\n                        result.content[span.offset : span.offset + span.length]\n                        for span in style.spans\n                    ]\n                )\n                style_content.append(f\"Handwritten content: {handwritten_text}\")\n\n            if style.font_style and include_font_styles:\n                styled_text = \",\".join(\n                    [\n                        result.content[span.offset : span.offset + span.length]\n                        for span in style.spans\n                    ]\n                )\n                style_content.append(f\"'{style.font_style}' font style: {styled_text}\")\n\n    for page in result.pages:\n        page_content = []\n\n        if page.lines:\n            for line_idx, line in enumerate(page.lines):\n                if include_line_numbers:\n                    page_content.append(f\" Line #{line_idx}: {line.content}\")\n                else:\n                    page_content.append(f\"{line.content}\")\n\n        if page.selection_marks and include_selection_marks:\n            # TODO: figure this out\n            for selection_mark_idx, selection_mark in enumerate(page.selection_marks):\n                page_content.append(\n                    f\"Selection mark #{selection_mark_idx}: State is '{selection_mark.state}' within bounding polygon \"\n                    f\"'{selection_mark.polygon}' and has a confidence of {selection_mark.confidence}\"\n                )\n\n        content.append(\"\\n\".join(page_content))\n\n    if doc_per_page:\n        return style_content + content\n    else:\n        return [\n            \"\\n\\n\".join(\n                [\n                    \"\\n\".join(style_content),\n                    \"\\n\\n\".join(\n                        f\"Page {i+1}:\\n{page_content}\"\n                        for i, page_content in enumerate(content)\n                    ),\n                ]\n            )\n        ]\n</code></pre> <p>options:     heading_level: 3     show_root_heading: true</p> <p>Extract text and image information from a PDF file using PaddleOCR for image-based PDFs.</p> <p>Note: this is very slow!!</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input PDF file.</p> required <code>doc_per_page</code> <code>bool</code> <p>If True, return a list of strings, one per page. If False, return a single string.</p> <code>False</code> <code>ocr_enabled</code> <code>bool</code> <p>Whether to enable OCR for image-based PDFs.</p> <code>True</code> <code>lang</code> <code>str</code> <p>Language of the PDF file.</p> <code>'en'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Extracted content as a list of formatted strings.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef paddleocr_pdf_to_string(\n    input_path: str,\n    doc_per_page: bool = False,\n    ocr_enabled: bool = True,\n    lang: str = \"en\",\n) -&gt; list[str]:\n    \"\"\"\n    Extract text and image information from a PDF file using PaddleOCR for image-based PDFs.\n\n    **Note: this is very slow!!**\n\n    Args:\n        input_path (str): Path to the input PDF file.\n        doc_per_page (bool): If True, return a list of strings, one per page.\n            If False, return a single string.\n        ocr_enabled (bool): Whether to enable OCR for image-based PDFs.\n        lang (str): Language of the PDF file.\n\n    Returns:\n        list[str]: Extracted content as a list of formatted strings.\n    \"\"\"\n    import fitz\n    import numpy as np\n    from paddleocr import PaddleOCR\n\n    ocr = PaddleOCR(use_angle_cls=True, lang=lang)\n\n    pdf_content = []\n\n    with fitz.open(input_path) as pdf:\n        for page_num in range(len(pdf)):\n            page = pdf[page_num]\n            text = page.get_text()\n            images = []\n\n            # Extract image information\n            for img_index, img in enumerate(page.get_images(full=True)):\n                rect = page.get_image_bbox(img)\n                images.append(f\"Image {img_index + 1}: bbox {rect}\")\n\n            page_content = f\"Page {page_num + 1}:\\n\"\n            page_content += f\"Text:\\n{text}\\n\"\n            page_content += \"Images:\\n\" + \"\\n\".join(images) + \"\\n\"\n\n            if not text and ocr_enabled:\n                mat = fitz.Matrix(2, 2)\n                pix = page.get_pixmap(matrix=mat)\n                img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(\n                    pix.height, pix.width, 3\n                )\n\n                ocr_result = ocr.ocr(img, cls=True)\n                page_content += \"OCR Results:\\n\"\n                for line in ocr_result[0]:\n                    bbox, (text, _) = line\n                    page_content += f\"{bbox}, {text}\\n\"\n\n            pdf_content.append(page_content)\n\n    if not doc_per_page:\n        return [\"\\n\\n\".join(pdf_content)]\n\n    return pdf_content\n</code></pre> <p>options:     heading_level: 3     show_root_heading: true</p>"},{"location":"examples/custom-parsing/#using-function-arguments-with-parsing-tools","title":"Using Function Arguments with Parsing Tools","text":"<p>When using parsing tools in your DocETL configuration, you can pass additional arguments to the parsing functions.</p> <p>For example, when using the xlsx_to_string parsing tool, you can specify options like the orientation of the data, the order of columns, or whether to process each sheet separately. Here's an example of how to use such kwargs in your configuration:</p> <pre><code>datasets:\n  my_sales:\n    type: file\n    source: local\n    path: \"sales_data/sales_paths.json\"\n    parsing_tools:\n      - name: excel_parser\n        function: xlsx_to_string\n        orientation: row\n        col_order: [\"Date\", \"Product\", \"Quantity\", \"Price\"]\n        doc_per_sheet: true\n</code></pre>"},{"location":"examples/custom-parsing/#contributing-built-in-parsing-tools","title":"Contributing Built-in Parsing Tools","text":"<p>While DocETL provides several built-in parsing tools, the community can always benefit from additional utilities. If you've developed a parsing tool that you think could be useful for others, consider contributing it to the DocETL repository. Here's how you can add new built-in parsing utilities:</p> <ol> <li>Fork the DocETL repository on GitHub.</li> <li>Clone your forked repository to your local machine.</li> <li>Navigate to the <code>docetl/parsing_tools.py</code> file.</li> <li>Add your new parsing function to this file. The function should also be added to the <code>PARSING_TOOLS</code> dictionary.</li> <li>Update the documentation in the function's docstring.</li> <li>Create a pull request to merge your changes into the main DocETL repository.</li> </ol> <p>Guidelines for Contributing Parsing Tools</p> <p>When contributing a new parsing tool, make sure it follows these guidelines:</p> <ul> <li>The function should have a clear, descriptive name.</li> <li>Include comprehensive docstrings explaining the function's purpose, parameters, and return value. The return value should be a list of strings.</li> <li>Handle potential errors gracefully and provide informative error messages.</li> <li>If your parser requires additional dependencies, make sure to mention them in the pull request.</li> </ul>"},{"location":"examples/custom-parsing/#creating-custom-parsing-tools","title":"Creating Custom Parsing Tools","text":"<p>If the built-in tools don't meet your needs, you can create your own custom parsing tools. Here's how:</p> <ol> <li>Define your parsing function in the <code>parsing_tools</code> section of your configuration.</li> <li>Ensure your function takes a item (dict) as input and returns a list of items (dicts).</li> <li>Use your custom parser in the <code>parsing</code> section of your dataset configuration.</li> </ol> <p>For example:</p> <pre><code>parsing_tools:\n  - name: my_custom_parser\n    function_code: |\n      def my_custom_parser(item: Dict) -&gt; List[Dict]:\n          # Your custom parsing logic here\n          return [processed_data]\n\ndatasets:\n  my_dataset:\n    type: file\n    source: local\n    path: \"data/paths.json\"\n    parsing:\n      - function: my_custom_parser\n</code></pre>"},{"location":"examples/mining-product-reviews/","title":"Mining Product Reviews: Identifying Polarizing Themes in Video Games","text":"<p>This tutorial demonstrates how to use DocETL to analyze product reviews, specifically focusing on identifying polarizing themes across multiple video games. We'll walk through the process of building a pipeline that extracts insights from Steam game reviews, resolves common themes, and generates comprehensive reports.</p> <p>Optimization Cost</p> <p>Optimizing this pipeline can be computationally expensive and time-consuming, especially for large datasets. The process involves running multiple LLM calls and comparisons between different plans, which can result in significant resource usage and potential costs.</p> <p>For reference, optimizing a pipeline of this complexity could cost up to $70 in OpenAI credits, depending on the size of your dataset and the specific models used. Always monitor your usage and set appropriate limits to avoid unexpected charges.</p>"},{"location":"examples/mining-product-reviews/#task-overview","title":"Task Overview","text":"<p>Our goal is to create a pipeline that will:</p> <ol> <li>Identify polarizing themes within individual game reviews</li> <li>Resolve similar themes across different games</li> <li>Generate reports of polarizing themes common across games, supported by quotes from different game reviews</li> </ol> <p>We'll be using a subset of the STEAM review dataset. We've created a subset that contains reviews for 500 of the most popular games, with approximately 400 reviews per game, balanced between positive and negative ratings. For each game, we concatenate all reviews into a single text for analysis---so we'll have 500 input items/reviews, each representing a game. You can get the dataset sample here.</p>"},{"location":"examples/mining-product-reviews/#pipeline-structure","title":"Pipeline Structure","text":"<p>Let's examine the pipeline structure and its operations:</p> <pre><code>pipeline:\n  steps:\n    - name: game_analysis\n      input: steam_reviews\n      operations:\n        - identify_polarizing_themes\n        - unnest_polarizing_themes\n        - resolve_themes\n        - aggregate_common_themes\n\n  output:\n    type: file\n    path: \"output_polarizing_themes.json\"\n    intermediate_dir: \"intermediates\"\n</code></pre> Full Pipeline Configuration <pre><code>default_model: gpt-4o-mini\n\nsystem_prompt:\n  dataset_description: a collection of reviews for video games\n  persona: a marketing analyst analyzing player opinions and themes\n\ndatasets:\n  steam_reviews:\n    type: file\n    path: \"path/to/top_apps_steam_sample.json\"\n\noperations:\n  - name: identify_polarizing_themes\n    optimize: true\n    type: map\n    prompt: |\n      Analyze the following concatenated reviews for a video game and identify polarizing themes that divide player opinions. A polarizing theme is one that some players love while others strongly dislike.\n\n      Game: {{ input.app_name }}\n      Reviews: {{ input.concatenated_reviews }}\n\n      For each polarizing theme you identify:\n      1. Provide a summary of the theme\n      2. Explain why it's polarizing\n      3. Include supporting quotes from both positive and negative perspectives\n\n      Aim to identify ~10 polarizing themes, if present.\n\n    output:\n      schema:\n        polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n\n  - name: unnest_polarizing_themes\n    type: unnest\n    unnest_key: polarizing_themes\n    recursive: true\n    depth: 2\n\n  - name: resolve_themes\n    type: resolve\n    optimize: true\n    comparison_prompt: |\n      Are the themes \"{{ input1.theme }}\" and \"{{ input2.theme }}\" the same? Here is some context to help you decide:\n\n      Theme 1: {{ input1.theme }}\n      Summary 1: {{ input1.summary }}\n\n      Theme 2: {{ input2.theme }}\n      Summary 2: {{ input2.summary }}\n    resolution_prompt: |\n      Given the following themes, please come up with a theme that best captures the essence of all the themes:\n\n      {% for input in inputs %}\n      Theme {{ loop.index }}: {{ input.theme }}\n      {% if not loop.last %}\n      ---\n      {% endif %}\n      {% endfor %}\n\n      Based on these themes, provide a consolidated theme that captures the essence of all the above themes. Ensure that the consolidated theme is concise yet comprehensive.\n    output:\n      schema:\n        theme: str\n\n  - name: aggregate_common_themes\n    type: reduce\n    optimize: true\n    reduce_key: theme\n    prompt: |\n      You are given a theme and summary that appears across multiple video games, along with various apps and review quotes related to this theme. Your task is to consolidate this information into a comprehensive report.\n\n      For each input, you will receive:\n      - theme: A specific polarizing theme\n      - summary: A brief summary of the theme\n      - app_name: The name of the game\n      - positive_quotes: List of supporting quotes from positive perspectives\n      - negative_quotes: List of supporting quotes from negative perspectives\n\n      Create a report that includes:\n      1. The name of the common theme\n      2. A summary of the theme and why it's common across games\n      3. Representative quotes from different games, both positive and negative\n\n      Here's the information for the theme:\n      Theme: {{ inputs[0].theme }}\n      Summary: {{ inputs[0].summary }}\n\n      {% for app in inputs %}\n      Game: {{ app.app_name }}\n      Positive Quotes: {{ app.positive_quotes }}\n      Negative Quotes: {{ app.negative_quotes }}\n      {% if not loop.last %}\n      ----------------------------------------\n      {% endif %}\n      {% endfor %}\n\n    output:\n      schema:\n        theme_summary: str\n        representative_quotes: \"list[{game: str, quote: str, sentiment: str}]\"\n\npipeline:\n  steps:\n    - name: game_analysis\n      input: steam_reviews\n      operations:\n        - identify_polarizing_themes\n        - unnest_polarizing_themes\n        - resolve_themes\n        - aggregate_common_themes\n\n  output:\n    type: file\n    path: \"path/to/output_polarizing_themes.json\"\n    intermediate_dir: \"path/to/intermediates\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#pipeline-operations","title":"Pipeline Operations","text":""},{"location":"examples/mining-product-reviews/#1-identify-polarizing-themes","title":"1. Identify Polarizing Themes","text":"<p>This map operation processes each game's reviews to identify polarizing themes:</p> <pre><code>- name: identify_polarizing_themes\n  optimize: true\n  type: map\n  prompt: |\n    Analyze the following concatenated reviews for a video game and identify polarizing themes that divide player opinions. A polarizing theme is one that some players love while others strongly dislike.\n\n    Game: {{ input.app_name }}\n    Reviews: {{ input.concatenated_reviews }}\n\n    For each polarizing theme you identify:\n    1. Provide a summary of the theme\n    2. Explain why it's polarizing\n    3. Include supporting quotes from both positive and negative perspectives\n\n    Aim to identify ~10 polarizing themes, if present.\n\n  output:\n    schema:\n      polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#2-unnest-polarizing-themes","title":"2. Unnest Polarizing Themes","text":"<p>This operation flattens the list of themes extracted from each game:</p> <pre><code>- name: unnest_polarizing_themes\n  type: unnest\n  unnest_key: polarizing_themes\n  recursive: true\n  depth: 2\n</code></pre>"},{"location":"examples/mining-product-reviews/#3-resolve-themes","title":"3. Resolve Themes","text":"<p>This operation identifies and consolidates similar themes across different games:</p> <pre><code>- name: resolve_themes\n  type: resolve\n  optimize: true\n  comparison_prompt: |\n    Are the themes \"{{ input1.theme }}\" and \"{{ input2.theme }}\" the same? Here is some context to help you decide:\n\n    Theme 1: {{ input1.theme }}\n    Summary 1: {{ input1.summary }}\n\n    Theme 2: {{ input2.theme }}\n    Summary 2: {{ input2.summary }}\n  resolution_prompt: |\n    Given the following themes, please come up with a theme that best captures the essence of all the themes:\n\n    {% for input in inputs %}\n    Theme {{ loop.index }}: {{ input.theme }}\n    {% if not loop.last %}\n    ---\n    {% endif %}\n    {% endfor %}\n\n    Based on these themes, provide a consolidated theme that captures the essence of all the above themes. Ensure that the consolidated theme is concise yet comprehensive.\n  output:\n    schema:\n      theme: str\n</code></pre>"},{"location":"examples/mining-product-reviews/#4-aggregate-common-themes","title":"4. Aggregate Common Themes","text":"<p>This reduce operation generates a comprehensive report for each common theme:</p> <pre><code>- name: aggregate_common_themes\n  type: reduce\n  optimize: true\n  reduce_key: theme\n  prompt: |\n    You are given a theme and summary that appears across multiple video games, along with various apps and review quotes related to this theme. Your task is to consolidate this information into a comprehensive report.\n\n    For each input, you will receive:\n    - theme: A specific polarizing theme\n    - summary: A brief summary of the theme\n    - app_name: The name of the game\n    - positive_quotes: List of supporting quotes from positive perspectives\n    - negative_quotes: List of supporting quotes from negative perspectives\n\n    Create a report that includes:\n    1. The name of the common theme\n    2. A summary of the theme and why it's common across games\n    3. Representative quotes from different games, both positive and negative\n\n    Here's the information for the theme:\n    Theme: {{ inputs[0].theme }}\n    Summary: {{ inputs[0].summary }}\n\n    {% for app in inputs %}\n    Game: {{ app.app_name }}\n    Positive Quotes: {{ app.positive_quotes }}\n    Negative Quotes: {{ app.negative_quotes }}\n    {% if not loop.last %}\n    ----------------------------------------\n    {% endif %}\n    {% endfor %}\n\n  output:\n    schema:\n      theme_summary: str\n      representative_quotes: \"list[{game: str, quote: str, sentiment: str}]\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#optimizing-the-pipeline","title":"Optimizing the Pipeline","text":"<p>After writing the pipeline, we can use the DocETL <code>build</code> command to optimize it:</p> <pre><code>docetl build pipeline.yaml\n</code></pre> <p>This command, with <code>optimize: true</code> set for the map and resolve operations, provides us with:</p> <ol> <li> <p>A chunking-based plan for the map operation: This helps handle the large input sizes (up to 380,000 tokens) by breaking them into manageable chunks. The optimizer gives us a chunking plan of 87,776 tokens per chunk, with 10% of the previous chunk as peripheral context.</p> </li> <li> <p>Blocking statements and thresholds for the resolve operation: This optimizes the theme resolution process, making it more efficient when dealing with a large number of themes across multiple games. The optimizer provided us with blocking keys of <code>summary</code> and <code>theme</code>, and a threshold of 0.596 for similarity (to get 95% recall of duplicates).</p> </li> </ol> <p>These optimizations are crucial for handling the scale of our dataset, which includes 500 games with an average of 66,000 tokens per game, and 12% of the items/reviews exceeding the context length limits of the OpenAI LLMs (128k tokens).</p> Optimized Pipeline <pre><code>default_model: gpt-4o-mini\n\ndatasets:\n  steam_reviews:\n    type: file\n    path: \"/path/to/steam_reviews_dataset.json\"\n\noperations:\n  - name: split_identify_polarizing_themes\n    type: split\n    split_key: concatenated_reviews\n    method: token_count\n    method_kwargs:\n      num_tokens: 87776\n    optimize: false\n\n  - name: gather_concatenated_reviews_identify_polarizing_themes\n    type: gather\n    content_key: concatenated_reviews_chunk\n    doc_id_key: split_identify_polarizing_themes_id\n    order_key: split_identify_polarizing_themes_chunk_num\n    peripheral_chunks:\n      previous:\n        tail:\n          count: 0.1\n    optimize: false\n\n  - name: submap_identify_polarizing_themes\n    type: map\n    prompt: |\n      Analyze the following review snippet from a video game {{ input.app_name }} and identify any polarizing themes within it. A polarizing theme is one that diverges opinions among players, where some express strong approval while others express strong disapproval.\n\n      Review Snippet: {{ input.concatenated_reviews_chunk_rendered }}\n\n      For each polarizing theme you identify:\n      1. Provide a brief summary of the theme\n      2. Explain why it's polarizing\n      3. Include supporting quotes from both positive and negative perspectives.\n\n      Aim to identify and analyze 3-5 polarizing themes within this snippet. Only process the main chunk.\n    model: gpt-4o-mini\n    output:\n      schema:\n        polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n    optimize: false\n\n  - name: subreduce_identify_polarizing_themes\n    type: reduce\n    reduce_key: [\"split_identify_polarizing_themes_id\"]\n    prompt: |\n      Combine the following results and create a cohesive summary of ~10 polarizing themes for the video game {{ inputs[0].app_name }}:\n\n      {% for chunk in inputs %}\n          {% for theme in chunk.polarizing_themes %}\n              {{ theme }}\n              ----------------------------------------\n          {% endfor %}\n      {% endfor %}\n\n      Make sure each theme is unique and not a duplicate of another theme. You should include summaries and supporting quotes (both positive and negative) for each theme.\n    model: gpt-4o-mini\n    output:\n      schema:\n        polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n    pass_through: true\n    associative: true\n    optimize: false\n    synthesize_resolve: false\n\n  - name: unnest_polarizing_themes\n    type: unnest\n    unnest_key: polarizing_themes\n    recursive: true\n    depth: 2\n\n  - name: resolve_themes\n    type: resolve\n    blocking_keys:\n      - summary\n      - theme\n    blocking_threshold: 0.596\n    optimize: true\n    comparison_prompt: |\n      Are the themes \"{{ input1.theme }}\" and \"{{ input2.theme }}\" the same? Here is some context to help you decide:\n\n      Theme 1: {{ input1.theme }}\n      Summary 1: {{ input1.summary }}\n\n      Theme 2: {{ input2.theme }}\n      Summary 2: {{ input2.summary }}\n    resolution_prompt: |\n      Given the following themes, please come up with a theme that best captures the essence of all the themes:\n\n      {% for input in inputs %}\n      Theme {{ loop.index }}: {{ input.theme }}\n      {% if not loop.last %}\n      ---\n      {% endif %}\n      {% endfor %}\n\n      Based on these themes, provide a consolidated theme that captures the essence of all the above themes. Ensure that the consolidated theme is concise yet comprehensive.\n    output:\n      schema:\n        theme: str\n\n  - name: aggregate_common_themes\n    type: reduce\n    reduce_key: theme\n    prompt: |\n      You are given a theme and summary that appears across multiple video games, along with various apps and review quotes related to this theme. Your task is to consolidate this information into a comprehensive report.\n\n      For each input, you will receive:\n      - theme: A specific polarizing theme\n      - summary: A brief summary of the theme\n      - app_name: The name of the game\n      - positive_quotes: List of supporting quotes from positive perspectives\n      - negative_quotes: List of supporting quotes from negative perspectives\n\n      Create a report that includes:\n      1. The name of the common theme\n      2. A summary of the theme and why it's common across games\n      3. Representative quotes from different games, both positive and negative\n\n      Here's the information for the theme:\n      Theme: {{ inputs[0].theme }}\n      Summary: {{ inputs[0].summary }}\n\n      {% for app in inputs %}\n      Game: {{ app.app_name }}\n      Positive Quotes: {{ app.positive_quotes }}\n      Negative Quotes: {{ app.negative_quotes }}\n      {% if not loop.last %}\n      ----------------------------------------\n      {% endif %}\n      {% endfor %}\n\n    output:\n      schema:\n        theme_summary: str\n        representative_quotes: \"list[{game: str, quote: str, sentiment: str}]\"\n\npipeline:\n  steps:\n    - name: game_analysis\n      input: steam_reviews\n      operations:\n        - split_identify_polarizing_themes\n        - gather_concatenated_reviews_identify_polarizing_themes\n        - submap_identify_polarizing_themes\n        - subreduce_identify_polarizing_themes\n        - unnest_polarizing_themes\n        - resolve_themes\n        - aggregate_common_themes\n\n  output:\n    type: file\n    path: \"/path/to/output/output_polarizing_themes.json\"\n    intermediate_dir: \"/path/to/intermediates\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#running-the-pipeline","title":"Running the Pipeline","text":"<p>With our optimized pipeline in place, we can now run it:</p> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>This command will process the game reviews, extract polarizing themes, resolve similar themes across games, and generate comprehensive reports for each common theme. The results will be saved in output_polarizing_themes.json, providing insights into the polarizing aspects of various video games based on user reviews.</p> <p>The output costs for running this pipeline will depend on the size of the dataset and the specific models used. We used gpt-4o-mini and had ~200,000 reviews we were aggregating. Here's the logs from my terminal:</p> <pre><code>docetl run workloads/steamgames/pipeline_opt.yaml\n[11:05:46] Performing syntax check on all operations...\n           Syntax check passed for all operations.\n           Running Operation:\n             Type: split\n             Name: split_identify_polarizing_themes\n[11:06:08] Intermediate saved for operation 'split_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: gather\n             Name: gather_concatenated_reviews_identify_polarizing_themes\n[11:06:10] Intermediate saved for operation 'gather_concatenated_reviews_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: map\n             Name: submap_identify_polarizing_themes\n\u2839 Running step game_analysis...\n[11:06:14] Intermediate saved for operation 'submap_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: reduce\n             Name: subreduce_identify_polarizing_themes\n\u2834 Running step game_analysis...\n[11:06:16] Intermediate saved for operation 'subreduce_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: unnest\n             Name: unnest_polarizing_themes\n[11:06:25] Intermediate saved for operation 'unnest_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: resolve\n             Name: resolve_themes\n[11:06:37] Comparisons saved by blocking: 6802895 (97.50%)\n\u2826 Running step game_analysis...\n[13:05:58] Number of keys before resolution: 3736\n           Number of distinct keys after resolution: 1421\n\u2839 Running step game_analysis...\n[13:06:23] Self-join selectivity: 0.1222\n[13:06:36] Intermediate saved for operation 'resolve_themes' in step 'game_analysis'\n           Running Operation:\n             Type: reduce\n             Name: aggregate_common_themes\n\u2834 Running step game_analysis...\n[13:08:05] Intermediate saved for operation 'aggregate_common_themes' in step 'game_analysis'\n           Flushing cache to disk...\n           Cache flushed to disk.\n  Step game_analysis completed. Cost: $13.21\n  Operation split_identify_polarizing_themes completed. Cost: $0.00\n  Operation gather_concatenated_reviews_identify_polarizing_themes completed. Cost: $0.00\n  Operation submap_identify_polarizing_themes completed. Cost: $5.02\n  Operation subreduce_identify_polarizing_themes completed. Cost: $0.38\n  Operation unnest_polarizing_themes completed. Cost: $0.00\n  Operation resolve_themes completed. Cost: $7.56\n  Operation aggregate_common_themes completed. Cost: $0.26\n           \ud83d\udcbe Output saved to output_polarizing_themes.json\n           Total cost: $13.21\n           Total time: 7339.11 seconds\n</code></pre> <p>Upon further analysis, 1421 themes is still a lot! I realized that my resolve operation was not exactly what I wanted---it did not merge together themes that I believed were similar, since the comparison prompt only asked if theme X or Y were the same. I should have given context in the comparison prompt, such as \"Could one of these themes be a subset of the other?\" This underscores the iterative nature of pipeline development and the importance of refining prompts to achieve the desired results; we don't really know what the desired results are until we see the output.</p> <p>Something else we could have done is included a list of themes we care about in the original map operation, e.g., graphics. Since our map prompt was very open-ended, the LLM could have generated themes that we didn't care about, leading to a large number of themes in the output.</p> <p>Anyways, we've filtered the 1421 reports down to 65 themes/reports that contain quotes from 3 or more different games. You can check out the output here.</p>"},{"location":"examples/ollama/","title":"Medical Document Classification with Ollama","text":"<p>This tutorial demonstrates how to use DocETL with Ollama models to classify medical documents into predefined categories. We'll use a simple map operation to process a set of medical records, ensuring that sensitive information remains private by using a locally-run model.</p>"},{"location":"examples/ollama/#setup","title":"Setup","text":"<p>Prerequisites</p> <p>Before we begin, make sure you have Ollama installed and running on your local machine.</p> <p>You'll need to set the OLLAMA_API_BASE environment variable:</p> <pre><code>export OLLAMA_API_BASE=http://localhost:11434/\n</code></pre> <p>API Details</p> <p>For more information on the Ollama REST API, refer to the Ollama documentation.</p>"},{"location":"examples/ollama/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Let's create a pipeline that classifies medical documents into categories such as \"Cardiology\", \"Neurology\", \"Oncology\", etc.</p> <p>Initial Pipeline Configuration</p> <pre><code>datasets:\n  medical_records:\n    type: file\n    path: \"medical_records.json\"\n\ndefault_model: ollama/llama3\n\nsystem_prompt:\n  dataset_description: a collection of medical records\n  persona: a medical practitioner analyzing patient symptoms and reactions to medications\n\noperations:\n  - name: classify_medical_record\n    type: map\n    output:\n      schema:\n        categories: \"list[str]\"\n    prompt: |\n      Classify the following medical record into one or more of these categories: Cardiology, Neurology, Oncology, Pediatrics, Orthopedics.\n\n      Medical Record:\n      {{ input.text }}\n\n      Return your answer as a JSON list of strings, e.g., [\"Cardiology\", \"Neurology\"].\n\npipeline:\n  steps:\n    - name: medical_classification\n      input: medical_records\n      operations:\n        - classify_medical_record\n\noutput:\n  type: file\n  path: \"classified_records.json\"\n</code></pre>"},{"location":"examples/ollama/#running-the-pipeline-with-a-sample","title":"Running the Pipeline with a Sample","text":"<p>To test our pipeline and estimate the required timeout, we'll first run it on a sample of documents.</p> <p>Modify the <code>classify_medical_record</code> operation in your configuration to include a <code>sample</code> parameter:</p> <pre><code>operations:\n  - name: classify_medical_record\n    type: map\n    sample: 5\n    output:\n      schema:\n        categories: \"list[str]\"\n    prompt: |\n      Classify the following medical record into one or more of these categories: Cardiology, Neurology, Oncology, Pediatrics, Orthopedics.\n\n      Medical Record:\n      {{ input.text }}\n\n      Return your answer as a JSON list of strings, e.g., [\"Cardiology\", \"Neurology\"].\n</code></pre> <p>Now, run the pipeline with this sample configuration:</p> <pre><code>docetl run pipeline.yaml\n</code></pre>"},{"location":"examples/ollama/#adjusting-the-timeout","title":"Adjusting the Timeout","text":"<p>After running the sample, note the time it took to process 5 documents.</p> <p>Timeout Calculation</p> <p>Let's say it took 100 seconds to process 5 documents. You can use this to estimate the time needed for your full dataset. For example, if you have 1000 documents in total, you might want to set the timeout to:</p> <p>(100 seconds / 5 documents) * 1000 documents = 20,000 seconds</p> <p>Now, adjust your pipeline configuration to include this timeout and remove the sample parameter:</p> <pre><code>operations:\n  - name: classify_medical_record\n    type: map\n    timeout: 20000\n    output:\n      schema:\n        categories: \"list[str]\"\n    prompt: |\n      Classify the following medical record into one or more of these categories: Cardiology, Neurology, Oncology, Pediatrics, Orthopedics.\n      Medical Record:\n      {{ input.text }}\n      Return your answer as a JSON list of strings, e.g., [\"Cardiology\", \"Neurology\"].\n</code></pre> <p>Caching</p> <p>DocETL caches results (even between runs), so if the same document is processed again, the answer will be returned from the cache rather than processed again (significantly speeding up processing).</p>"},{"location":"examples/ollama/#running-the-full-pipeline","title":"Running the Full Pipeline","text":"<p>Now you can run the full pipeline with the adjusted timeout:</p> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>This will process all your medical records, classifying them into the predefined categories.</p>"},{"location":"examples/ollama/#conclusion","title":"Conclusion","text":"<p>Key Takeaways</p> <ul> <li>This pipeline demonstrates how to use Ollama with DocETL for local processing of sensitive data.</li> <li>Ollama integrates into multi-operation pipelines, maintaining data privacy.</li> <li>Ollama is a local model, so it is much slower than leveraging an LLM API like OpenAI. Adjust the timeout accordingly.</li> <li>DocETL's sample and timeout parameters help optimize the pipeline for efficient use of Ollama's capabilities.</li> </ul> <p>For more information, e.g., for specific models, visit https://ollama.com/.</p>"},{"location":"examples/pdf-analysis-gemini/","title":"Analyzing NTSB Airplane Crash Reports","text":"<p>This tutorial demonstrates how to analyze National Transportation Safety Board (NTSB) airplane crash reports using PDF processing capabilities of certain LLM providers. We'll build a pipeline that extracts crash causes and synthesizes common patterns across incidents.</p> <p>LLM Requirements</p> <p>PDF processing is only supported with Claude (Anthropic) or Gemini (Google) models.</p>"},{"location":"examples/pdf-analysis-gemini/#dataset-overview","title":"Dataset Overview","text":"<p>The dataset contains 689 NTSB airplane crash reports--the reports corresponding to fatal accidents after 2020. You can download it here: NTSB Airplane Crashes</p>"},{"location":"examples/pdf-analysis-gemini/#pipeline-overview","title":"Pipeline Overview","text":"<p>Our pipeline will:</p> <ol> <li>Process PDF crash reports from the NTSB database to extract causes and recommendations</li> <li>Synthesize common patterns across all analyzed crashes</li> </ol> <p>Let's examine the pipeline structure:</p> <pre><code>pipeline:\n  steps:\n    - name: analyze_crashes\n      input: crashes\n      operations:\n        - extract_crash_cause # this is a map operation\n        - synthesize_findings # this is a reduce operation\n</code></pre> <p>Full Pipeline Configuration</p> <pre><code>datasets:\n  crashes:\n    type: file\n    path: \"fatal.json\"\n\ndefault_model: gemini/gemini-2.0-flash\n\noperations:\n  - name: extract_crash_cause\n    type: map\n    pdf_url_key: ReportUrl\n    skip_on_error: true # Skip llm calls where the PDF is malformed or not found\n    output:\n      schema:\n        cause: str\n        contributing_factors: \"list[str]\"\n        recommendations: str\n    prompt: |\n      Analyze this NTSB airplane crash report and extract:\n      1. The primary cause of the crash (2-3 sentences)\n      2. Any contributing factors (list)\n      3. Key safety recommendations made\n\n  - name: synthesize_findings\n    type: reduce\n    reduce_key: _all\n    output:\n      schema:\n        summary: str\n    prompt: |\n      Analyze the following airplane crash reports:\n\n      {% for item in inputs %}\n      Report {{loop.index}}:\n      Cause: {{ item.cause }}\n      Contributing Factors: {{ item.contributing_factors | join(\", \") }}\n      Recommendations: {{ item.recommendations }}\n\n      {% endfor %}\n\n      Generate a comprehensive analysis that:\n      1. Identifies common causes across incidents\n      2. Lists recurring contributing factors\n      3. Synthesizes key safety recommendations\n      4. Highlights any notable patterns\n\n      Format your response as a structured report.\n\npipeline:\n  steps:\n    - name: analyze_crashes\n      input: crashes\n      operations:\n        - extract_crash_cause\n        - synthesize_findings\n\n  output:\n    type: file\n    path: \"crash_analysis.json\"\n    intermediate_dir: \"checkpoints\"\n</code></pre>"},{"location":"examples/pdf-analysis-gemini/#sample-output","title":"Sample Output","text":"<p>Here's the output we get from running the pipeline:</p> <p>Sample Analysis Output</p> <p>Airplane Crash Report Analysis:</p> <p>1. Common Causes:</p> <p>After analyzing the provided airplane crash reports, the most common primary causes include:</p> <ul> <li>Loss of Control (often due to aerodynamic stall, spatial disorientation, or pilot incapacitation)</li> <li>Engine Failure (often due to fuel exhaustion, mechanical issues, or improper maintenance)</li> <li>Controlled Flight Into Terrain (CFIT) (often in IMC or low visibility)</li> <li>Pilot Error (poor decision-making, failure to maintain airspeed, inadequate pre-flight planning).</li> </ul> <p>2. Recurring Contributing Factors:</p> <p>Several contributing factors recur across multiple reports:</p> <ul> <li>Improper Maintenance (inadequate inspections, incorrect repairs)</li> <li>Fuel Issues (fuel exhaustion, fuel contamination, improper fuel management)</li> <li>Adverse Weather Conditions (IMC, icing, turbulence, low visibility)</li> <li>Pilot Impairment (fatigue, alcohol/drug use, medical conditions)</li> <li>Failure to Maintain Airspeed (leading to stalls)</li> <li>Low Altitude Maneuvering</li> <li>Lack of Instrument Proficiency</li> <li>Poor Decision-Making (continuing flight into adverse conditions, improper risk assessment)</li> <li>Spatial Disorientation (particularly in IMC or at night)</li> <li>Inadequate Pre-flight Planning (weather, fuel, weight and balance).</li> <li>Exceeding Aircraft Limitations (weight, structural, etc.)</li> </ul> <p>3. Synthesized Key Safety Recommendations:</p> <p>Based on the analyzed reports, key safety recommendations can be synthesized:</p> <ul> <li>Enhanced Pilot Training:<ul> <li>Stall recognition and recovery techniques</li> <li>Instrument meteorological conditions (IMC) flight procedures and spatial disorientation awareness.</li> <li>Mountain flying techniques and high-density altitude operations</li> <li>Emergency procedures training, particularly related to engine failures.</li> <li>Aerobatic Maneuver training</li> </ul> </li> <li>Improved Maintenance Practices:<ul> <li>Adherence to manufacturer's recommended maintenance schedules and procedures</li> <li>Thorough inspections of critical components (fuel systems, control cables, engines)</li> <li>Proper documentation of maintenance and repairs</li> <li>Emphasis on proper installation and torquing of critical parts.</li> </ul> </li> <li>Robust Pre-flight Planning:<ul> <li>Thorough weather briefings and in-flight weather monitoring</li> <li>Accurate fuel planning and management</li> <li>Weight and balance calculations</li> <li>Familiarization with terrain, obstacles, and airport characteristics.</li> </ul> </li> <li>Sound Aeronautical Decision-Making:<ul> <li>Avoid flying under the influence of alcohol or drugs</li> <li>Avoid self-induced pressure to complete a flight</li> <li>Recognize personal limitations and make conservative go/no-go decisions</li> <li>Proper risk management</li> </ul> </li> <li>Effective Use of Technology:<ul> <li>Installation and proper use of angle-of-attack indicators</li> <li>Use of autopilot systems and electronic flight displays</li> <li>Ensure aircraft has and is broadcasting ADS-B signals, and use traffic advisory systems when available</li> </ul> </li> <li>Awareness of Physiological Factors:<ul> <li>Understanding of spatial disorientation and how to mitigate its effects</li> <li>Awareness of the effects of fatigue, medical conditions, and medications on pilot performance.</li> <li>Use of oxygen at night.</li> </ul> </li> <li>Adherence to Regulations and Procedures:<ul> <li>Compliance with minimum safe altitudes and approach procedures</li> <li>Proper use of checklists</li> <li>Following air traffic control instructions.</li> </ul> </li> <li>Maintain proper aircraft certification.</li> </ul> <p>4. Notable Patterns:</p> <ul> <li>VFR into IMC: A significant number of accidents involve pilots without instrument ratings continuing visual flight into instrument meteorological conditions.</li> <li>Loss of Control on Approach: A recurring theme is loss of control during the approach phase, often related to stalls, wind shear, or unstable approaches.</li> <li>Pilot Actions Under Stress: Many accidents involve pilots making poor decisions under stressful situations, such as engine failures or adverse weather conditions.</li> <li>Experimental Aircraft Issues: Several reports involve experimental amateur-built aircraft, highlighting potential risks associated with construction, maintenance, and pilot familiarity.</li> <li>Medical Incapacitation: Several accidents were potentially caused by medical incapacitation of the pilot, suggesting that it may be necessary to have health safety standards, especially for older pilots.</li> <li>Power Lines: A concerning number of incidents involve collision with power lines during aerial application or low-altitude maneuvering.</li> </ul> <p>The pipeline costs &lt; $0.05 USD to run.</p>"},{"location":"examples/presidential-debate-themes/","title":"Presidential Debate Themes Analysis","text":"<p>This tutorial explains how to analyze themes in presidential debates using the DocETL pipeline. We'll cover the pipeline structure, explain each operation, and discuss the importance of theme resolution.</p>"},{"location":"examples/presidential-debate-themes/#pipeline-overview","title":"Pipeline Overview","text":"<p>Our goal is to build a pipeline that will:</p> <ol> <li>Extract key themes and viewpoints from presidential debate transcripts</li> <li>Analyze how these themes have evolved over time, with references to specific debates and quotes</li> </ol> <p>You can take a look at the raw data here.</p> <p>Let's examine the pipeline structure and its operations:</p> <pre><code>pipeline:\n  steps:\n    - name: debate_analysis\n      input: debates\n      operations:\n        - extract_themes_and_viewpoints\n        - unnest_themes\n        - summarize_theme_evolution\n</code></pre> Full Pipeline Configuration <pre><code>datasets:\n  debates:\n    type: file\n    path: \"data.json\"\n\nsystem_prompt:\n  dataset_description: a collection of transcripts of presidential debates\n  persona: a political analyst\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_themes_and_viewpoints\n    type: map\n    output:\n      schema:\n        themes: \"list[{theme: str, viewpoints: str}]\"\n    prompt: |\n      Analyze the following debate transcript for {{ input.title }} on {{ input.date }}:\n\n      {{ input.content }}\n\n      Extract the main themes discussed in this debate and the viewpoints of the candidates on these themes.\n      Return a list of themes and corresponding viewpoints (including the specific quotes from the debate) in the following format:\n      [\n        {\n          \"theme\": \"Theme 1\",\n          \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n        },\n        {\n          \"theme\": \"Theme 2\",\n          \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n        },\n        ...\n      ]\n\n  - name: unnest_themes\n    type: unnest\n    unnest_key: themes\n    recursive: true\n\n  - name: summarize_theme_evolution\n    type: reduce\n    reduce_key: theme\n    output:\n      schema:\n        theme: str\n        report: str\n    prompt: |\n      Analyze the following viewpoints on the theme \"{{ inputs[0].theme }}\" from various debates over the years:\n\n      {% for item in inputs %}\n      Year: {{ item.year }}\n      Date: {{ item.date }}\n      Title: {{ item.title }}\n      Viewpoints: {{ item.viewpoints }}\n\n      {% endfor %}\n\n      Generate a comprehensive summary of how Democratic and Republican viewpoints on this theme have evolved through the years. Include supporting quotes from the debates to illustrate key points or shifts in perspective.\n\n      Your summary should:\n      1. Identify *all* major trends or shifts in each party's stance over time\n      2. Highlight any significant agreements or disagreements between the parties\n      3. Note any external events or factors that may have influenced changes in viewpoints\n      4. Use specific quotes to support your analysis\n      5. The title should contain the start and end years of the analysis\n\n      Format your response as a well-structured report.\n\npipeline:\n  steps:\n    - name: debate_analysis\n      input: debates\n      operations:\n        - extract_themes_and_viewpoints\n        - unnest_themes\n        - summarize_theme_evolution\n\n  output:\n    type: file\n    path: \"theme_evolution_analysis.json\"\n    intermediate_dir: \"checkpoints\"\n</code></pre>"},{"location":"examples/presidential-debate-themes/#pipeline-operations","title":"Pipeline Operations","text":""},{"location":"examples/presidential-debate-themes/#1-extract-themes-and-viewpoints","title":"1. Extract Themes and Viewpoints","text":"<pre><code>- name: extract_themes_and_viewpoints\n  type: map\n  output:\n    schema:\n      themes: \"list[{theme: str, viewpoints: str}]\"\n  prompt: |\n    Analyze the following debate transcript for {{ input.title }} on {{ input.date }}:\n\n    {{ input.content }}\n\n    Extract the main themes discussed in this debate and the viewpoints of the candidates on these themes.\n    Return a list of themes and corresponding viewpoints in the following format:\n    [\n      {\n        \"theme\": \"Theme 1\",\n        \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n      },\n      {\n        \"theme\": \"Theme 2\",\n        \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n      },\n      ...\n    ]\n</code></pre> <p>This operation processes each debate transcript to identify main themes and candidates' viewpoints. It uses AI to analyze the content and structure the output in a consistent format.</p>"},{"location":"examples/presidential-debate-themes/#2-unnest-themes","title":"2. Unnest Themes","text":"<pre><code>- name: unnest_themes\n  type: unnest\n  unnest_key: themes\n  recursive: true\n</code></pre> <p>The unnest operation flattens the list of themes extracted from each debate. This step prepares the data for further analysis by creating individual entries for each theme.</p>"},{"location":"examples/presidential-debate-themes/#3-summarize-theme-evolution","title":"3. Summarize Theme Evolution","text":"<pre><code>- name: summarize_theme_evolution\n  type: reduce\n  reduce_key: theme\n  output:\n    schema:\n      theme: str\n      report: str\n  prompt: |\n    Analyze the following viewpoints on the theme \"{{ inputs[0].theme }}\" from various debates over the years:\n\n    {% for item in inputs %}\n    Year: {{ item.year }}\n    Date: {{ item.date }}\n    Title: {{ item.title }}\n    Viewpoints: {{ item.viewpoints }}\n\n    {% endfor %}\n\n    Generate a comprehensive summary of how Democratic and Republican viewpoints on this theme have evolved through the years. Include supporting quotes from the debates to illustrate key points or shifts in perspective.\n\n    Your summary should:\n    1. Identify all major trends or shifts in each party's stance over time\n    2. Highlight any significant agreements or disagreements between the parties\n    3. Note any external events or factors that may have influenced changes in viewpoints\n    4. Use specific quotes to support your analysis\n    5. The title should contain the start and end years of the analysis\n\n    Format your response as a well-structured report.\n</code></pre> <p>This operation analyzes how each theme has evolved over time. It considers viewpoints from multiple debates, identifies trends, and generates a comprehensive summary of the theme's evolution.</p>"},{"location":"examples/presidential-debate-themes/#the-need-for-theme-resolution","title":"The Need for Theme Resolution","text":"<p>An important consideration in this pipeline is the potential for similar themes to be generated with slightly different wording (e.g., \"Climate Change Policy\" vs. \"Environmental Regulations\"). To address this, we need to add a resolve operation before the summarization step.</p> <p>To synthesize a resolve operation, we can use the <code>docetl build</code> command:</p> <pre><code>docetl build pipeline.yaml\n</code></pre> <p>This command adds a resolve operation to our pipeline, resulting in an optimized version:</p> <pre><code>operations:\n    ...\n    - name: synthesized_resolve_0\n      type: resolve\n      blocking_keys:\n        - theme\n      blocking_threshold: 0.6465\n      comparison_model: gpt-4o-mini\n      comparison_prompt: |\n        Compare the following two debate themes:\n\n        [Entity 1]:\n        {{ input1.theme }}\n\n        [Entity 2]:\n        {{ input2.theme }}\n\n        Are these themes likely referring to the same concept? Consider the following attributes:\n        - The core subject matter being discussed\n        - The context in which the theme is presented\n        - The viewpoints of the candidates associated with each theme\n\n        Respond with \"True\" if they are likely the same theme, or \"False\" if they are likely different themes.\n      embedding_model: text-embedding-3-small\n      compare_batch_size: 1000\n      output:\n        schema:\n          theme: string\n      resolution_model: gpt-4o-mini\n      resolution_prompt: |\n        Analyze the following duplicate themes:\n\n        {% for key in inputs %}\n        Entry {{ loop.index }}:\n        {{ key.theme }}\n\n        {% endfor %}\n\n        Create a single, consolidated key that combines the information from all duplicate entries. When merging, follow these guidelines:\n        1. Prioritize the most comprehensive and detailed viewpoint available among the duplicates. If multiple entries discuss the same theme with varying details, select the entry that includes the most information.\n        2. Ensure clarity and coherence in the merged key; if key terms or phrases are duplicated, synthesize them into a single statement or a cohesive description that accurately represents the theme.\n\n        Ensure that the merged key conforms to the following schema:\n        {\n          \"theme\": \"string\"\n        }\n\n        Return the consolidated key as a single JSON object.\n\n\npipeline:\n  steps:\n    - name: debate_analysis\n      input: debates\n      operations:\n        - extract_themes_and_viewpoints\n        - unnest_themes\n        - synthesized_resolve_0\n        - summarize_theme_evolution\n</code></pre> <p>The new <code>synthesized_resolve_0</code> operation groups similar themes together, ensuring a more accurate and comprehensive analysis of each theme's evolution.</p>"},{"location":"examples/presidential-debate-themes/#running-the-optimized-pipeline","title":"Running the Optimized Pipeline","text":"<p>With the resolve operation in place, we can now run our optimized pipeline:</p> <pre><code>docetl run pipeline_opt.yaml\n</code></pre> <p>This command processes the debate transcripts, extracts themes, resolves similar themes, and generates summaries of theme evolution over time. The results will be saved in <code>theme_evolution_analysis.json</code>, providing insights into the changing landscape of topics discussed in presidential debates. Since we've also set an <code>intermediate_dir</code> in our pipeline configuration, intermediate results will be saved in the <code>intermediate_dir</code> directory.</p> <p>Here's the output from running our optimized pipeline:</p> <pre><code>$ docetl run pipeline_opt.yaml\n[09:28:17] Performing syntax check on all operations...\n           Syntax check passed for all operations.\n           Running Operation:\n             Type: map\n             Name: extract_themes_and_viewpoints\n\u2827 Running step debate_analysis...\n[09:28:36] Intermediate saved for operation 'extract_themes_and_viewpoints'\n           Running Operation:\n             Type: unnest\n             Name: unnest_themes\n           Intermediate saved for operation 'unnest_themes'\n           Running Operation:\n             Type: resolve\n             Name: synthesized_resolve_0\n[09:28:38] Comparisons saved by blocking: 56002 (97.75%)\n\u280b Running step debate_analysis...\n[09:29:02] Number of keys before resolution: 339\n           Number of distinct keys after resolution: 152\n\u280b Running step debate_analysis...\n[09:29:04] Self-join selectivity: 0.0390\n           Intermediate saved for operation 'synthesized_resolve_0'\n           Running Operation:\n             Type: reduce\n             Name: summarize_theme_evolution\n\u283c Running step debate_analysis...\n[09:29:54] Intermediate saved for operation 'summarize_theme_evolution'\n           Flushing cache to disk...\n           Cache flushed to disk.\n  Step debate_analysis completed. Cost: $0.29\n  Operation extract_themes_and_viewpoints completed. Cost: $0.16\n  Operation unnest_themes completed. Cost: $0.00\n  Operation synthesized_resolve_0 completed. Cost: $0.04\n  Operation summarize_theme_evolution completed. Cost: $0.09\n           \ud83d\udcbe Output saved to theme_evolution_analysis_baseline.json\n           Total cost: $0.29\n           Total time: 97.25 seconds\n</code></pre> <p>This output shows the progress of our pipeline execution, including the different operations performed, intermediate saves, and the final results. Note the total cost was only $0.29!</p>"},{"location":"examples/presidential-debate-themes/#initial-results","title":"Initial Results","text":"<p>Our pipeline generated reports on various themes discussed in the presidential debates. We've put the results up here. However, upon inspection, we found that these reports were lacking in depth and recency. Let's look at a few examples:</p> <p>Example Reports Lacking in Recent Quotes</p> Infrastructure DevelopmentCrime and Gun ControlDrug Policy <pre><code># Infrastructure Development: A Comparative Analysis of Democratic and Republican Viewpoints from 1992 to 2023\n\n## Introduction\nInfrastructure development has long been a pivotal theme in American political discourse, with varying perspectives presented by major party candidates. This report analyzes shifts and trends in Democratic and Republican viewpoints from 1992, during the second presidential debate between George Bush, Bill Clinton, and Ross Perot, to 2023.\n\n## Republican Viewpoints\n### Early 1990s\nIn 1992, President George Bush emphasized a forward-looking approach to infrastructure, stating, \"We passed this year the most furthest looking transportation bill in the history of this country...$150 billion for improving the infrastructure.\" This statement indicated a commitment to substantial federal investment in infrastructure aimed at enhancing transportation networks.\n\n### 2000s\nMoving into the early 2000s, the Republican party maintained a focus on infrastructure but began to frame it within the context of economic growth and public-private partnerships. However, after the 2008 financial crisis, there was a noticeable shift. The party emphasized tax cuts and reducing regulation over large public investments in infrastructure.\n\n### Recent Years\nBy 2020 and 2021, under the Trump administration, the emphasis returned to infrastructure. However, the tone shifted towards emphasizing private sector involvement and deregulation rather than large public spending. The Republican approach became more fragmented, with some factions calling for aggressive infrastructure investment, while others remained cautious about expenditures.\n\n## Democratic Viewpoints\n### Early 1990s\nIn contrast, Governor Bill Clinton in 1992 proposed a more systematic investment strategy, noting, \"My plan would dedicate $20 billion a year in each of the next 4 years for investments in new transportation.\" This highlighted a stronger emphasis on direct federal involvement in infrastructure as a means of fostering economic opportunity and job creation.\n\n### 2000s\nThrough the late 1990s and early 2000s, the Democratic party continued to push for comprehensive federal infrastructure plans, often attached to broader economic initiatives aimed at reducing inequality and spurring job growth. The party emphasized sustainable infrastructure and investments that address climate change.\n\n### Recent Years\nBy 2020, under the Biden administration, the Democrat viewpoint strongly advocated for significant infrastructure investments, combining traditional infrastructure with climate resilience. The American Jobs Plan symbolized this shift, proposing vast funds for transit systems, renewable energy projects, and rural broadband internet. The framing increasingly included social equity as a core component of infrastructure, influenced by movements advocating for racial and economic justice.\n\n## Agreements and Disagreements\n### Agreements\nDespite inherent differences, both parties have historically acknowledged the necessity of infrastructure investments for economic growth. Both Bush and Clinton in 1992 recognized infrastructure as vital for job creation, but diverged on the scope and funding mechanisms.\n\n### Disagreements\nOver the years, major disagreements have surfaced, particularly in funding approaches. The Republican party has increasingly favored private sector involvement and tax incentives, while Democrats have consistently pushed for robust federal spending and the incorporation of progressive values into infrastructure projects.\n\n## Influencing Factors\nThe evolution of viewpoints has often mirrored external events such as economic recessions, technological advancement, and social movements. The post-9/11 era and the 2008 financial crisis notably shifted priorities, with bipartisan discussions centered around recovery through infrastructure spending. Additionally, increasing awareness of climate change and social justice has over the years significantly influenced Democratic priorities, leading to a more inclusive and sustainable approach to infrastructure development.\n\n## Conclusion\nThe comparative analysis of Democratic and Republican viewpoints on infrastructure development from 1992 to 2023 reveals significant shifts in priorities and strategies. While both parties agree on the need for infrastructure improvements, their approaches and underlying philosophies continue to diverge, influenced by economic, social, and environmental factors.\n</code></pre> <pre><code>## The Evolution of Democratic and Republican Viewpoints on Crime and Gun Control: 1992-2023\n\n### Introduction\nThis report analyzes the shifting perspectives of the Democratic and Republican parties on the theme of \"Crime and Gun Control\" from 1992 to 2023. The exploration encompasses key debates, significant shifts in stance, party alignments, and influences from external events that shaped these viewpoints.\n\n### Democratic Party Viewpoints\n1. **Initial Stance (1992)**: In the early 1990s, the Democratic viewpoint, as exemplified by Governor Bill Clinton during the Second Presidential Debate in 1992, supported individual gun ownership but emphasized the necessity of regulation: \"I support the right to keep and bear arms...but I believe we have to have some way of checking handguns before they're sold.\"\n- **Trend**: This reflects a moderate position seeking to balance gun rights with public safety\u2014a common theme in Democratic rhetoric during this era that resonated with many constituents.\n\n2. **Shift Towards Stricter Gun Control (Late 1990s - 2000s)**: Following events such as the Columbine High School shooting in 1999, the Democratic Party increasingly advocated for more stringent gun control measures. The passing of the Brady Bill and the Assault Weapons Ban in 1994 marked a peak in regulatory measures supported by Democrats, emphasizing public safety over gun ownership rights.\n- **Quote Impact**: During this time, Democratic leaders often highlighted the need for legislative action to combat rising gun violence.\n\n3. **Response to Mass Shootings (2010s)**: The tragic events of the Sandy Hook Elementary School shooting in 2012 ignited a renewed push for gun control from leading Democrats, including then-President Barack Obama. His call for \"common-sense gun laws\" marked a decisive moment in Democrat advocacy, focusing on background checks and bans on assault weapons.\n- **Quote**: Obama stated, \"We can't tolerate this anymore. These tragedies must end. And to end them, we must change.\"\n\n4. **Current Stance (2020s)**: The Democratic viewpoint has continued to become increasingly aligned with comprehensive gun control measures, including calls for universal background checks and red flag laws. In the wake of ongoing gun violence, this approach highlights a commitment to addressing systemic issues related to crime and public safety.\n\n### Republican Party Viewpoints\n1. **Consistent Support for Gun Rights (1992)**: In the same 1992 debate, President George Bush emphasized the rights of gun owners, stating, \"I'm a sportsman and I don't think you ought to eliminate all kinds of weapons.\" This illustrates a steadfast commitment to Second Amendment rights that has characterized Republican positions over the years.\n- **Trend**: The Republican Party has traditionally promoted a pro-gun agenda, often resisting calls for stricter regulations or bans.\n\n2. **Response to Gun Control Advocacy (2000s)**: As Democrats pushed for stricter gun laws, Republicans increasingly framed these measures as infringements on individual rights. The response to high-profile shootings tended to focus on mental health and crime prevention rather than gun regulation.\n- **Disagreement**: Republicans consistently argued against the effectiveness of gun control, indicating belief in personal responsibility and the right to self-defense.\n\n3. **Shift to Increased Resistance (2010s)**: In the wake of prominent mass shootings, the Republican party maintained its focus on supporting gun rights, opposing federal gun control initiatives. Notable figures, such as former NRA spokesperson Dana Loesch, articulated this resistance by stating, \"We are not going to let tragedies be used to violate our rights.\"\n- **Impact of External Events**: The rise of organizations like the NRA and increased gun ownership among constituents have fortified this pro-gun stance.\n\n4. **Contemporary Stance (2020s)**: Currently, the Republican viewpoint remains largely unchanged with an emphasis on individual rights to bear arms and skepticism regarding the effectiveness of gun control laws. Recent discussions around gun violence often focus on addressing crime through law enforcement and community safety programs instead of legislative gun restrictions.\n\n### Key Agreements and Disagreements\n- **Common Ground**: Both parties, at different times, have recognized the necessity for addressing gun-related violence but diverge on methods\u2014Democrats typically advocate for regulations while Republicans focus on rights preservation.\n- **Disagreements**: A significant divide exists on whether stricter gun laws equate to reduced crime rates, with Republicans consistently refuting this correlation, arguing instead that law-abiding citizens need access to firearms for self-defense.\n\n### Conclusion\nThe evolution of viewpoints on crime and gun control from 1992 to 2023 highlights a pronounced divergence between the Democratic and Republican parties. While Democrats have increasingly pursued stricter regulatory measures focused on public safety, Republicans have maintained a consistent advocacy for gun rights, underscoring a broader ideological conflict over individual freedoms and collective responsibility for public safety. The trajectories of both parties reflect their core values and responses to notable events impacting society.\n</code></pre> <pre><code>## Evolution of Drug Policy Viewpoints: 1996 to 2023\n\n### Summary of Democratic and Republican Perspectives on Drug Policy\nOver the years, drug policy has been a contentious issue in American politics, reflecting profound changes within both the Democratic and Republican parties. This report examines how views have evolved, highlighting significant trends, areas of agreement and disagreement, and influential external factors utilizing debates as primary reference points.\n\n### Democratic Party Trends:\n1. **Increased Emphasis on Comprehensive Approaches**:\n   - In the 1996 Clinton-Dole debate, President Bill Clinton stated, \"We have dramatically increased control and enforcement at the border.\" This reflects a focus on enforcement as part of drug policy. However, Clinton's later years signaled a shift towards recognizing the need for treatment and prevention.\n\n2. **Emergence of Harm Reduction and Decriminalization**:\n   - Moving into the 2000s and beyond, Democrats began to embrace harm reduction strategies. For instance, during the 2020 Democratic primary debates, candidates such as Bernie Sanders and Elizabeth Warren emphasized decriminalization and treatment over incarceration, signifying a notable shift from punitive measures.\n\n3. **Growing Advocacy for Social Justice**:\n   - Recent years have seen an alignment with social justice movements, arguing that drug policy disproportionately affects marginalized communities. Kamala Harris in 2020 stated, \"We need to truly decriminalize marijuana and address the impact of the War on Drugs on communities of color.\"\n\n### Republican Party Trends:\n1. **Strong Focus on Law and Order**:\n   - During the 1996 debate, Senator Bob Dole reflected a traditional Republican stance, highlighting concerns over drugs without markedly addressing the social implications. \"The President doesn't want to politicize drugs, but it's already politicized Medicare...\" displays a defensive posture toward the political ramifications of drug issues.\n\n2. **Shift Towards Treatment and Prevention**:\n   - By the mid-2010s, there was a growing recognition of the opioid crisis, leading to a bipartisan approach promoting treatment. For example, former President Donald Trump, in addressing the opioid epidemic, stated, \"We have to take care of our people. We can't just lock them up.\"\n\n3. **Conflict Between Hardline Stance and Pragmatism**:\n   - Despite some shifts, many Republicans still emphasize law enforcement solutions. This tension was evident in the polarizing responses to marijuana legalization across states, with figures like former Attorney General Jeff Sessions taking a hardline stance against marijuana legalization, contrasting with more progressive approaches adopted by some Republican governors.\n\n### Areas of Agreement:\n1. **Opioid Crisis**:\n   - Both parties acknowledged the severity of the opioid epidemic, leading to legislation aimed at addressing addiction and treatment, indicating a rare consensus on the need for health-focused solutions.\n\n### Areas of Disagreement:\n1. **Approach to Drug Policy**:\n   - The Democratic party's shift towards decriminalization and harm reduction contrasts sharply with segments of the Republican party that still advocate for strict enforcement and criminalization of certain drugs.\n\n### Influential External Events and Factors:\n1. **The Opioid Crisis**:\n   - The rise of the opioid epidemic has forced both parties to reevaluate their positions on drug policy, pushing them towards more compassionate approaches focusing on addiction treatment.\n\n2. **Social Justice Movements**:\n   - The Black Lives Matter movement and other social justice efforts have altered the discourse surrounding drug policies, with increased focus on the need to rectify injustices in enforcement practices, particularly among minorities.\n\n### Conclusion:\nThrough the years, drug policy viewpoints within the Democratic and Republican parties have experienced significant evolution, characterized by complex layers of agreement and disagreement. As social dynamics shift, both parties continue to grapple with finding a balanced approach towards a more effective drug policy that prioritizes health and social justice.\n</code></pre> <p>Upon inspecting the intermediates, it appears that the map operation is doing a good job at extracting relevant information. The issue seems to lie in the reduce operation, which is ignoring some of the analysis.</p> <p>It's possible that trying to summarize all the insights across all debates for a topic in a single LLM call is too ambitious. To address this, we can set <code>optimize: true</code> for the reduce operation.</p> <p>Let's update our <code>summarize_theme_evolution</code> operation in the pipeline:</p> <pre><code>- name: summarize_theme_evolution\n  type: reduce\n  reduce_key: theme\n  optimize: true\n  output:\n    schema:\n      theme: str\n      report: str\n  prompt: |\n    [... existing prompt ...]\n</code></pre> <p>Rerunning the build command <code>docetl build pipeline.yaml</code> will synthesize the optimized reduce operation (make sure <code>pipeline.yaml</code> is the pipeline you want to optimize).</p>"},{"location":"examples/presidential-debate-themes/#running-the-pipeline-with-optimized-reduce","title":"Running the Pipeline (with Optimized Reduce)","text":"<p>In our optimized pipeline, we see that DocETL added a <code>gleaning</code> configuration to the reduce operation:</p> <pre><code>- name: summarize_theme_evolution\n  type: reduce\n  reduce_key: theme\n  ...\n  gleaning:\n    num_rounds: 1\n    validation_prompt: |\n        1. Does the output adequately summarize the evolution of viewpoints on the theme based on the\n        provided debate texts? Are all critical shifts and trends mentioned?\n        2. Are there any crucial quotes or data points missing from the output that\n        were present in the debate transcripts that could reinforce the analysis?\n        3. Is the output well-structured and easy to follow, following any\n        formatting guidelines specified in the prompt, such as using headings for\n        sections or maintaining a coherent narrative flow?\n</code></pre> <p>Tip</p> <p>Note that you should always feel free to edit the <code>validation_prompt</code> to better suit your specific needs! The optimizer uses LLMs to write all prompts, but you have the best context on your task and what you're looking for in the output, so you should adjust anything accordingly.</p> <p>And when running the pipeline, we can observe the impact of this optimization; for example, one of the outputs gets amended to include more recent quotes:</p> <pre><code>Validator improvements (gleaning round 1):\n1. **Coverage of Critical Shifts and Trends**: While the output captures several significant trends and shifts in Democratic and Republican viewpoints, it could benefit from a more thorough overview, especially about the changing perceptions and proposals related to economic downturns and recoveries across the decades. For instance, it could include how the response to the 2008 financial crisis tied back to historical precedents, linking back to earlier debates about the importance of jobs and middle-class stability (like in the 1976 or 1992 debates).\n\n2. **Missing Quotes and Data Points**: The response could further bolster the analysis by incorporating additional quotes that illustrate the evolving narrative, particularly surrounding tax fairness and job creation. For example, quotes like George Bush in 1984 calling out \"working man\" perspectives against seemingly progressive taxation could enhance the depth. Additionally, quotes from debates emphasizing the impact of tax cuts on economic recovery and job growth\u2014such as Obama's focus on the automotive industry's recovery in 2012 or McCain's 'putting homeowners first'\u2014could provide essential context to the arguments for both parties.\n\n3. **Structure and Flow**: Overall, the output is fairly well-structured and maintains a logical flow, using headings appropriately to signal key sections. However, it may benefit from clearer subsections under each party's overview to delineate specific key points, such as 'Tax Policy', 'Job Creation', and 'Response to Economic Crises'. This would enhance readability and assist the reader in navigating the shifts in viewpoints. For example, adding bullet points or more vivid transitions between historical periods could clarify the evolution timeline. Moreover, resolving any redundancy (such as multiple mentions of similar themes across years) would streamline the narrative.\n</code></pre> <p>Check out the new output here to see the improvements made by the optimized pipeline! Of course, we can probably optimize the initial map operation too, do prompt engineering, and more to further enhance the pipeline.</p> <p>Interactive Pipeline Creation</p> <p>We're currently building interfaces to interactively create and iterate on these pipelines after seeing outputs. This will allow for more intuitive pipeline development and refinement based on real-time results. If you're interested in this feature or would like to provide input, please reach out to us! Your feedback can help shape the future of DocETL's user experience.</p>"},{"location":"examples/rate-limiting/","title":"Rate Limiting","text":"<p>When using DocETL, you might have rate limits based on your usage tier with various API providers. To help manage these limits and prevent exceeding them, DocETL allows you to configure rate limits in your YAML configuration file.</p>"},{"location":"examples/rate-limiting/#configuring-rate-limits","title":"Configuring Rate Limits","text":"<p>You can add rate limits to your YAML config by including a <code>rate_limits</code> key with specific configurations for different types of API calls. Here's an example of how to set up rate limits:</p> <pre><code>rate_limits:\n  embedding_call:\n    - count: 1000\n      per: 1\n      unit: second\n  llm_call:\n    - count: 1\n      per: 1\n      unit: second\n    - count: 10\n      per: 5\n      unit: hour\n  llm_tokens:\n    - count: 1000000\n      per: 1\n      unit: minute\n</code></pre> <p>Your YAML configuration should have a <code>rate_limits</code> key with the config as shown above. This example sets limits for embedding calls and language model (LLM) calls, with multiple rules for LLM calls to accommodate different time scales.</p> <p>You can also use rate limits in the Python API, passing in a <code>rate_limits</code> dictionary when you initialize the <code>Pipeline</code> object.</p>"},{"location":"examples/split-gather/","title":"Split and Gather Example: Analyzing Trump Immunity Case","text":"<p>This example demonstrates how to use the Split and Gather operations in DocETL to process and analyze a large legal document, specifically the government's motion for immunity determinations in the case against former President Donald Trump. You can download the dataset we'll be using here. This dataset contains a single document.</p>"},{"location":"examples/split-gather/#problem-statement","title":"Problem Statement","text":"<p>We want to analyze a lengthy legal document to identify all people involved in the Trump vs. United States case regarding presidential immunity. The document is too long to process in a single operation, so we need to split it into manageable chunks and then gather context to ensure each chunk can be analyzed effectively.</p>"},{"location":"examples/split-gather/#chunking-strategy","title":"Chunking Strategy","text":"<p>When dealing with long documents, it's often necessary to break them down into smaller, manageable pieces. This is where the Split and Gather operations come in handy:</p> <ol> <li> <p>Split Operation: This divides the document into smaller chunks based on token count or delimiters. For legal documents, using a token count method is often preferable to ensure consistent chunk sizes.</p> </li> <li> <p>Gather Operation: After splitting, we use the Gather operation to add context to each chunk. This operation can include content from surrounding chunks, as well as document-level metadata and headers, ensuring that each piece maintains necessary context for accurate analysis.</p> </li> </ol> <p>Pipeline Overview</p> <p>Our pipeline will follow these steps:</p> <ol> <li>Extract metadata from the full document</li> <li>Split the document into chunks</li> <li>Extract headers from each chunk</li> <li>Gather context for each chunk</li> <li>Analyze each chunk to identify people and their involvements in the case</li> <li>Reduce the results to compile a comprehensive list of people and their roles</li> </ol>"},{"location":"examples/split-gather/#example-pipeline-and-output","title":"Example Pipeline and Output","text":"<p>Here's a breakdown of the pipeline defined in trump-immunity_opt.yaml:</p> <ol> <li> <p>Dataset Definition:     We define a dataset (json file) with a single document.</p> </li> <li> <p>Metadata Extraction:     We define a map operation to extract any document-level metadata that we want to pass to each chunk being processed.</p> </li> <li> <p>Split Operation:     The document is split into chunks using the following configuration:</p> <pre><code>- name: split_find_people_and_involvements\n  type: split\n  method: token_count\n  method_kwargs:\n    num_tokens: 3993\n  split_key: extracted_text\n</code></pre> <p>This operation splits the document into chunks of approximately 3993 tokens each. This size is chosen to balance between maintaining context and staying within model token limits. <code>split_key</code> should be the field in the document that contains the text to split.</p> </li> <li> <p>Header Extraction:     We define a map operation to extract headers from each chunk. Then, when rendering each chunk, we can also render the headers in levels above the headers in the chunk--ensuring that we can maintain hierarchical context, even when the headers are in other chunks.</p> </li> <li> <p>Gather Operation:     Context is gathered for each chunk using the following configuration:</p> <pre><code>- name: gather_extracted_text_find_people_and_involvements\n  type: gather\n  content_key: extracted_text_chunk # (1)!\n  doc_header_key: headers # (2)!\n  doc_id_key: split_find_people_and_involvements_id # (3)!\n  order_key: split_find_people_and_involvements_chunk_num # (4)!\n  peripheral_chunks:\n    next:\n      head:\n        count: 1\n    previous:\n      tail:\n        count: 1\n</code></pre> <ol> <li>The field containing the chunk content; the split_key with \"_chunk\" appended. Automatically exists as a result of the split operation. This is required.</li> <li>The field containing the extracted headers for each chunk. Only exists if you have a header extraction map operation. This can be omitted if you don't have headers extracted for each chunk.</li> <li>The unique identifier for each document; the split operation name with \"_id\" appended. Automatically exists as a result of the split operation. This is required.</li> <li>The field indicating the order of chunks; the split operation name with \"_chunk_num\" appended. Automatically exists as a result of the split operation. This is required.</li> </ol> <p>This operation gathers context for each chunk, including the previous chunk, the current chunk, and the next chunk. We also render the headers populated by the previous operation.</p> </li> <li> <p>Chunk Analysis:     We define a map operation to analyze each chunk.</p> </li> <li> <p>Result Reduction:     We define a reduce operation to reduce the results of the map operation (applied to each chunk) to a single list of people and their involvements in the case.</p> </li> </ol> <p>Here is the full pipeline configuration, with the split and gather operations highlighted. Assuming the sample dataset looks like this:</p> <pre><code>[\n  {\n    \"pdf_url\": \"https://storage.courtlistener.com/recap/gov.uscourts.dcd.258148/gov.uscourts.dcd.258148.252.0.pdf\"\n  }\n]\n</code></pre> Full Pipeline Configuration <pre><code>datasets:\n  legal_doc:\n    type: file\n    path: /path/to/your/dataset.json\n    parsing: # (1)!\n      - function: azure_di_read\n        input_key: pdf_url\n        output_key: extracted_text\n        function_kwargs:\n          use_url: true\n          include_line_numbers: true\n\ndefault_model: gpt-4o-mini\n\nsystem_prompt:\n  dataset_description: the Trump vs. United States case\n  persona: a legal analyst\n\noperations:\n  - name: extract_metadata_find_people_and_involvements\n    type: map\n    model: gpt-4o-mini\n    prompt: |\n      Given the document excerpt: {{ input.extracted_text }}\n      Extract all the people mentioned and summarize their involvements in the case described.\n    output:\n      schema:\n        metadata: str\n\n  - name: split_find_people_and_involvements\n    type: split\n    method: token_count\n    method_kwargs:\n      num_tokens: 3993\n    split_key: extracted_text\n\n  - name: header_extraction_extracted_text_find_people_and_involvements\n    type: map\n    model: gpt-4o-mini\n    output:\n      schema:\n        headers: \"list[{header: string, level: integer}]\"\n    prompt: |\n      Analyze the following chunk of a document and extract any headers you see.\n\n      { input.extracted_text_chunk }\n\n      Examples of headers and their levels based on the document structure:\n      - \"GOVERNMENT'S MOTION FOR IMMUNITY DETERMINATIONS\" (level 1)\n      - \"Legal Framework\" (level 1)\n      - \"Section I\" (level 2)\n      - \"Section II\" (level 2)\n      - \"Section III\" (level 2)\n      - \"A. Formation of the Conspiracies\" (level 3)\n      - \"B. The Defendant Knew that His Claims of Outcome-Determinative Fraud Were False\" (level 3)\n      - \"1. Arizona\" (level 4)\n      - \"2. Georgia\" (level 4)\n\n  - name: gather_extracted_text_find_people_and_involvements\n    type: gather\n    content_key: extracted_text_chunk\n    doc_header_key: headers\n    doc_id_key: split_find_people_and_involvements_id\n    order_key: split_find_people_and_involvements_chunk_num\n    peripheral_chunks:\n      next:\n        head:\n          count: 1\n      previous:\n        tail:\n          count: 1\n\n  - name: submap_find_people_and_involvements\n    type: map\n    model: gpt-4o-mini\n    output:\n      schema:\n        people_and_involvements: list[str]\n    prompt: |\n      Given the document excerpt: {{ input.extracted_text_chunk_rendered }}\n      Extract all the people mentioned and summarize their involvements in the case described. Only process the main chunk.\n\n  - name: subreduce_find_people_and_involvements\n    type: reduce\n    model: gpt-4o-mini\n    associative: true\n    pass_through: true\n    synthesize_resolve: false\n    output:\n      schema:\n        people_and_involvements: list[str]\n    reduce_key:\n      - split_find_people_and_involvements_id\n    prompt: |\n      Given the following extracted information about individuals involved in the case, compile a comprehensive list of people and their specific involvements in the case:\n\n      {% for chunk in inputs %}\n      {% for involvement in chunk.people_and_involvements %}\n      - {{ involvement }}\n      {% endfor %}\n      {% endfor %}\n\n      Make sure to include all the people and their involvements. If a person has multiple involvements, group them together.\n\npipeline:\n  steps:\n    - name: analyze_document\n      input: legal_doc\n      operations:\n        - extract_metadata_find_people_and_involvements\n        - split_find_people_and_involvements\n        - header_extraction_extracted_text_find_people_and_involvements\n        - gather_extracted_text_find_people_and_involvements\n        - submap_find_people_and_involvements\n        - subreduce_find_people_and_involvements\n\n  output:\n    type: file\n    path: /path/to/your/output/people_and_involvements.json\n    intermediate_dir: /path/to/your/intermediates\n</code></pre> <ol> <li>This is an example parsing function, as explained in the Parsing docs. You can define your own parsing function to extract the text you want to split, or just have the text be directly in the json file.</li> </ol> <p>Running the pipeline with <code>docetl run pipeline.yaml</code> will execute the pipeline and save the output to the path specified in the output section. It cost $0.05 and took 23.8 seconds with gpt-4o-mini.</p> <p>Here's a table with one column listing all the people mentioned in the case and their involvements:</p> Final Output People Involved in the Case and Their Involvements DONALD J. TRUMP: Defendant accused of orchestrating a criminal scheme to overturn the 2020 presidential election results through deceit and collaboration with private co-conspirators; charged with leading conspiracies to overturn the 2020 presidential election; made numerous claims of election fraud and pressured officials to find votes to overturn the election results; incited a crowd to march to the Capitol; communicated with various officials regarding election outcomes; exerted political pressure on Vice President Pence; publicly attacked fellow party members for not supporting his claims; involved in spreading false claims about the election, including through Twitter; pressured state legislatures to take unlawful actions regarding electors; influenced campaign decisions and narrative regarding the election results; called for action to overturn the certified results and demanded compliance from officials; worked with co-conspirators on efforts to promote fraudulent elector plans and led actions that culminated in the Capitol riot. MICHAEL R. PENCE: Vice President at the time, pressured by Trump to obstruct Congress's certification of the election; informed Trump there was no evidence of significant fraud; encouraged Trump to accept election results; involved in discussions with Trump regarding election challenges and strategies; publicly asserted his constitutional limitations in the face of Trump's pressure; became the target of attacks from Trump and the Capitol rioters; sought to distance himself from Trump's efforts to overturn the election. CC1: Private attorney who Trump enlisted to falsely claim victory and perpetuate fraud allegations; participated in efforts to influence political actions in targeted states; suggested the defendant declare victory despite ongoing counting; actively involved in making false fraud claims regarding the election; pressured state officials; spread false claims about election irregularities and raised threats against election workers; coordinated fraudulent elector meetings and misrepresented legal bases. CC2: Mentioned as a private co-conspirator involved in the efforts to invalidate election results; proposed illegal strategies to influence the election certification; urged others to decertify legitimate electors; involved in discussions influencing state officials; pressured Mike Pence to act against certification; experienced disappointment with Pence's rejection of proposed strategies; presented unlawful plans to key figures. CC3: Another private co-conspirator involved in scheming to undermine legitimate vote counts; promoted false claims during public hearings and made remarks inciting fraud allegations; encouraged fraudulent election lawsuits and made claims about voting machines; pressured other officials regarding claims of election fraud. CC5: Private political operative who collaborated in the conspiracy; worked on coordinating actions related to the fraudulent elector plan; engaged in text discussions regarding the electors and strategized about the fraud claims. CC6: Private political advisor providing strategic guidance to Trump's re-election efforts; involved in communications with campaign staff regarding the electoral vote processes. P1: Private political advisor who assisted with Trump's re-election campaign; advocated declaring victory before final counts; maintained a podcast spreading false claims about the election. P2: Trump's Campaign Manager, providing campaign direction during the election aftermath; informed the defendant regarding false claims related to state actions. P3: Deputy Campaign Manager, involved in assessing election outcomes; coordinated with team members discussing legal strategies post-election; marked by frequent contact with Trump regarding campaign operations. P4: Senior Campaign Advisor, part of the team advising Trump on election outcome communication; expressed skepticism about allegations of fraud; contradicted Trump's claims about deceased voters in Georgia. P5: Campaign operative and co-conspirator, instructed to create chaos during vote counting and incited unrest at polling places; engaged in discussions about the elector plan. P6: Private citizen campaign advisor who provided early warnings regarding the election outcome; engaged in discussions about the validity of allegations. P7: White House staffer and campaign volunteer who advised Trump on potential election challenges and outcomes; acted as a conduit between Trump and various officials; communicated political advice relevant to the election. P8: Staff member of Pence, who communicated about the electoral process and advised against Trump's unlawful plans; was involved in discussions of political strategy surrounding election results. P9: White House staffer who became a link between Trump and campaign efforts regarding fraud claims; provided truthful assessments of the situation; facilitated communications during post-election fraud discussions. P12: Attended non-official legislative hearings; involved in spreading disinformation about election irregularities. P15: Assistant to the President who overheard Trump's private comments about fighting to remain in power after the 2020 election; involved in discussions about various election-related strategies. P16: Governor of Arizona; received calls from Trump regarding election fraud claims and the count in Arizona. P18: Speaker of the Arizona State House contacted as part of efforts to challenge election outcomes; also expressed reservations about Trump's strategies. P21: Chief of Staff who exchanged communications about the fraudulent allegations; facilitated discussions and logistics during meetings. P22: Campaign attorney who verified that claims about deceased voters were false; participated in discussions around the integrity of the election results. P26: Georgia Attorney General contacted regarding fraud claims; openly stated there was no substantive evidence to support fraud allegations; discussed Texas v. Pennsylvania lawsuit with Trump. P33: Georgia Secretary of State; defended election integrity publicly; stated rumors of election fraud were false; involved in discussions about the impact of fraudulent elector claims in Georgia. P39: RNC Chairwoman; advised against lobbying with state legislators; coordinated with Trump on fraudulent elector efforts; refused to promote inaccurate reports regarding election fraud. P47: Philadelphia City Commissioner; stated there was no evidence of widespread fraud; targeted by Trump for criticism after his public statements. P52: Attorney General who publicly stated that there was no evidence of fraud that would affect election results; faced pressure from Trump's narrative. P50: CISA Director; publicly declared the election secure; faced backlash after contradicting Trump's claims about election fraud. P53: Various Republican U.S. Senators participated in rallies organized by Trump; linked to his campaign efforts regarding the election process. P54: Campaign staff member involved in strategizing about elector votes; discussed procedures and expectations surrounding election tasks and claims. P57: Former U.S. Representative who opted out of the fraudulent elector plan in Pennsylvania; cited legal concerns about the actions being proposed. P58: A staff member of Pence involved in communications directing Pence regarding official duties, managing conversations surrounding election processes. P59: Community organizers who were engaged in discussions relating to Trump's electoral undertakings. P60: Individual responses to Trump's directives aimed at influencing ongoing election outcomes and legislative actions."},{"location":"examples/split-gather/#optional-compiling-a-pipeline-into-a-split-gather-pipeline","title":"Optional: Compiling a Pipeline into a Split-Gather Pipeline","text":"<p>You can also compile a pipeline into a split-gather pipeline using the <code>docetl build</code> command. Say we had a much simpler pipeline for the same document analysis task as above, with just one map operation to extract people and their involvements.</p> <pre><code>default_model: gpt-4o-mini\n\ndatasets:\n  legal_doc: # (1)!\n    path: /path/to/dataset.json\n    type: file\n    parsing: # (2)!\n      - input_key: pdf_url\n        function: azure_di_read\n        output_key: extracted_text\n        function_kwargs:\n          use_url: true\n          include_line_numbers: true\n\noperations:\n  - name: find_people_and_involvements\n    type: map\n    optimize: true\n    prompt: |\n      Given this document, extract all the people and their involvements in the case described by the document.\n\n      {{ input.extracted_text }}\n\n      Return a list of people and their involvements in the case.\n    output:\n      schema:\n        people_and_involvements: list[str]\n\npipeline:\n  steps:\n    - name: analyze_document\n      input: legal_doc\n      operations:\n        - find_people_and_involvements\n\n  output:\n    type: file\n    path: \"/path/to/output/people_and_involvements.json\"\n</code></pre> <ol> <li>This is an example parsing function, as explained in the Parsing docs. You can define your own parsing function to extract the text you want to split, or just have the text be directly in the json file. If you want the text directly in the json file, you can have your json be a list of objects with a single field \"extracted_text\".</li> <li>You can remove this parsing section if you don't need to parse the document (i.e., if the text is already in the json file in the \"extracted_text\" field in the object).</li> </ol> <p>In the pipeline above, we don't have any split or gather operations. Running <code>docetl build pipeline.yaml [--model=gpt-4o-mini]</code> will output a new pipeline_opt.yaml file with the split and gather operations highlighted--like we had defined in the previous example. Note that this cost us $20 to compile, since we tried a bunch of different plans...</p>"},{"location":"execution/running-pipelines/","title":"Additional Notes","text":"<p>Here are some additional notes to help you get the most out of your pipeline:</p> <ul> <li>Sampling Operations: If you want to run an operation on a random sample of your data, you can set the sample parameter for that operation. For example:</li> </ul> <pre><code>operations:\n  extract_medications:\n    sample: 100 # This will run the operation on a random sample of 100 items\n    # ... rest of the operation configuration\n</code></pre> <ul> <li> <p>Caching: DocETL caches the results of operations by default. This means that if you run the same operation on the same data multiple times, the results will be retrieved from the cache rather than being recomputed. You can clear the cache by running docetl clear-cache.</p> </li> <li> <p>The run Function: The main entry point for running a pipeline is the run function in docetl/cli.py. Here's a description of its parameters and functionality:</p> </li> </ul> <p>Run the configuration specified in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>max_threads</code> <code>int | None</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef run(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    max_threads: int | None = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n):\n    \"\"\"\n    Run the configuration specified in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        max_threads (int | None): Maximum number of threads to use for running operations.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    runner = DSLRunner.from_yaml(str(yaml_file), max_threads=max_threads)\n    runner.load_run_save()\n</code></pre> <p>handler: python   options:     members: - run   show_root_full_path: true   show_root_toc_entry: true   show_root_heading: true   show_source: false   show_name: true</p> <ul> <li>Intermediate Output: If you provide an intermediate directory in your configuration, the outputs of each operation will be saved to this directory. This allows you to inspect the results of individual steps in the pipeline and can be useful for debugging or analyzing the pipeline's progress. Set the intermediate_dir parameter in your pipeline's output configuration to specify the directory where intermediate results should be saved; e.g.,</li> </ul> <pre><code>pipeline:\n  output:\n    type: file\n    path: ...\n    intermediate_dir: intermediate_results\n</code></pre>"},{"location":"operators/cluster/","title":"Cluster operation","text":"<p>The Cluster operation in DocETL groups all items into a binary tree using agglomerative clustering of the embedding of some keys, and annotates each item with the path through this tree down to the item (Note that the path is reversed, starting with the most specific grouping, and ending in the root of the tree, the cluster that encompasses all your input).</p> <p>Each cluster is summarized using an llm prompt, taking the summaries of its children as inputs (or for the leaf nodes, the actual items).</p>"},{"location":"operators/cluster/#example-grouping-concepts-from-a-knowledge-graph","title":"\ud83d\ude80 Example: Grouping concepts from a knowledge-graph","text":"<pre><code>- name: cluster_concepts\n  type: cluster\n  max_batch_size: 5\n  embedding_keys:\n    - concept\n    - description\n  output_key: categories # This is optional, and defaults to \"clusters\"\n  summary_schema:\n    concept: str\n    description: str\n  summary_prompt: |\n    The following describes two related concepts. What concept\n    encompasses both? Try not to be too broad; it might be that one of\n    these two concepts already encompasses the other; in that case,\n    you should just use that concept.\n\n    {% for input in inputs %}\n    {{input.concept}}:\n    {{input.description}}\n    {% endfor %}\n\n    Provide the title of the super-concept, and a description.\n</code></pre> <p>This cluster operation processes a set of concepts, each with a title and a description, and groups them into a tree of categories.</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"concept\": \"Shed\",\n    \"description\": \"A shed is typically a simple, single-story roofed structure, often used for storage, for hobbies, or as a workshop, and typically serving as outbuilding, such as in a back garden or on an allotment. Sheds vary considerably in their size and complexity of construction, from simple open-sided ones designed to cover bicycles or garden items to large wood-framed structures with shingled roofs, windows, and electrical outlets. Sheds used on farms or in the industry can be large structures. The main types of shed construction are metal sheathing over a metal frame, plastic sheathing and frame, all-wood construction (the roof may be asphalt shingled or sheathed in tin), and vinyl-sided sheds built over a wooden frame. Small sheds may include a wooden or plastic floor, while more permanent ones may be built on a concrete pad or foundation. Sheds may be lockable to deter theft or entry by children, domestic animals, wildlife, etc.\"\n  },\n  {\n    \"concept\": \"Barn\",\n    \"description\": \"A barn is an agricultural building usually on farms and used for various purposes. In North America, a barn refers to structures that house livestock, including cattle and horses, as well as equipment and fodder, and often grain.[2] As a result, the term barn is often qualified e.g. tobacco barn, dairy barn, cow house, sheep barn, potato barn. In the British Isles, the term barn is restricted mainly to storage structures for unthreshed cereals and fodder, the terms byre or shippon being applied to cow shelters, whereas horses are kept in buildings known as stables.[2][3] In mainland Europe, however, barns were often part of integrated structures known as byre-dwellings (or housebarns in US literature). In addition, barns may be used for equipment storage, as a covered workplace, and for activities such as threshing.\"\n  },\n  {\n    \"concept\": \"Tree house\",\n    \"description\": \"A tree house, tree fort or treeshed, is a platform or building constructed around, next to or among the trunk or branches of one or more mature trees while above ground level. Tree houses can be used for recreation, work space, habitation, a hangout space and observation. People occasionally connect ladders or staircases to get up to the platforms.\"\n  },\n  {\n    \"concept\": \"Castle\",\n    \"description\": \"A castle is a type of fortified structure built during the Middle Ages predominantly by the nobility or royalty and by military orders. Scholars usually consider a castle to be the private fortified residence of a lord or noble. This is distinct from a mansion, palace, and villa, whose main purpose was exclusively for pleasance and are not primarily fortresses but may be fortified.[a] Use of the term has varied over time and, sometimes, has also been applied to structures such as hill forts and 19th- and 20th-century homes built to resemble castles. Over the Middle Ages, when genuine castles were built, they took on a great many forms with many different features, although some, such as curtain walls, arrowslits, and portcullises, were commonplace.\"\n  },\n  {\n    \"concept\": \"Fortress\",\n    \"description\": \"A fortification (also called a fort, fortress, fastness, or stronghold) is a military construction designed for the defense of territories in warfare, and is used to establish rule in a region during peacetime. The term is derived from Latin fortis ('strong') and facere ('to make'). From very early history to modern times, defensive walls have often been necessary for cities to survive in an ever-changing world of invasion and conquest. Some settlements in the Indus Valley Civilization were the first small cities to be fortified. In ancient Greece, large stone walls had been built in Mycenaean Greece, such as the ancient site of Mycenae (known for the huge stone blocks of its 'cyclopean' walls). A Greek phrourion was a fortified collection of buildings used as a military garrison, and is the equivalent of the Roman castellum or fortress. These constructions mainly served the purpose of a watch tower, to guard certain roads, passes, and borders. Though smaller than a real fortress, they acted as a border guard rather than a real strongpoint to watch and maintain the border.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"concept\": \"Shed\",\n    \"description\": \"A shed is typically a simple, single-story roofed structure, often used for storage, for hobbies, or as a workshop, and typically serving as outbuilding, such as in a back garden or on an allotment. Sheds vary considerably in their size and complexity of construction, from simple open-sided ones designed to cover bicycles or garden items to large wood-framed structures with shingled roofs, windows, and electrical outlets. Sheds used on farms or in the industry can be large structures. The main types of shed construction are metal sheathing over a metal frame, plastic sheathing and frame, all-wood construction (the roof may be asphalt shingled or sheathed in tin), and vinyl-sided sheds built over a wooden frame. Small sheds may include a wooden or plastic floor, while more permanent ones may be built on a concrete pad or foundation. Sheds may be lockable to deter theft or entry by children, domestic animals, wildlife, etc.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9907871670904073,\n        \"concept\": \"Outbuildings\",\n        \"description\": \"Outbuildings are structures that are separate from a main building, typically located on a property for purposes such as storage, workshops, or housing animals and equipment. This category includes structures like sheds and barns, which serve specific functions like storing tools, equipment, or livestock.\"\n      },\n      {\n        \"distance\": 1.148880974178631,\n        \"concept\": \"Auxiliary Structures\",\n        \"description\": \"Auxiliary structures are secondary or additional buildings that serve various practical purposes related to a main dwelling or property. This category encompasses structures like tree houses and outbuildings, which provide functional, recreational, or storage spaces, often designed to enhance the usability of the property.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Barn\",\n    \"description\": \"A barn is an agricultural building usually on farms and used for various purposes. In North America, a barn refers to structures that house livestock, including cattle and horses, as well as equipment and fodder, and often grain.[2] As a result, the term barn is often qualified e.g. tobacco barn, dairy barn, cow house, sheep barn, potato barn. In the British Isles, the term barn is restricted mainly to storage structures for unthreshed cereals and fodder, the terms byre or shippon being applied to cow shelters, whereas horses are kept in buildings known as stables.[2][3] In mainland Europe, however, barns were often part of integrated structures known as byre-dwellings (or housebarns in US literature). In addition, barns may be used for equipment storage, as a covered workplace, and for activities such as threshing.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9907871670904073,\n        \"concept\": \"Outbuildings\",\n        \"description\": \"Outbuildings are structures that are separate from a main building, typically located on a property for purposes such as storage, workshops, or housing animals and equipment. This category includes structures like sheds and barns, which serve specific functions like storing tools, equipment, or livestock.\"\n      },\n      {\n        \"distance\": 1.148880974178631,\n        \"concept\": \"Auxiliary Structures\",\n        \"description\": \"Auxiliary structures are secondary or additional buildings that serve various practical purposes related to a main dwelling or property. This category encompasses structures like tree houses and outbuildings, which provide functional, recreational, or storage spaces, often designed to enhance the usability of the property.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Tree house\",\n    \"description\": \"A tree house, tree fort or treeshed, is a platform or building constructed around, next to or among the trunk or branches of one or more mature trees while above ground level. Tree houses can be used for recreation, work space, habitation, a hangout space and observation. People occasionally connect ladders or staircases to get up to the platforms.\",\n    \"categories\": [\n      {\n        \"distance\": 1.148880974178631,\n        \"concept\": \"Auxiliary Structures\",\n        \"description\": \"Auxiliary structures are secondary or additional buildings that serve various practical purposes related to a main dwelling or property. This category encompasses structures like tree houses and outbuildings, which provide functional, recreational, or storage spaces, often designed to enhance the usability of the property.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Castle\",\n    \"description\": \"A castle is a type of fortified structure built during the Middle Ages predominantly by the nobility or royalty and by military orders. Scholars usually consider a castle to be the private fortified residence of a lord or noble. This is distinct from a mansion, palace, and villa, whose main purpose was exclusively for pleasance and are not primarily fortresses but may be fortified.[a] Use of the term has varied over time and, sometimes, has also been applied to structures such as hill forts and 19th- and 20th-century homes built to resemble castles. Over the Middle Ages, when genuine castles were built, they took on a great many forms with many different features, although some, such as curtain walls, arrowslits, and portcullises, were commonplace.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9152435235428339,\n        \"concept\": \"Fortified structures\",\n        \"description\": \"Fortified structures refer to buildings designed to protect from attacks and enhance defense. This category encompasses various forms of military architecture, including castles and fortresses. Castles serve as private residences for nobility or military orders with substantial fortification features, while fortresses are broader military constructions aimed at defending territories and establishing control. Both types share the common purpose of defense against invasion, though they serve different social and functional roles.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Fortress\",\n    \"description\": \"A fortification (also called a fort, fortress, fastness, or stronghold) is a military construction designed for the defense of territories in warfare, and is used to establish rule in a region during peacetime. The term is derived from Latin fortis ('strong') and facere ('to make'). From very early history to modern times, defensive walls have often been necessary for cities to survive in an ever-changing world of invasion and conquest. Some settlements in the Indus Valley Civilization were the first small cities to be fortified. In ancient Greece, large stone walls had been built in Mycenaean Greece, such as the ancient site of Mycenae (known for the huge stone blocks of its 'cyclopean' walls). A Greek phrourion was a fortified collection of buildings used as a military garrison, and is the equivalent of the Roman castellum or fortress. These constructions mainly served the purpose of a watch tower, to guard certain roads, passes, and borders. Though smaller than a real fortress, they acted as a border guard rather than a real strongpoint to watch and maintain the border.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9152435235428339,\n        \"concept\": \"Fortified structures\",\n        \"description\": \"Fortified structures refer to buildings designed to protect from attacks and enhance defense. This category encompasses various forms of military architecture, including castles and fortresses. Castles serve as private residences for nobility or military orders with substantial fortification features, while fortresses are broader military constructions aimed at defending territories and establishing control. Both types share the common purpose of defense against invasion, though they serve different social and functional roles.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  }\n]\n</code></pre></p>"},{"location":"operators/cluster/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"cluster\".</li> <li><code>embedding_keys</code>: A list of keys to use for the embedding that is clustered on</li> <li><code>summary_prompt</code>: The prompt used to summarize a cluster based on its children. Access input variables by iterating over <code>inputs</code> with <code>{% for input in inputs %}</code> and accessing properties with <code>{{input.keyname}}</code>.</li> <li><code>summary_schema</code>: The schema for the summary of each cluster. This is the output schema for the <code>summary_prompt</code> based llm call.</li> </ul>"},{"location":"operators/cluster/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>output_key</code> The name of the output key where the cluster path will be inserted in the items. \"clusters\" <code>model</code> The language model to use Falls back to <code>default_model</code> <code>embedding_model</code> The embedding model to use \"text-embedding-3-small\" <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>sample</code> Number of items to sample for this operation None <code>litellm_completion_kwargs</code> Additional parameters to pass to LiteLLM completion calls. {}"},{"location":"operators/code/","title":"Code Operations","text":"<p>Code operations in DocETL allow you to define transformations using Python code rather than LLM prompts. This is useful when you need deterministic processing, complex calculations, or want to leverage existing Python libraries.</p>"},{"location":"operators/code/#motivation","title":"Motivation","text":"<p>While LLM-powered operations are powerful for natural language tasks, sometimes you need operations that are:</p> <ul> <li>Deterministic and reproducible</li> <li>Integrated with external Python libraries</li> <li>Focused on structured data transformations</li> <li>Math-based or computationally intensive (something an LLM is not good at)</li> </ul> <p>Code operations provide a way to handle these cases efficiently without LLM overhead.</p>"},{"location":"operators/code/#types-of-code-operations","title":"Types of Code Operations","text":""},{"location":"operators/code/#code-map-operation","title":"Code Map Operation","text":"<p>The Code Map operation applies a Python function to each item in your input data independently.</p> Example Code Map Operation <pre><code>- name: extract_keywords\n  type: code_map\n  code: |\n    def transform(doc) -&gt; dict:\n        # Your transformation code here\n        keywords = doc['text'].lower().split()\n        return {\n            'keywords': keywords,\n            'keyword_count': len(keywords)\n        }\n</code></pre> <p>The code must define a <code>transform</code> function that takes a single document as input and returns a dictionary of transformed values.</p>"},{"location":"operators/code/#code-reduce-operation","title":"Code Reduce Operation","text":"<p>The Code Reduce operation aggregates multiple items into a single result using a Python function.</p> Example Code Reduce Operation <pre><code>- name: aggregate_stats\n  type: code_reduce\n  reduce_key: category\n  code: |\n    def transform(items) -&gt; dict:\n        total = sum(item['value'] for item in items)\n        avg = total / len(items)\n        return {\n            'total': total,\n            'average': avg,\n            'count': len(items)\n        }\n</code></pre> <p>The transform function for reduce operations takes a list of items as input and returns a single aggregated result.</p>"},{"location":"operators/code/#code-filter-operation","title":"Code Filter Operation","text":"<p>The Code Filter operation allows you to filter items based on custom Python logic.</p> Example Code Filter Operation <pre><code>- name: filter_valid_entries\n  type: code_filter  \n  code: |\n    def transform(doc) -&gt; bool:\n        # Return True to keep the document, False to filter it out\n        return doc['score'] &gt;= 0.5 and len(doc['text']) &gt; 100\n</code></pre> <p>The transform function should return True for items to keep and False for items to filter out.</p>"},{"location":"operators/code/#configuration","title":"Configuration","text":""},{"location":"operators/code/#required-parameters","title":"Required Parameters","text":"<ul> <li>type: Must be \"code_map\", \"code_reduce\", or \"code_filter\"</li> <li>code: Python code containing the transform function. For map, the function must take a single document as input and return a document (a dictionary). For reduce, the function must take a list of documents as input and return a single aggregated document (a dictionary). For filter, the function must take a single document as input and return a boolean value indicating whether to keep the document.</li> </ul>"},{"location":"operators/code/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default drop_keys List of keys to remove from output (code_map only) None reduce_key Key(s) to group by (code_reduce only) \"_all\" pass_through Pass through unmodified keys from first item in group (code_reduce only) false concurrent_thread_count The number of threads to start the number of logical CPU cores (os.cpu_count()) limit Maximum number of outputs to produce before stopping Processes all data <p>The <code>limit</code> parameter behaves differently for each operation type:</p> <ul> <li>code_map: Limits the number of input documents to process</li> <li>code_filter: Limits the number of documents that pass the filter (outputs)</li> <li>code_reduce: Limits the number of groups to reduce, selecting the smallest groups first (by document count)</li> </ul>"},{"location":"operators/equijoin/","title":"Equijoin Operation (Experimental)","text":"<p>The Equijoin operation in DocETL is an experimental feature designed for joining two datasets based on flexible, LLM-powered criteria. It leverages many of the same techniques as the Resolve operation, but applies them to the task of joining datasets rather than deduplicating within a single dataset.</p>"},{"location":"operators/equijoin/#motivation","title":"Motivation","text":"<p>While traditional database joins rely on exact matches, real-world data often requires more nuanced joining criteria. Equijoin allows for joins based on semantic similarity or complex conditions, making it ideal for scenarios where exact matches are impossible or undesirable.</p>"},{"location":"operators/equijoin/#example-matching-job-candidates-to-job-postings","title":"\ud83d\ude80 Example: Matching Job Candidates to Job Postings","text":"<p>Let's explore a practical example of using the Equijoin operation to match job candidates with suitable job postings based on skills and experience.</p> <pre><code>- name: match_candidates_to_jobs\n  type: equijoin\n  comparison_prompt: |\n    Compare the following job candidate and job posting:\n\n    Candidate Skills: {{ left.skills }}\n    Candidate Experience: {{ left.years_experience }}\n\n    Job Required Skills: {{ right.required_skills }}\n    Job Desired Experience: {{ right.desired_experience }}\n\n    Is this candidate a good match for the job? Consider both the overlap in skills and the candidate's experience level. Respond with \"True\" if it's a good match, or \"False\" if it's not a suitable match.\n</code></pre> <p>This Equijoin operation matches job candidates to job postings:</p> <ol> <li>It uses the <code>comparison_prompt</code> to determine if a candidate is a good match for a job.</li> <li>The operation can be optimized to use efficient blocking rules, reducing the number of comparisons.</li> </ol> <p>Jinja2 Syntax with left and right</p> <p>The prompt template uses Jinja2 syntax, allowing you to reference input fields directly (e.g., <code>left.skills</code>). You can reference the left and right documents using <code>left</code> and <code>right</code> respectively.</p> <p>Automatic Blocking</p> <p>If you don't specify any blocking configuration (<code>blocking_threshold</code>, <code>blocking_conditions</code>, or <code>limit_comparisons</code>), the Equijoin operation will automatically compute an optimal embedding-based blocking threshold at runtime. It samples pairs from your data, runs LLM comparisons on the sample, and finds a threshold that achieves 95% recall by default. You can adjust this with the <code>blocking_target_recall</code> parameter.</p>"},{"location":"operators/equijoin/#blocking","title":"Blocking","text":"<p>Like the Resolve operation, Equijoin supports blocking techniques to improve efficiency. For details on how blocking works and how to implement it, please refer to the Blocking section in the Resolve operation documentation.</p>"},{"location":"operators/equijoin/#adding-blocking-rules","title":"Adding Blocking Rules","text":"<p>Equijoin lets you specify explicit blocking logic to skip record pairs that are obviously unrelated before any LLM calls are made.</p>"},{"location":"operators/equijoin/#blocking_keys","title":"<code>blocking_keys</code>","text":"<p>Provide one or more field names for each side of the join. The selected values are concatenated and embedded; the cosine similarity of the left vs. right embeddings is then compared against <code>blocking_threshold</code> (defaults to <code>1.0</code>). If the similarity meets or exceeds that threshold, the pair moves on to the <code>comparison_prompt</code>; otherwise it is skipped. If you omit <code>blocking_keys</code>, all key\u2013value pairs of each record are embedded by default.</p> <pre><code>blocking_keys:\n  left:\n    - medicine\n  right:\n    - extracted_medications\n</code></pre>"},{"location":"operators/equijoin/#blocking_threshold","title":"<code>blocking_threshold</code>","text":"<p>Optionally set a numeric <code>blocking_threshold</code> (0 \u2013 1) representing the minimum cosine similarity (computed with the selected <code>embedding_model</code>) that the concatenated blocking keys must achieve to be considered a candidate pair. Anything below the threshold is filtered out without invoking the LLM.</p> <pre><code>blocking_threshold: 0.35\nembedding_model: text-embedding-3-small\n</code></pre> <p>A full Equijoin step combining both ideas might look like:</p> <pre><code>- name: join_meds_transcripts\n  type: equijoin\n  blocking_keys:\n    left:\n      - medicine\n    right:\n      - extracted_medications\n  blocking_threshold: 0.3535\n  embedding_model: text-embedding-3-small\n  comparison_prompt: |\n    Compare the following medication names:\n\n    {{ left.medicine }}\n\n    {{ right.extracted_medications }}\n\n    Determine if these entries refer to the same medication.\n</code></pre>"},{"location":"operators/equijoin/#auto-generating-rules-experimental","title":"Auto-generating Rules (Experimental)","text":"<p><code>docetl build pipeline.yaml</code> can call the Optimizer to propose <code>blocking_keys</code> and an appropriate <code>blocking_threshold</code> based on a sample of your data. This feature is experimental; always review the suggested rules to ensure they do not exclude valid matches.</p>"},{"location":"operators/equijoin/#parameters","title":"Parameters","text":"<p>Equijoin shares many parameters with the Resolve operation. For a detailed list of required and optional parameters, please see the Parameters section in the Resolve operation documentation.</p>"},{"location":"operators/equijoin/#equijoin-specific-parameters","title":"Equijoin-Specific Parameters","text":"Parameter Description Default <code>limits</code> Maximum matches for each left/right item: <code>{\"left\": n, \"right\": m}</code> No limit <code>blocking_keys</code> Keys for embedding blocking: <code>{\"left\": [...], \"right\": [...]}</code> All keys from each dataset <code>blocking_threshold</code> Embedding similarity threshold for considering pairs Auto-computed if not set <code>blocking_target_recall</code> Target recall when auto-computing blocking threshold (0.0 to 1.0) 0.95 <p>Key differences from Resolve:</p> <ul> <li><code>resolution_prompt</code> is not used in Equijoin.</li> <li><code>blocking_keys</code> uses a dict with <code>left</code> and <code>right</code> keys instead of a simple list.</li> </ul>"},{"location":"operators/equijoin/#incorporating-into-a-pipeline","title":"Incorporating Into a Pipeline","text":"<p>Here's an example of how to incorporate the Equijoin operation into a pipeline using the job candidate matching scenario:</p> <pre><code>model: gpt-4o-mini\n\ndatasets:\n  candidates:\n    type: file\n    path: /path/to/candidates.json\n  job_postings:\n    type: file\n    path: /path/to/job_postings.json\n\noperations:\n  - name: match_candidates_to_jobs:\n    type: equijoin\n    comparison_prompt: |\n      Compare the following job candidate and job posting:\n\n      Candidate Skills: {{ left.skills }}\n      Candidate Experience: {{ left.years_experience }}\n\n      Job Required Skills: {{ right.required_skills }}\n      Job Desired Experience: {{ right.desired_experience }}\n\n      Is this candidate a good match for the job? Consider both the overlap in skills and the candidate's experience level. Respond with \"True\" if it's a good match, or \"False\" if it's not a suitable match.\n\npipeline:\n  steps:\n    - name: match_candidates_to_jobs\n      operations:\n        - match_candidates_to_jobs:\n            left: candidates\n            right: job_postings\n\n  output:\n    type: file\n    path: \"/path/to/matched_candidates_jobs.json\"\n</code></pre> <p>This pipeline configuration demonstrates how to use the Equijoin operation to match job candidates with job postings. The pipeline reads candidate and job posting data from JSON files, performs the matching using the defined comparison prompt, and outputs the results to a new JSON file.</p>"},{"location":"operators/equijoin/#best-practices","title":"Best Practices","text":"<ol> <li>Leverage the Optimizer: Use <code>docetl build pipeline.yaml</code> to automatically generate efficient blocking rules for your Equijoin operation.</li> <li>Craft Thoughtful Comparison Prompts: Design prompts that effectively determine whether two records should be joined based on your specific use case.</li> <li>Balance Precision and Recall: When optimizing, consider the trade-off between catching all potential matches and reducing unnecessary comparisons.</li> <li>Mind Resource Constraints: Use <code>limit_comparisons</code> if you need to cap the total number of comparisons for large datasets.</li> <li>Iterate and Refine: Start with a small sample of your data to test and refine your join criteria before running on the full dataset.</li> </ol> <p>For additional best practices that apply to both Resolve and Equijoin operations, see the Best Practices section in the Resolve operation documentation.</p>"},{"location":"operators/extract/","title":"Extract Operation","text":"<p>Why use Extract instead of Map?</p> <p>The Extract operation is specifically optimized for isolating portions of source text without synthesis or summarization. Unlike Map operations, which typically transform content, Extract pulls out specific sections verbatim. This provides three key advantages:</p> <ol> <li>Cost efficiency: Lower output token costs when extracting large chunks of text</li> <li>Precision: Extracts exact text without introducing LLM interpretation or potential hallucination</li> <li>Simplified workflow: No need to define output schemas - extractions maintain the original text format</li> </ol> <p>The Extract operation identifies and extracts specific sections of text from documents based on provided criteria. It's particularly useful for isolating relevant content from larger documents for further processing or analysis.</p>"},{"location":"operators/extract/#example-extracting-key-findings-from-research-reports","title":"Example: Extracting Key Findings from Research Reports","text":"<p>Here's a practical example of using the Extract operation to pull out key findings from research reports:</p> <pre><code>- name: findings\n  type: extract\n  prompt: |\n    Extract all sections that discuss key findings, results, or conclusions from this research report.\n    Focus on paragraphs that:\n    - Summarize experimental outcomes\n    - Present statistical results\n    - Describe discovered insights\n    - State conclusions drawn from the research\n\n    Only extract the most important and substantive findings.\n  document_keys: [\"report_text\"]\n  model: \"gpt-4.1-mini\"\n</code></pre> <p>This operation converts text into a line-numbered format, uses an LLM to identify relevant content, and extracts the specified text ranges. The extracted content is added to the document with the suffix \"_extracted_findings\".</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"report_id\": \"R-2023-001\",\n    \"report_text\": \"EXPERIMENTAL METHODS\\n\\nThe study utilized a mixed-methods approach combining quantitative surveys (n=230) and qualitative interviews (n=42) with participants from diverse demographic backgrounds. Data collection occurred between January and March 2023.\\n\\nRESULTS\\n\\nThe analysis revealed three primary patterns of user engagement. First, 68% of participants reported daily interaction with the platform, significantly higher than previous industry benchmarks (p&lt;0.01). Second, user retention showed strong correlation with personalization features (r=0.72). Finally, demographic factors such as age and technical proficiency were not significant predictors of engagement, contradicting prior research in this domain.\\n\\nDISCUSSION\\n\\nThese findings suggest that platform design priorities should emphasize personalization capabilities over demographic targeting. The high daily engagement rates indicate market readiness for similar applications, while the lack of demographic effects points to broad accessibility across user segments.\\n\\nLIMITATIONS\\n\\nThe study was limited by its focus on early adopters, which may not represent the broader potential user base. Additionally, the three-month timeframe may not capture seasonal variations in user behavior.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"report_id\": \"R-2023-001\",\n    \"report_text\": \"EXPERIMENTAL METHODS\\n\\nThe study utilized a mixed-methods approach combining quantitative surveys (n=230) and qualitative interviews (n=42) with participants from diverse demographic backgrounds. Data collection occurred between January and March 2023.\\n\\nRESULTS\\n\\nThe analysis revealed three primary patterns of user engagement. First, 68% of participants reported daily interaction with the platform, significantly higher than previous industry benchmarks (p&lt;0.01). Second, user retention showed strong correlation with personalization features (r=0.72). Finally, demographic factors such as age and technical proficiency were not significant predictors of engagement, contradicting prior research in this domain.\\n\\nDISCUSSION\\n\\nThese findings suggest that platform design priorities should emphasize personalization capabilities over demographic targeting. The high daily engagement rates indicate market readiness for similar applications, while the lack of demographic effects points to broad accessibility across user segments.\\n\\nLIMITATIONS\\n\\nThe study was limited by its focus on early adopters, which may not represent the broader potential user base. Additionally, the three-month timeframe may not capture seasonal variations in user behavior.\",\n    \"report_text_extracted_findings\": \"The analysis revealed three primary patterns of user engagement. First, 68% of participants reported daily interaction with the platform, significantly higher than previous industry benchmarks (p&lt;0.01). Second, user retention showed strong correlation with personalization features (r=0.72). Finally, demographic factors such as age and technical proficiency were not significant predictors of engagement, contradicting prior research in this domain.\\n\\nThese findings suggest that platform design priorities should emphasize personalization capabilities over demographic targeting. The high daily engagement rates indicate market readiness for similar applications, while the lack of demographic effects points to broad accessibility across user segments.\"\n  }\n]\n</code></pre></p>"},{"location":"operators/extract/#output-formats","title":"Output Formats","text":"<p>The Extract operation offers two output formats controlled by the <code>format_extraction</code> parameter:</p>"},{"location":"operators/extract/#string-format-default","title":"String Format (Default)","text":"<p>With <code>format_extraction: true</code>, extracted text segments are joined with newlines into a single string:</p> <pre><code>- name: findings\n  type: extract\n  prompt: \"Extract the key findings from this research report.\"\n  document_keys: [\"report_text\"]\n  format_extraction: true  # Default setting\n</code></pre> <p>The resulting output combines all extractions:</p> <pre><code>{\n  \"report_id\": \"R-2023-001\",\n  \"report_text\": \"... original text ...\",\n  \"report_text_extracted_findings\": \"Finding 1 about daily engagement rates...\\n\\nFinding 2 about personalization features...\"\n}\n</code></pre> <p>This format works well for human readability, further LLM processing, and when extractions should be treated as a coherent unit.</p>"},{"location":"operators/extract/#list-format","title":"List Format","text":"<p>With <code>format_extraction: false</code>, each extracted text segment remains separate in a list:</p> <pre><code>- name: findings\n  type: extract\n  prompt: \"Extract the key findings from this research report.\"\n  document_keys: [\"report_text\"]\n  format_extraction: false\n</code></pre> <p>The resulting output preserves each extraction as a distinct item:</p> <pre><code>{\n  \"report_id\": \"R-2023-001\",\n  \"report_text\": \"... original text ...\",\n  \"report_text_extracted_findings\": [\n    \"Finding 1 about daily engagement rates...\",\n    \"Finding 2 about personalization features...\"\n  ]\n}\n</code></pre> <p>This format is better for individual processing of extractions, counting distinct items, or creating structured data from separate extractions.</p>"},{"location":"operators/extract/#extraction-strategies","title":"Extraction Strategies","text":"<p>The Extract operation offers two main strategies:</p>"},{"location":"operators/extract/#line-number-strategy","title":"Line Number Strategy","text":"<p>This strategy reformats the input text with line numbers, then asks the LLM to identify relevant line ranges. The system extracts those specific ranges, removes line number prefixes, and eliminates duplicates. This works well for extracting multi-line passages or entire sections.</p>"},{"location":"operators/extract/#regex-strategy","title":"Regex Strategy","text":"<p>This strategy asks the LLM to generate regex patterns matching the desired content. The system applies these patterns to find matches in the original text. This works well for extracting structured data like dates, codes, or specific formatted information.</p>"},{"location":"operators/extract/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: Unique name for the operation</li> <li><code>type</code>: Must be \"extract\"</li> <li><code>prompt</code>: Instructions specifying what content to extract. This does not need to be a Jinja template.</li> <li><code>document_keys</code>: List of document fields containing text to process</li> </ul>"},{"location":"operators/extract/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>model</code> Language model to use Falls back to <code>default_model</code> <code>extraction_method</code> \"line_number\" or \"regex\" \"line_number\" <code>format_extraction</code> Join with newlines (<code>true</code>) or keep as list (<code>false</code>) <code>true</code> <code>extraction_key_suffix</code> Suffix for output field names \"extracted\" <code>timeout</code> Timeout for LLM calls in seconds 120 <code>skip_on_error</code> Continue processing if errors occur false <code>litellm_completion_kwargs</code> Additional parameters for LiteLLM calls {} <code>limit</code> Maximum number of documents to extract from before stopping Processes all data <code>retriever</code> Name of a retriever to use for RAG. See Retrievers. None <code>save_retriever_output</code> If true, saves the retrieved context to <code>_&lt;operation_name&gt;_retrieved_context</code> in the output. False <p>When <code>limit</code> is set, Extract only reformats and submits the first N documents. This is handy when the upstream dataset is large and you want to cap cost while previewing results.</p>"},{"location":"operators/extract/#best-practices","title":"Best Practices","text":"<p>Create specific extraction prompts with clear criteria about what content to extract or exclude. Choose the appropriate extraction method based on your needs:</p> <ul> <li>Use <code>line_number</code> for extracting paragraphs, sections, or content spanning multiple lines</li> <li>Use <code>regex</code> for extracting specific patterns like dates, codes, or formatted data</li> </ul> <p>Process only document fields containing relevant text, and consider enabling <code>skip_on_error</code> for batch processing where individual failures shouldn't halt the entire operation.</p> <p>Format your output based on downstream requirements: - Use the default string format when the extractions form a coherent whole - Use the list format when each extraction needs individual processing or analysis</p>"},{"location":"operators/extract/#use-cases","title":"Use Cases","text":"<p>The Extract operation is valuable for:</p> <ul> <li>Document summarization - extracting executive summaries or key findings</li> <li>Data extraction - isolating dates, measurements, or specific values from text</li> <li>Content filtering - pulling relevant sections from lengthy documents</li> <li>Evidence gathering - collecting specific statements on given topics</li> <li>Preprocessing - creating focused inputs for downstream analysis </li> </ul>"},{"location":"operators/filter/","title":"Filter Operation","text":"<p>The Filter operation in DocETL is used to selectively process data items based on specific conditions. It behaves similarly to the Map operation, but with a key difference: items that evaluate to false are filtered out of the dataset, allowing you to include or exclude data points from further processing in your pipeline.</p>"},{"location":"operators/filter/#motivation","title":"Motivation","text":"<p>Filtering is crucial when you need to:</p> <ul> <li>Focus on the most relevant data points</li> <li>Remove noise or irrelevant information from your dataset</li> <li>Create subsets of data for specialized analysis</li> <li>Optimize downstream processing by reducing data volume</li> </ul>"},{"location":"operators/filter/#example-filtering-high-impact-news-articles","title":"\ud83d\ude80 Example: Filtering High-Impact News Articles","text":"<p>Let's look at a practical example of using the Filter operation to identify high-impact news articles based on certain criteria.</p> <pre><code>- name: filter_high_impact_articles\n  type: filter\n  prompt: |\n    Analyze the following news article:\n    Title: \"{{ input.title }}\"\n    Content: \"{{ input.content }}\"\n\n    Determine if this article is high-impact based on the following criteria:\n    1. Covers a significant global or national event\n    2. Has potential long-term consequences\n    3. Affects a large number of people\n    4. Is from a reputable source\n\n    Respond with 'true' if the article meets at least 3 of these criteria, otherwise respond with 'false'.\n\n  output:\n    schema:\n      is_high_impact: boolean\n\n  model: gpt-4-turbo\n  validate:\n    - isinstance(output[\"is_high_impact\"], bool)\n</code></pre> <p>This Filter operation processes news articles and determines whether they are \"high-impact\" based on specific criteria. Unlike a Map operation, which would process all articles and add an \"is_high_impact\" field to each, this Filter operation will only pass through articles that meet the criteria, effectively removing low-impact articles from the dataset.</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"title\": \"Global Climate Summit Reaches Landmark Agreement\",\n    \"content\": \"In a historic move, world leaders at the Global Climate Summit have unanimously agreed to reduce carbon emissions by 50% by 2030. This unprecedented agreement involves all major economies and sets binding targets for renewable energy adoption, reforestation, and industrial emissions reduction. Experts hail this as a turning point in the fight against climate change, with potential far-reaching effects on global economies, energy systems, and everyday life for billions of people.\"\n  },\n  {\n    \"title\": \"Local Bakery Wins Best Croissant Award\",\n    \"content\": \"Downtown's favorite bakery, 'The Crusty Loaf', has been awarded the title of 'Best Croissant' in the annual City Food Festival. Owner Maria Garcia attributes the win to their use of imported French butter and a secret family recipe. Local food critics praise the bakery's commitment to traditional baking methods.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"title\": \"Global Climate Summit Reaches Landmark Agreement\",\n    \"content\": \"In a historic move, world leaders at the Global Climate Summit have unanimously agreed to reduce carbon emissions by 50% by 2030. This unprecedented agreement involves all major economies and sets binding targets for renewable energy adoption, reforestation, and industrial emissions reduction. Experts hail this as a turning point in the fight against climate change, with potential far-reaching effects on global economies, energy systems, and everyday life for billions of people.\"\n  }\n]\n</code></pre></p> <p>This example demonstrates how the Filter operation distinguishes between high-impact news articles and those of more local or limited significance. The climate summit article is retained in the dataset due to its global significance, long-term consequences, and wide-ranging effects. The local bakery story, while interesting, doesn't meet the criteria for a high-impact article and is filtered out of the dataset.</p>"},{"location":"operators/filter/#configuration","title":"Configuration","text":""},{"location":"operators/filter/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"filter\".</li> <li><code>prompt</code>: The prompt template to use for the filtering condition. Access input variables with <code>input.keyname</code>.</li> <li><code>output</code>: Schema definition for the output from the LLM. It must include only one field, a boolean field.</li> </ul>"},{"location":"operators/filter/#optional-parameters","title":"Optional Parameters","text":"<p>See map optional parameters for additional configuration options, including <code>batch_prompt</code> and <code>max_batch_size</code>.</p>"},{"location":"operators/filter/#limiting-filtered-outputs","title":"Limiting filtered outputs","text":"<p><code>limit</code> behaves slightly differently for filter operations than for map operations. Because filter drops documents whose predicate evaluates to <code>false</code>, the limit counts only the documents that would be retained (i.e., the ones whose boolean output is <code>true</code>). DocETL will continue evaluating additional inputs until it has collected <code>limit</code> passing documents and then stop scheduling further LLM calls. This ensures you can request \u201cthe first N matches\u201d without paying to score the entire dataset.</p> <p>Validation</p> <p>For more details on validation techniques and implementation, see operators.</p>"},{"location":"operators/filter/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Criteria: Define clear and specific criteria for filtering in your prompt.</li> <li>Boolean Output: Ensure your prompt guides the LLM to produce a clear boolean output.</li> <li>Data Flow Awareness: Remember that unlike Map, Filter will reduce the size of your dataset. Ensure this aligns with your pipeline's objectives.</li> </ol>"},{"location":"operators/gather/","title":"Gather Operation","text":"<p>The Gather operation in DocETL is designed to maintain context when processing divided documents. It complements the Split operation by adding contextual information from surrounding chunks to each segment.</p>"},{"location":"operators/gather/#motivation","title":"Motivation","text":"<p>When splitting long documents, such as complex legal contracts or court transcripts, individual chunks often lack sufficient context for accurate analysis or processing. This can lead to several challenges:</p> <ul> <li>Loss of reference information (e.g., terms defined in earlier sections)</li> <li>Incomplete representation of complex clauses that span multiple chunks</li> <li>Difficulty in understanding the broader document structure</li> <li>Missing important context from preambles or introductory sections</li> </ul> <p>Context Challenge in Legal Documents</p> <p>Imagine a lengthy merger agreement split into chunks. A single segment might contain clauses referencing \"the Company\" or \"Effective Date\" without clearly defining these terms. Without context from previous chunks, it becomes challenging to interpret the legal implications accurately.</p>"},{"location":"operators/gather/#how-gather-works","title":"How Gather Works","text":"<p>The Gather operation addresses these challenges by:</p> <ol> <li>Identifying relevant surrounding chunks (peripheral context)</li> <li>Adding this context to each chunk</li> <li>Preserving document structure information</li> </ol>"},{"location":"operators/gather/#peripheral-context","title":"Peripheral Context","text":"<p>Peripheral context refers to the surrounding text or information that helps provide a more complete understanding of a specific chunk of content. In legal documents, this can include:</p> <ul> <li>Preceding text that introduces key terms, parties, or conditions</li> <li>Following text that elaborates on clauses presented in the current chunk</li> <li>Document structure information, such as article or section headers</li> <li>Summarized versions of nearby chunks for efficient context provision</li> </ul>"},{"location":"operators/gather/#document-structure","title":"Document Structure","text":"<p>The Gather operation can maintain document structure through header hierarchies. This is particularly useful for preserving the overall structure of complex legal documents like contracts, agreements, or regulatory filings.</p>"},{"location":"operators/gather/#example-enhancing-context-in-legal-document-analysis","title":"\ud83d\ude80 Example: Enhancing Context in Legal Document Analysis","text":"<p>Let's walk through an example of using the Gather operation to process a long merger agreement.</p>"},{"location":"operators/gather/#step-1-extract-metadata-map-operation-before-splitting","title":"Step 1: Extract Metadata (Map operation before splitting)","text":"<p>First, we extract important metadata from the full document:</p> <pre><code>- name: extract_metadata\n  type: map\n  prompt: |\n    Extract the following metadata from the merger agreement:\n    1. Agreement Date\n    2. Parties involved\n    3. Total value of the merger (if specified)\n\n    Agreement text:\n    {{ input.agreement_text }}\n\n    Return the extracted information in a structured format.\n  output:\n    schema:\n      agreement_date: string\n      parties: list[string]\n      merger_value: string\n</code></pre>"},{"location":"operators/gather/#step-2-split-operation","title":"Step 2: Split Operation","text":"<p>Next, we split the document into manageable chunks:</p> <pre><code>- name: split_merger_agreement\n  type: split\n  split_key: agreement_text\n  method: token_count\n  method_kwargs:\n    num_tokens: 1000\n</code></pre>"},{"location":"operators/gather/#step-3-extract-headers-map-operation","title":"Step 3: Extract Headers (Map operation)","text":"<p>We extract headers from each chunk:</p> <pre><code>- name: extract_headers\n  type: map\n  input:\n    - agreement_text_chunk\n  prompt: |\n    Extract any section headers from the following merger agreement chunk:\n    {{ input.agreement_text_chunk }}\n    Return the headers as a list, preserving their hierarchy.\n  output:\n    schema:\n      headers: \"list[{header: string, level: integer}]\"\n</code></pre>"},{"location":"operators/gather/#step-4-gather-operation","title":"Step 4: Gather Operation","text":"<p>Now, we apply the Gather operation:</p> <pre><code>- name: context_gatherer\n  type: gather\n  content_key: agreement_text_chunk\n  doc_id_key: split_merger_agreement_id\n  order_key: split_merger_agreement_chunk_num\n  peripheral_chunks:\n    previous:\n      middle:\n        content_key: agreement_text_chunk_summary\n      tail:\n        content_key: agreement_text_chunk\n    next:\n      head:\n        count: 1\n        content_key: agreement_text_chunk\n  doc_header_key: headers\n</code></pre>"},{"location":"operators/gather/#step-5-analyze-chunks-map-operation-after-gather","title":"Step 5: Analyze Chunks (Map operation after Gather)","text":"<p>Finally, we analyze each chunk with its gathered context:</p> <pre><code>- name: analyze_chunks\n  type: map\n  input:\n    - agreement_text_chunk_rendered\n    - agreement_date\n    - parties\n    - merger_value\n  prompt: |\n    Analyze the following chunk of a merger agreement, considering the provided metadata:\n\n    Agreement Date: {{ input.agreement_date }}\n    Parties: {{ input.parties | join(', ') }}\n    Merger Value: {{ input.merger_value }}\n\n    Chunk content:\n    {{ input.agreement_text_chunk_rendered }}\n\n    Provide a summary of key points and any potential legal implications in this chunk.\n  output:\n    schema:\n      summary: string\n      legal_implications: list[string]\n</code></pre> <p>This configuration:</p> <ol> <li>Extracts important metadata from the full document before splitting</li> <li>Splits the document into manageable chunks</li> <li>Extracts headers from each chunk</li> <li>Gathers context for each chunk, including:</li> <li>Summaries of the chunks before the previous chunk</li> <li>The full content of the previous chunk</li> <li>The full content of the current chunk</li> <li>The full content of the next chunk</li> <li>Extracted headers for levels directly above headers in the current chunk, for structural context</li> <li>Analyzes each chunk with its gathered context and the extracted metadata</li> </ol>"},{"location":"operators/gather/#configuration","title":"Configuration","text":"<p>The Gather operation includes several key components:</p> <ul> <li><code>type</code>: Always set to \"gather\"</li> <li><code>doc_id_key</code>: Identifies chunks from the same original document</li> <li><code>order_key</code>: Specifies the sequence of chunks within a group</li> <li><code>content_key</code>: Indicates the field containing the chunk content</li> <li><code>peripheral_chunks</code>: Specifies how to include context from surrounding chunks</li> <li><code>doc_header_key</code> (optional): Denotes a field representing extracted headers for each chunk</li> <li><code>sample</code> (optional): Number of samples to use for the operation</li> </ul>"},{"location":"operators/gather/#peripheral-chunks-configuration","title":"Peripheral Chunks Configuration","text":"<p>The <code>peripheral_chunks</code> configuration in the Gather operation is highly flexible, allowing users to precisely control how context is added to each chunk. This configuration determines which surrounding chunks are included and how they are presented.</p>"},{"location":"operators/gather/#structure","title":"Structure","text":"<p>The <code>peripheral_chunks</code> configuration is divided into two main sections:</p> <ol> <li><code>previous</code>: Defines how chunks preceding the current chunk are included.</li> <li><code>next</code>: Defines how chunks following the current chunk are included.</li> </ol> <p>Each of these sections can contain up to three subsections:</p> <ul> <li><code>head</code>: The first chunk(s) in the section.</li> <li><code>middle</code>: Chunks between the <code>head</code> and <code>tail</code> sections.</li> <li><code>tail</code>: The last chunk(s) in the section.</li> </ul>"},{"location":"operators/gather/#configuration-options","title":"Configuration Options","text":"<p>For each subsection, you can specify:</p> <ul> <li><code>count</code>: The number of chunks to include (for <code>head</code> and <code>tail</code> only).</li> <li><code>content_key</code>: The key in the chunk data that contains the content to use.</li> </ul>"},{"location":"operators/gather/#example-configuration","title":"Example Configuration","text":"<pre><code>peripheral_chunks:\n  previous:\n    head:\n      count: 1\n      content_key: full_content\n    middle:\n      content_key: summary_content\n    tail:\n      count: 2\n      content_key: full_content\n  next:\n    head:\n      count: 1\n      content_key: full_content\n</code></pre> <p>This configuration would:</p> <ol> <li>Include the full content of the very first chunk.</li> <li>Include summaries of all chunks between the <code>head</code> and <code>tail</code> of the previous section.</li> <li>Include the full content of 2 chunks immediately before the current chunk.</li> <li>Include the full content of 1 chunk immediately after the current chunk.</li> </ol>"},{"location":"operators/gather/#behavior-details","title":"Behavior Details","text":"<ol> <li> <p>Content Selection:    If a <code>content_key</code> is specified that's different from the main content key, it's treated as a summary. This is useful for including condensed versions of chunks in the <code>middle</code> section to save space. If no <code>content_key</code> is specified, it defaults to the main content key of the operation.</p> </li> <li> <p>Chunk Ordering:    For the <code>previous</code> section, chunks are processed in reverse order (from the current chunk towards the beginning of the document). For the <code>next</code> section, chunks are processed in forward order.</p> </li> <li> <p>Skipped Content:    If there are gaps between included chunks, the operation inserts a note indicating how many characters were skipped. Example: <code>[... 5000 characters skipped ...]</code></p> </li> <li> <p>Chunk Labeling:    Each included chunk is labeled with its order number and whether it's a summary. Example: <code>[Chunk 5 (Summary)]</code> or <code>[Chunk 6]</code></p> </li> </ol>"},{"location":"operators/gather/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Balance Context and Conciseness: Use full content for immediate context (<code>head</code>) and summaries for <code>middle</code> sections to provide context without overwhelming the main content.</p> </li> <li> <p>Adapt to Document Structure: Adjust the <code>count</code> for <code>head</code> and <code>tail</code> based on the typical length of your document sections.</p> </li> <li> <p>Use Asymmetric Configurations: You might want more previous context than next context, or vice versa, depending on your specific use case.</p> </li> <li> <p>Consider Performance: Including too much context can increase processing time and token usage. Use summaries and selective inclusion to optimize performance.</p> </li> </ol> <p>By leveraging this flexible configuration, you can tailor the Gather operation to provide the most relevant context for your specific document processing needs, balancing completeness with efficiency.</p>"},{"location":"operators/gather/#output","title":"Output","text":"<p>The Gather operation adds a new field to each input document, named by appending \"_rendered\" to the <code>content_key</code>. This field contains:</p> <ol> <li>The reconstructed header hierarchy (if applicable)</li> <li>Previous context (if any)</li> <li>The main chunk, clearly marked</li> <li>Next context (if any)</li> <li>Indications of skipped content between contexts</li> </ol> <p>Sample Output for Merger Agreement</p> <pre><code>agreement_text_chunk_rendered: |\n  _Current Section:_ # Article 5: Representations and Warranties &gt; ## 5.1 Representations and Warranties of the Company\n\n  --- Previous Context ---\n  [Chunk 17] ... summary of earlier sections on definitions and parties ...\n  [... 500 characters skipped ...]\n  [Chunk 18] The Company hereby represents and warrants to Parent and Merger Sub as follows, except as set forth in the Company Disclosure Schedule:\n  --- End Previous Context ---\n\n  --- Begin Main Chunk ---\n  5.1.1 Organization and Qualification. The Company is duly organized, validly existing, and in good standing under the laws of its jurisdiction of organization and has all requisite corporate power and authority to own, lease, and operate its properties and to carry on its business as it is now being conducted...\n  --- End Main Chunk ---\n\n  --- Next Context ---\n  [Chunk 20] 5.1.2 Authority Relative to This Agreement. The Company has all necessary corporate power and authority to execute and deliver this Agreement, to perform its obligations hereunder, and to consummate the Merger...\n  [... 750 characters skipped ...]\n  --- End Next Context ---\n</code></pre>"},{"location":"operators/gather/#handling-document-structure","title":"Handling Document Structure","text":"<p>A key feature of the Gather operation is its ability to maintain document structure through header hierarchies. This is particularly useful for preserving the overall structure of complex documents like legal contracts, technical manuals, or research papers.</p>"},{"location":"operators/gather/#how-header-handling-works","title":"How Header Handling Works","text":"<ol> <li>Headers are typically extracted from each chunk using a Map operation after the Split but before the Gather.</li> <li>The Gather operation uses these extracted headers to reconstruct the relevant header hierarchy for each chunk.</li> <li>When rendering a chunk, the operation includes all the most recent headers from higher levels found in previous chunks.</li> <li>This ensures that each rendered chunk includes a complete \"path\" of headers leading to its content, preserving the document's overall structure and context.</li> </ol>"},{"location":"operators/gather/#example-header-handling-in-legal-contracts","title":"Example: Header Handling in Legal Contracts","text":"<p>Let's look at an example of how the Gather operation handles document headers in the context of legal contracts:</p> <p></p> <p>In this figure:</p> <ol> <li>We see two chunks (18 and 20) from a 74-page legal contract.</li> <li>Each chunk goes through a Map operation to extract headers.</li> <li>For Chunk 18, a level 1 header \"Grant of License\" is extracted.</li> <li>For Chunk 20, a level 3 header \"ctDNA Licenses\" is extracted.</li> <li>When rendering Chunk 20, the Gather operation includes:</li> <li>The level 1 header from Chunk 18 (\"Grant of License\")</li> <li>A level 2 header from Chunk 19 (not pictured, but included in the rendered output)</li> <li>The current level 3 header from Chunk 20 (\"ctDNA Licenses\")</li> </ol> <p>This hierarchical structure provides crucial context for understanding the content of Chunk 20, even though the higher-level headers are not directly present in that chunk.</p> <p>Importance of Header Hierarchy</p> <p>By maintaining the header hierarchy, the Gather operation ensures that each chunk is placed in its proper context within the overall document structure. This is especially crucial for complex documents where understanding the relationship between different sections is key to accurate analysis or processing.</p>"},{"location":"operators/gather/#best-practices_1","title":"Best Practices","text":"<ol> <li> <p>Extract Metadata Before Splitting: Run a map operation on the full document before splitting to extract any metadata that could be useful when processing any chunk (e.g., agreement dates, parties involved). Reference this metadata in subsequent map operations after the gather step.</p> </li> <li> <p>Balance Context and Efficiency: For ultra-long documents, consider using summarized versions of chunks in the \"middle\" sections to strike a balance between providing context and managing the overall size of the processed data.</p> </li> <li> <p>Preserve Document Structure: Utilize the <code>doc_header_key</code> parameter to include relevant structural information from the original document, which can be important for understanding the context of complex or structured content.</p> </li> <li> <p>Tailor Context to Your Task: Adjust the <code>peripheral_chunks</code> configuration based on the specific needs of your analysis. Some tasks may require more preceding context, while others might benefit from more following context.</p> </li> <li> <p>Combine with Other Operations: The Gather operation is most powerful when used in conjunction with Split, Map (for summarization or header extraction), and subsequent analysis operations to process long documents effectively.</p> </li> <li> <p>Consider Performance: Be mindful of the increased token count when adding context. Adjust your downstream operations accordingly to handle the larger input sizes, and use summarized context where appropriate to manage token usage.</p> </li> <li> <p>Use <code>next</code> Sparingly: The <code>next</code> parameter in the Gather operation should be used judiciously. It's primarily beneficial in specific scenarios:</p> <ul> <li>When dealing with structured data or tables where the next chunk provides essential context for understanding the current chunk.</li> <li>In cases where the end of a chunk might cut off a sentence or important piece of information that continues in the next chunk.</li> </ul> <p>For most text-based documents, focusing on the <code>prev</code> context is usually sufficient. Overuse of <code>next</code> can lead to unnecessary token consumption and potential redundancy in the gathered output.</p> <p>When to Use <code>next</code></p> <p>Consider using <code>next</code> when processing:</p> <pre><code>- Financial reports with multi-page tables\n- Technical documents where diagrams span multiple pages\n- Legal contracts where clauses might be split across chunk boundaries\n</code></pre> <p>By default, it's recommended to set <code>next=0</code> unless you have a specific need for forward context in your document processing pipeline.</p> </li> </ol>"},{"location":"operators/link-resolve/","title":"Cluster operation","text":"<p>The <code>link_resolve</code> operation in DocETL is used to fix links between items, e.g. in a knoweldge graph. It assumes you have already ensured that the item id:s themselves are canonical, e.g. by running resolve first.</p> <p>It will examine every id specified in a link from one item to another and compare it to all item id:s. If it is not present with an exact match, it is going to be compared to each of them using an llm prompt to try to find a match.</p> <p>Note that this is a one-sided approach, compared to the resolve operation: It assumes that item id:s are canonical / correct.</p>"},{"location":"operators/link-resolve/#example-knowledge-graph-of-boating-terms","title":"\ud83d\ude80 Example: Knowledge graph of boating terms","text":"<pre><code>- name: fix_links\n  type: link_resolve\n  id_key: title\n  link_key: related_to\n  blocking_threshold: 0.85\n  embedding_model: text-embedding-ada-002\n  comparison_model: gpt-4o-mini\n  comparison_prompt: |\n    Compare the following two concepts:\n\n    Concept 1: [{{ link_value }}]\n    Concept 2: [{{ id_value }}]\n\n    Are these concepts likely refering to the same thing? When\n    comparing them, also consider the following description of\n    concept 2:\n\n      {{ item.description }}\n\n    Respond with \"True\" if they are likely the same concept, or \"False\" if they are likely different concepts.\n</code></pre> Sample Input and Output <p>The above make two replacements in the <code>related_to</code> keys: <code>Main sail</code> -&gt; <code>Sail (main)</code> and <code>Sailing boat</code> -&gt; <code>Sailing vessel</code>.</p> <pre><code>Input:\n```json\n[\n  {\n    \"title\": \"Sailing vessel\",\n    \"related_to\": [\"Main sail\", \"Main mast\", \"Rudder\"],\n    \"description\": \"A boat or ship propelled by sails. Typically with a very hydrodynamical hull.\"\n  },\n  {\n    \"title\": \"Catamaran\",\n    \"related_to\": [\"Sailing boat\", \"Hull\"],\n    \"description\": \"A type of vessel with two parallel hulls\"\n  },\n  {\n    \"title\": \"Sail (main)\",\n    \"related_to\": [\"Sheet\"],\n    \"description\": \"A cloth set up to catch the force of the wind\"\n  },\n  {\n    \"title\": \"Sheet (sailing)\",\n    \"related_to\": [],\n    \"description\": \"Ropes used to trim the angle of the sails, deciding the (angle of) wind force applied to the masts\"\n  },\n  {\n    \"title\": \"Rudder angle\",\n    \"related_to\": [],\n    \"description\": \"The rudder angle together with the wind force on the mast(s) decides the rate of turn of the vessel\"\n  }\n]\n```\n\nOutput:\n```json\n[\n  {\n    \"title\": \"Sailing vessel\",\n    \"related_to\": [\"Sail (main)\", \"Main mast\", \"Rudder\"],\n    \"description\": \"A boat or ship propelled by sails. Typically with a very hydrodynamical hull.\"\n  },\n  {\n    \"title\": \"Catamaran\",\n    \"related_to\": [\"Sailing vessel\", \"Hull\"],\n    \"description\": \"A type of vessel with two parallel hulls\"\n  },\n  {\n    \"title\": \"Sail (main)\",\n    \"related_to\": [\"Sheet\"],\n    \"description\": \"A cloth set up to catch the force of the wind\"\n  },\n  {\n    \"title\": \"Sheet (sailing)\",\n    \"related_to\": [],\n    \"description\": \"Ropes used to trim the angle of the sails, deciding the (angle of) wind force applied to the masts\"\n  },\n  {\n    \"title\": \"Rudder angle\",\n    \"related_to\": [],\n    \"description\": \"The rudder angle together with the wind force on the mast(s) decides the rate of turn of the vessel\"\n  }\n]\n```\n</code></pre>"},{"location":"operators/link-resolve/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"link_resolve\".</li> <li><code>id_key</code>: A key to use for item id:s</li> <li><code>link_key</code>: A key to make replacements in</li> <li><code>blocking_threshold</code>: Embedding similarity threshold for considering entries as potential matches</li> <li><code>comparison_prompt</code>: The prompt template to use for comparing potential matches.</li> </ul>"},{"location":"operators/link-resolve/#optional-parameters","title":"Optional Parameters","text":""},{"location":"operators/map/","title":"Map Operation","text":"<p>The Map operation in DocETL applies a specified transformation to each item in your input data, allowing for complex processing and insight extraction from large, unstructured documents.</p>"},{"location":"operators/map/#example-analyzing-long-form-news-articles","title":"\ud83d\ude80 Example: Analyzing Long-Form News Articles","text":"<p>Let's see a practical example of using the Map operation to analyze long-form news articles, extracting key information and generating insights.</p> <pre><code>- name: analyze_news_article\n  type: map\n  prompt: |\n    Analyze the following news article:\n    \"{{ input.article }}\"\n\n    Provide the following information:\n    1. Main topic (1-3 words)\n    2. Summary (2-3 sentences)\n    3. Key entities mentioned (list up to 5, with brief descriptions)\n    4. Sentiment towards the main topic (positive, negative, or neutral)\n    5. Potential biases or slants in reporting (if any)\n    6. Relevant categories (e.g., politics, technology, environment; list up to 3)\n    7. Credibility score (1-10, where 10 is highly credible)\n\n  output:\n    schema:\n      main_topic: string\n      summary: string\n      key_entities: list[object]\n      sentiment: string\n      biases: list[string]\n      categories: list[string]\n      credibility_score: integer\n\n  model: gpt-4o-mini\n  validate:\n    - len(output[\"main_topic\"].split()) &lt;= 3\n    - len(output[\"key_entities\"]) &lt;= 5\n    - output[\"sentiment\"] in [\"positive\", \"negative\", \"neutral\"]\n    - len(output[\"categories\"]) &lt;= 3\n    - 1 &lt;= output[\"credibility_score\"] &lt;= 10\n  num_retries_on_validate_failure: 2\n</code></pre> <p>This Map operation processes long-form news articles to extract valuable insights:</p> <ol> <li>Identifies the main topic of the article.</li> <li>Generates a concise summary.</li> <li>Extracts key entities (people, organizations, locations) mentioned in the article.</li> <li>Analyzes the overall sentiment towards the main topic.</li> <li>Identifies potential biases or slants in the reporting.</li> <li>Categorizes the article into relevant topics.</li> <li>Assigns a credibility score based on the content and sources.</li> </ol> <p>The operation includes validation to ensure the output meets our expectations and will retry up to 2 times if validation fails.</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"article\": \"In a groundbreaking move, the European Union announced yesterday a comprehensive plan to transition all member states to 100% renewable energy by 2050. The ambitious proposal, dubbed 'Green Europe 2050', aims to completely phase out fossil fuels and nuclear power across the continent.\n\n    European Commission President Ursula von der Leyen stated, 'This is not just about fighting climate change; it's about securing Europe's energy independence and economic future.' The plan includes massive investments in solar, wind, and hydroelectric power, as well as significant funding for research into new energy storage technologies.\n\n    However, the proposal has faced criticism from several quarters. Some Eastern European countries, particularly Poland and Hungary, argue that the timeline is too aggressive and could damage their economies, which are still heavily reliant on coal. Industry groups have also expressed concern about the potential for job losses in the fossil fuel sector.\n\n    Environmental groups have largely praised the initiative, with Greenpeace calling it 'a beacon of hope in the fight against climate change.' However, some activists argue that the 2050 target is not soon enough, given the urgency of the climate crisis.\n\n    The plan also includes provisions for a 'just transition,' with billions of euros allocated to retraining workers and supporting regions that will be most affected by the shift away from fossil fuels. Additionally, it proposes stricter energy efficiency standards for buildings and appliances, and significant investments in public transportation and electric vehicle infrastructure.\n\n    Experts are divided on the feasibility of the plan. Dr. Maria Schmidt, an energy policy researcher at the University of Berlin, says, 'While ambitious, this plan is achievable with the right political will and technological advancements.' However, Dr. John Smith from the London School of Economics warns, 'The costs and logistical challenges of such a rapid transition should not be underestimated.'\n\n    As the proposal moves forward for debate in the European Parliament, it's clear that 'Green Europe 2050' will be a defining issue for the continent in the coming years, with far-reaching implications for Europe's economy, environment, and global leadership in climate action.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"main_topic\": \"EU Renewable Energy\",\n    \"summary\": \"The European Union has announced a plan called 'Green Europe 2050' to transition all member states to 100% renewable energy by 2050. The ambitious proposal aims to phase out fossil fuels and nuclear power, invest in renewable energy sources, and includes provisions for a 'just transition' to support affected workers and regions.\",\n    \"key_entities\": [\n      {\n        \"name\": \"European Union\",\n        \"description\": \"Political and economic union of 27 member states\"\n      },\n      {\n        \"name\": \"Ursula von der Leyen\",\n        \"description\": \"European Commission President\"\n      },\n      {\n        \"name\": \"Poland\",\n        \"description\": \"Eastern European country critical of the plan\"\n      },\n      {\n        \"name\": \"Hungary\",\n        \"description\": \"Eastern European country critical of the plan\"\n      },\n      {\n        \"name\": \"Greenpeace\",\n        \"description\": \"Environmental organization supporting the initiative\"\n      }\n    ],\n    \"sentiment\": \"positive\",\n    \"biases\": [\n      \"Slight bias towards environmental concerns over economic impacts\",\n      \"More emphasis on supportive voices than critical ones\"\n    ],\n    \"categories\": [\n      \"Environment\",\n      \"Politics\",\n      \"Economy\"\n    ],\n    \"credibility_score\": 8\n  }\n]\n</code></pre></p> <p>This example demonstrates how the Map operation can transform long, unstructured news articles into structured, actionable insights. These insights can be used for various purposes such as trend analysis, policy impact assessment, and public opinion monitoring.</p>"},{"location":"operators/map/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"map\".</li> </ul>"},{"location":"operators/map/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>prompt</code> The prompt template to use for the transformation. Access input variables with <code>input.keyname</code>. None <code>batch_prompt</code> Template for processing multiple documents in a single prompt. Access batch with <code>inputs</code> list. None <code>max_batch_size</code> Maximum number of documents to process in a single batch None <code>output.schema</code> Schema definition for the output from the LLM. None <code>output.n</code> Number of outputs to generate for each input. (only available for OpenAI models; this is used to generate multiple outputs from a single input and automatically turn into a bigger list) 1 <code>model</code> The language model to use Falls back to <code>default_model</code> <code>optimize</code> Flag to enable operation optimization <code>True</code> <code>recursively_optimize</code> Flag to enable recursive optimization of operators synthesized as part of rewrite rules <code>false</code> <code>sample</code> Number of samples to use for the operation Processes all data <code>limit</code> Maximum number of outputs to produce before stopping Processes all data <code>tools</code> List of tool definitions for LLM use None <code>validate</code> List of Python expressions to validate the output None <code>flush_partial_results</code> Write results of individual batches of map operation to disk for faster inspection False <code>num_retries_on_validate_failure</code> Number of retry attempts on validation failure 0 <code>gleaning</code> Configuration for advanced validation and LLM-based refinement None <code>drop_keys</code> List of keys to drop from the input before processing None <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>timeout</code> Timeout for each LLM call in seconds 120 <code>litellm_completion_kwargs</code> Additional parameters to pass to LiteLLM completion calls. {} <code>skip_on_error</code> If true, skip the operation if the LLM returns an error. False <code>bypass_cache</code> If true, bypass the cache for this operation. False <code>pdf_url_key</code> If specified, the key in the input that contains the URL of the PDF to process. None <code>calibrate</code> Improve consistency across documents by using sample data as reference anchors. False <code>num_calibration_docs</code> Number of documents to use sample and generate outputs for, for calibration. 10 <code>retriever</code> Name of a retriever to use for RAG. See Retrievers. None <code>save_retriever_output</code> If true, saves the retrieved context to <code>_&lt;operation_name&gt;_retrieved_context</code> in the output. False <p>Note: If <code>drop_keys</code> is specified, <code>prompt</code> and <code>output</code> become optional parameters.</p>"},{"location":"operators/map/#limiting-execution","title":"Limiting execution","text":"<p>Set <code>limit</code> when you only need the first N map results or want to cap LLM spend. The operation slices the processed dataset to the first <code>limit</code> entries and also stops scheduling new prompts once that many outputs have been produced, even if a prompt returns multiple records. Filter operations inherit this behavior but redefine the count so the limit only applies to records whose filter predicate evaluates to <code>true</code> (see Filter).</p> <p>Validation and Gleaning</p> <p>For more details on validation techniques and implementation, see operators.</p>"},{"location":"operators/map/#batch-processing","title":"Batch Processing","text":"<p>The Map operation supports processing multiple documents in a single prompt using the <code>batch_prompt</code> parameter. This can be more efficient than processing documents individually, especially for simpler tasks and shorter documents, especially when there are LLM call limits. However, larger batch sizes (even &gt; 5) can lead to more incorrect results, so use this feature judiciously.</p> Batch Processing Example <pre><code>- name: classify_documents\n  type: map\n  max_batch_size: 5  # Process up to 5 documents in a single LLM call\n  batch_prompt: |\n    Classify each of the following documents into categories (technology, business, or science):\n\n    {% for doc in inputs %}\n    Document {{loop.index}}:\n    {{doc.text}}\n    {% endfor %}\n\n    Provide a classification for each document.\n  prompt: |\n    Classify the following document:\n    {{input.text}}\n  output:\n    schema:\n      category: string\n</code></pre> <p>When using batch processing:</p> <ol> <li>The <code>batch_prompt</code> template receives an <code>inputs</code> list containing the batch of documents</li> <li>Use <code>max_batch_size</code> to control how many documents are processed in each batch</li> <li>You must also provide a <code>prompt</code> parameter that will be used in case the batch prompt's response cannot be parsed into the output schema</li> <li>Gleaning and validation are applied to each document in the batch individually, after the batch has been processed by the LLM</li> </ol> <p>Batch Size Considerations</p> <p>Choose your <code>max_batch_size</code> carefully:</p> <ul> <li>Larger batches may be more efficient but risk hitting token limits</li> <li>Start with smaller batches (3-5 documents) and adjust based on your needs</li> <li>Consider document length when setting batch size</li> </ul>"},{"location":"operators/map/#calibration-for-consistency","title":"Calibration for Consistency","text":"<p>The Map operation supports calibration to improve consistency across documents, especially for classification tasks or operations that require relative positioning (like rating scales). When enabled, calibration samples a subset of your documents, processes them with the original prompt, and then uses those results to generate reference anchors that help maintain consistency across all documents.</p> <p>This is particularly useful for: - Classification tasks where documents need to be evaluated relative to each other - Rating/scoring operations where you want consistent scales - Subjective judgments that benefit from concrete examples</p> Document Priority Classification with Calibration <p>Imagine you're processing a large collection of customer support tickets and want to classify them by priority. Without calibration, the LLM might be inconsistent - a \"medium\" priority ticket early in processing might be classified as \"high\" later when the LLM sees more severe issues.</p> <pre><code>- name: classify_ticket_priority\n  type: map\n  calibrate: true  # Enable calibration\n  num_calibration_docs: 15  # Use 15 tickets for calibration\n  prompt: |\n    Classify the following customer support ticket by priority level:\n\n    Subject: {{ input.subject }}\n    Description: {{ input.description }}\n    Customer Tier: {{ input.customer_tier }}\n\n    Classify as: low, medium, high, or critical\n  output:\n    schema:\n      priority: string\n      reasoning: string\n  model: gpt-4o-mini\n</code></pre> <p>How calibration works:</p> <ol> <li>Sample: Randomly selects 15 tickets from your dataset (using seed=42 for reproducibility)</li> <li>Process: Runs the original prompt on these 15 tickets</li> <li>Analyze: An LLM analyzes the sample results and generates reference anchors</li> <li>Augment: Appends these reference anchors to your original prompt</li> <li>Execute: Processes all tickets with the augmented prompt</li> </ol> <p>Example calibration output: <pre><code># Original prompt gets augmented with something like:\n# \n# For reference, consider 'Server completely down for 500+ users' \u2192 critical as your baseline for critical issues.\n# Documents similar to 'Login button not working for one user' \u2192 low priority.\n# For reference, consider 'Payment processing delays affecting checkout' \u2192 high as your standard for high priority issues.\n</code></pre></p> <p>When to Use Calibration</p> <p>Calibration is most beneficial when:</p> <ul> <li>Your task requires relative judgments (rating scales, classifications)</li> <li>You're processing documents that vary widely in characteristics</li> <li>Consistency across the entire dataset is more important than individual accuracy</li> <li>You have enough data for meaningful sampling (at least 20+ documents)</li> </ul> <p>Calibration Considerations</p> <ul> <li>Adds a small overhead cost (processes calibration samples + calibration analysis)</li> <li>Uses a fixed seed (42) for reproducible sampling</li> <li>The calibration LLM call uses temperature=0.0 for consistent results</li> </ul>"},{"location":"operators/map/#advanced-features","title":"Advanced Features","text":""},{"location":"operators/map/#pdf-processing","title":"PDF Processing","text":"<p>The Map operation can directly process PDFs using Claude or Gemini models. To use this feature:</p> <ol> <li>Your input dataset must contain a key representing the URL of the PDF to process</li> <li>Specify this field name using the <code>pdf_url_key</code> parameter in your map operation</li> <li>The URLs must be publicly accessible or accessible to your environment</li> </ol> PDF Processing Example <p>Here's an example of processing a dataset of papers, where each paper is represented by a URL.</p> <pre><code>datasets:\n  papers:\n    type: file\n    path: \"papers/urls.json\"  # Contains documents with PDF URLs\n\ndefault_model: gemini/gemini-2.0-flash  # or claude models\noperations:\n  - name: extract_paper_info\n    type: map\n    pdf_url_key: url  # Tells DocETL which field contains the PDF URL\n    prompt: |\n      Summarize the paper.\n    output:\n      schema:\n        paper_info: string\n\npipeline:\n  steps:\n    - name: extract_paper_info\n      input: papers\n      operations:\n        - extract_paper_info\n</code></pre> <p>Your input data (<code>papers/urls.json</code>) should contain documents with PDF URLs: <pre><code>[\n  {\"url\": \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card.pdf\"},\n  ...\n]\n</code></pre></p> <p>DocETL will: 1. Download each PDF 2. Extract the text content 3. Pass the content to the LLM with your prompt 4. Return the processed results:</p> <pre><code>[\n  {\n    \"url\": \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card.pdf\",\n    \"paper_info\": \"This paper introduces Claude 3.5 Haiku and the upgraded Claude 3.5 Sonnet...\"\n  },\n  ...\n]\n</code></pre>"},{"location":"operators/map/#tool-use","title":"Tool Use","text":"<p>Tools can extend the capabilities of the Map operation. Each tool is a Python function that can be called by the LLM during execution, and follows the OpenAI Function Calling API.</p> Tool Definition Example <pre><code>tools:\n- required: true\n    code: |\n    def count_words(text):\n        return {\"word_count\": len(text.split())}\n    function:\n    name: count_words\n    description: Count the number of words in a text string.\n    parameters:\n        type: object\n        properties:\n        text:\n            type: string\n        required:\n        - text\n</code></pre> <p>Warning</p> <p>Tool use and gleaning cannot be used simultaneously.</p>"},{"location":"operators/map/#input-truncation","title":"Input Truncation","text":"<p>If the input doesn't fit within the token limit, DocETL automatically truncates tokens from the middle of the input data, preserving the beginning and end which often contain more important context. A warning is displayed when truncation occurs.</p>"},{"location":"operators/map/#batching","title":"Batching","text":"<p>If you have a really large collection of documents and you don't want to run them through the Map operation at the same time, you can use the <code>batch_size</code> parameter to process data in smaller chunks. This can significantly reduce memory usage and improve performance.</p> <p>To enable batching in your map operations, you need to specify the <code>max_batch_size</code> parameter in your configuration.</p> <pre><code>- name: extract_summaries\n  type: map\n  max_batch_size: 5\n  clustering_method: random\n  prompt: |\n    Summarize this text: \"{{ input.text }}\"\n  output:\n    schema:\n      summary: string\n</code></pre> <p>In the above config, there will be no more than 5 API calls to the LLM at a time (i.e., 5 documents processed at a time, one per API call).</p>"},{"location":"operators/map/#dropping-keys","title":"Dropping Keys","text":"<p>You can use a map operation to act as an LLM no-op, and just drop any key-value pairs you don't want to save to the output file. To do this, you can use the <code>drop_keys</code> parameter.</p> <pre><code>- name: drop_keys_example\n  type: map\n  drop_keys:\n    - \"keyname1\"\n    - \"keyname2\"\n</code></pre>"},{"location":"operators/map/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Prompts: Write clear, specific prompts that guide the LLM to produce the desired output.</li> <li>Robust Validation: Use validation to ensure output quality and consistency.</li> <li>Appropriate Model Selection: Choose the right model for your task, balancing performance and cost.</li> <li>Optimize for Scale: For large datasets, consider using <code>sample</code> to test your operation before running on the full dataset.</li> <li>Use Tools Wisely: Leverage tools for complex calculations or operations that the LLM might struggle with. You can write any Python code in the tools, so you can even use tools to call other APIs or search the internet.</li> </ol>"},{"location":"operators/map/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>The Map operation supports generating multiple outputs for each input using the <code>output.n</code> parameter. This is particularly useful for synthetic data generation, content variations, or when you need multiple alternatives for each input item.</p> <p>When you set <code>output.n</code> to a value greater than 1, the operation will: 1. Process each input once 2. Generate multiple outputs based on the same input 3. Return all generated outputs as separate items in the result list</p> <p>This multiplies your dataset size by the factor of <code>n</code>.</p> Synthetic Email Generation Example <p>Imagine you have a dataset of prospects and want to generate multiple email templates for each person. Here's how to generate 10 different email templates per prospect:</p> <pre><code>datasets:\n  prospects:\n    type: file\n    path: \"prospects.json\"  # Contains names, companies, roles, etc.\n\noperations:\n  - name: generate_email_templates\n    type: map\n    bypass_cache: true\n    optimize: true\n    output:\n      n: 10  # Generate 10 unique emails per prospect\n      schema:\n        subject: \"str\"\n        body: \"str\"\n        call_to_action: \"str\"\n    prompt: |\n      Create a personalized cold outreach email for the following prospect:\n\n      Name: {{ input.name }}\n      Company: {{ input.company }}\n      Role: {{ input.role }}\n      Industry: {{ input.industry }}\n\n      The email should:\n      - Have a compelling subject line\n      - Be brief (3-5 sentences)\n      - Mention a specific pain point for their industry\n      - Include a clear call to action\n      - Sound natural and conversational, not sales-y\n\n      Your response should be formatted as:\n      Subject: [Your subject line]\n\n      [Your email body]\n\n      Call to action: [Your specific call to action]\n\npipeline:\n  steps:\n    - name: email_generation\n      input: prospects\n      operations:\n        - generate_email_templates\n\n  output:\n    type: file\n    path: \"email_templates.json\"\n</code></pre> <p>With this configuration, if your prospects.json file has 50 prospects, the output will contain 500 email templates (50 prospects \u00d7 10 emails each).</p> <p>Important Considerations</p> <ul> <li>The <code>output.n</code> parameter is only available for OpenAI models</li> <li>Higher values of <code>n</code> will increase the cost of your operation proportionally</li> <li>For optimal results, keep your prompt focused and clear about generating diverse outputs</li> <li>When using pandas, the <code>n</code> parameter can be passed directly to the <code>map</code> method</li> </ul>"},{"location":"operators/parallel-map/","title":"Parallel Map Operation","text":"<p>The Parallel Map operation in DocETL applies multiple independent transformations to each item in the input data concurrently, maintaining a 1:1 input-to-output ratio while generating multiple fields simultaneously.</p> <p>Similarity to Map Operation</p> <p>The Parallel Map operation is very similar to the standard Map operation. The key difference is that Parallel Map allows you to define multiple prompts that run concurrently (without having to explicitly create a DAG), whereas a standard Map operation typically involves a single transformation.</p>"},{"location":"operators/parallel-map/#configuration","title":"Configuration","text":"<p>Each prompt in the Parallel Map operation is responsible for generating specific fields of the output. The prompts are executed concurrently, improving efficiency when working with multiple transformations.</p> <p>The output schema should include all the fields generated by the individual prompts, ensuring that the results are combined into a single output item for each input.</p>"},{"location":"operators/parallel-map/#required-parameters","title":"Required Parameters","text":"Parameter Description <code>name</code> A unique name for the operation <code>type</code> Must be set to \"parallel_map\" <code>prompts</code> A list of prompt configurations (see below) <code>output</code> Schema definition for the combined output from all prompts <p>Each prompt configuration in the <code>prompts</code> list should contain:</p> <ul> <li><code>prompt</code>: The prompt template to use for the transformation</li> <li><code>output_keys</code>: List of keys that this prompt will generate</li> <li><code>model</code> (optional): The language model to use for this specific prompt</li> <li><code>gleaning</code> (optional): Advanced validation settings for this prompt (see Per-Prompt Gleaning section below)</li> </ul>"},{"location":"operators/parallel-map/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>model</code> The default language model to use Falls back to <code>default_model</code> <code>optimize</code> Flag to enable operation optimization True <code>recursively_optimize</code> Flag to enable recursive optimization false <code>sample</code> Number of samples to use for the operation Processes all data <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>litellm_completion_kwargs</code> Additional parameters to pass to LiteLLM completion calls. {} Why use Parallel Map instead of multiple Map operations? <p>While you could achieve similar results with multiple Map operations, Parallel Map offers several advantages:</p> <ol> <li>Concurrency: Prompts run in parallel, potentially reducing overall processing time.</li> <li>Simplified Configuration: You define multiple transformations in a single operation, reducing pipeline complexity.</li> <li>Unified Output: Results from all prompts are combined into a single output item, simplifying downstream operations.</li> </ol>"},{"location":"operators/parallel-map/#example-processing-job-applications","title":"\ud83d\ude80 Example: Processing Job Applications","text":"<p>Here's an example of a parallel map operation that processes job applications by extracting key information and evaluating candidates:</p> <pre><code>- name: process_job_application\n  type: parallel_map\n  prompts:\n    - name: extract_skills\n      prompt: \"Given the following resume: '{{ input.resume }}', list the top 5 relevant skills for a software engineering position.\"\n      output_keys:\n        - skills\n      gleaning:\n        num_rounds: 1\n        validation_prompt: |\n          Confirm the skills list contains **exactly** 5 distinct skills and each skill is one or two words long.\n      model: gpt-4o-mini\n    - name: calculate_experience\n      prompt: \"Based on the work history in this resume: '{{ input.resume }}', calculate the total years of relevant experience for a software engineering role.\"\n      output_keys:\n        - years_experience\n      model: gpt-4o-mini\n    - name: evaluate_cultural_fit\n      prompt: \"Analyze the following cover letter: '{{ input.cover_letter }}'. Rate the candidate's potential cultural fit on a scale of 1-10, where 10 is the highest.\"\n      output_keys:\n        - cultural_fit_score\n      model: gpt-4o-mini\n  output:\n    schema:\n      skills: list[string]\n      years_experience: float\n      cultural_fit_score: integer\n</code></pre> <p>This Parallel Map operation processes job applications by concurrently extracting skills, calculating experience, and evaluating cultural fit.</p>"},{"location":"operators/parallel-map/#advanced-validation-per-prompt-gleaning","title":"Advanced Validation: Per-Prompt Gleaning","text":"<p>Each prompt in a Parallel Map operation can include its own <code>gleaning</code> configuration. Gleaning works exactly as described in the operators overview but is scoped to the individual LLM call for that prompt. This allows you to tailor validation logic\u2014and even the model used\u2014to the specific transformation being performed.</p> <p>The structure of the <code>gleaning</code> block is identical:</p> <pre><code>gleaning:\n  num_rounds: 1               # maximum refinement iterations\n  validation_prompt: |        # judge prompt appended to the chat thread\n    Ensure the extracted skills list contains at least 5 distinct items.\n  model: gpt-4o-mini          # (optional) model for the validator LLM\n</code></pre>"},{"location":"operators/parallel-map/#example-with-per-prompt-gleaning","title":"\ud83d\udcc4 Example with Per-Prompt Gleaning","text":"<pre><code>- name: process_job_application\n  type: parallel_map\n  prompts:\n    - name: extract_skills\n      prompt: \"Given the following resume: '{{ input.resume }}', list the top 5 relevant skills for a software engineering position.\"\n      output_keys:\n        - skills\n      gleaning:\n        num_rounds: 1\n        validation_prompt: |\n          Confirm the skills list contains **exactly** 5 distinct skills and each skill is one or two words long.\n      model: gpt-4o-mini\n    - name: calculate_experience\n      prompt: \"Based on the work history in this resume: '{{ input.resume }}', calculate the total years of relevant experience for a software engineering role.\"\n      output_keys:\n        - years_experience\n      gleaning:\n        num_rounds: 2\n        validation_prompt: |\n          Verify that the years of experience is a non-negative number and round to one decimal place if necessary.\n    - name: evaluate_cultural_fit\n      prompt: \"Analyze the following cover letter: '{{ input.cover_letter }}'. Rate the candidate's potential cultural fit on a scale of 1-10, where 10 is the highest.\"\n      output_keys:\n        - cultural_fit_score\n      model: gpt-4o-mini\n  output:\n    schema:\n      skills: list[string]\n      years_experience: float\n      cultural_fit_score: integer\n</code></pre> <p>In this configuration, only the <code>extract_skills</code> and <code>calculate_experience</code> prompts use gleaning. Each prompt's validator runs immediately after its own LLM call and before the overall outputs are merged.</p>"},{"location":"operators/parallel-map/#advantages","title":"Advantages","text":"<ol> <li>Concurrency: Multiple transformations are applied simultaneously, potentially reducing overall processing time.</li> <li>Simplicity: Users can define multiple transformations without needing to create explicit DAGs in the configuration.</li> <li>Flexibility: Different models can be used for different prompts within the same operation.</li> <li>Maintainability: Each transformation can be defined and updated independently, making it easier to manage complex operations.</li> </ol>"},{"location":"operators/parallel-map/#best-practices","title":"Best Practices","text":"<ol> <li>Independent Transformations: Ensure that the prompts in a Parallel Map operation are truly independent of each other to maximize the benefits of concurrent execution.</li> <li>Balanced Prompts: Try to design prompts that have similar complexity and execution times to optimize overall performance.</li> <li>Output Schema Alignment: Ensure that the output schema correctly captures all the fields generated by the individual prompts.</li> <li>Lightweight Validators: When using per-prompt gleaning, keep validation prompts concise so that the cost and latency overhead stays manageable.</li> </ol>"},{"location":"operators/rank/","title":"Rank Operation","text":"<p>The Rank operation in DocETL sorts documents based on specified criteria. Note that this operation is designed to sort documents along some (latent) attribute in the data. It is not specifically meant for top-k or retrieval-like queries.</p> <p>We adapt algorithms from Human-Powered Sorts and Joins (VLDB 2012).</p>"},{"location":"operators/rank/#example-ranking-debates-by-level-of-controversy","title":"\ud83d\ude80 Example: Ranking Debates by Level of Controversy","text":"<p>Let's see a practical example of using the Rank operation to rank political debates based on how controversial they are:</p> <pre><code>- name: rank_by_controversy\n  type: rank\n  prompt: |\n    Order these debate transcripts based on how controversial the discussion is.\n    Consider factors like:\n    - The level of disagreement between candidates\n    - Discussion of divisive topics\n    - Strong emotional language\n    - Presence of conflicting viewpoints\n    - Public reaction mentioned in the transcript\n\n    Debates with the most controversial content should be ranked highest.\n  input_keys: [\"content\", \"title\", \"date\"]\n  direction: desc\n  rerank_call_budget: 10 # max number of LLM calls to use; also optional\n  initial_ordering_method: \"likert\"\n</code></pre> <p>This Rank operation ranks debate transcripts from most controversial to least controversial by:</p> <ol> <li>First generating ordinal scores (on the Likert scale) for the ranking criteria and each document. This executes an LLM call per document.</li> <li>Creating an initial ranking based on the scores from step 1.</li> <li>Using an LLM to perform more precise re-rankings on a sliding window of documents. This executes <code>rerank_call_budget</code> calls.</li> </ol> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"title\": \"Presidential Debate: Economy and Trade\",\n    \"date\": \"2020-09-29\",\n    \"content\": \"Moderator: Let's discuss trade policies. Candidate A, your response?\\n\\nCandidate A: My opponent's policies have shipped jobs overseas for decades! Our workers are suffering while other countries laugh at us.\\n\\nCandidate B: That's simply not true. The data shows our export growth has been strong. My opponent doesn't understand basic economics.\\n\\nCandidate A: [interrupting] You've been in government for 47 years and haven't fixed anything!\\n\\nCandidate B: If you'd let me finish... The manufacturing sector has actually added jobs under our policies.\\n\\nModerator: Please allow each other to finish. Let's move to healthcare...\"\n  },\n  {\n    \"title\": \"Vice Presidential Debate: Foreign Policy\",\n    \"date\": \"2020-10-07\",\n    \"content\": \"Moderator: What would your administration's approach be to China?\\n\\nCandidate C: We need strategic engagement that protects American interests while avoiding unnecessary conflict. My opponent has proposed policies that would damage our diplomatic relationships.\\n\\nCandidate D: I respectfully disagree with my colleague. Our current approach has been too soft. We need to stand firm on human rights issues and trade imbalances.\\n\\nCandidate C: I think we actually agree on the goals, if not the methods. The question is how to achieve them without harmful escalation.\\n\\nCandidate D: That's a fair point. Perhaps there's a middle ground that maintains pressure while keeping dialogue open.\\n\\nModerator: Thank you both for that thoughtful exchange. Moving to the Middle East...\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"title\": \"Presidential Debate: Economy and Trade\",\n    \"date\": \"2020-09-29\",\n    \"content\": \"Moderator: Let's discuss trade policies. Candidate A, your response?\\n\\nCandidate A: My opponent's policies have shipped jobs overseas for decades! Our workers are suffering while other countries laugh at us.\\n\\nCandidate B: That's simply not true. The data shows our export growth has been strong. My opponent doesn't understand basic economics.\\n\\nCandidate A: [interrupting] You've been in government for 47 years and haven't fixed anything!\\n\\nCandidate B: If you'd let me finish... The manufacturing sector has actually added jobs under our policies.\\n\\nModerator: Please allow each other to finish. Let's move to healthcare...\",\n    \"_rank\": 1\n  },\n  {\n    \"title\": \"Vice Presidential Debate: Foreign Policy\",\n    \"date\": \"2020-10-07\",\n    \"content\": \"Moderator: What would your administration's approach be to China?\\n\\nCandidate C: We need strategic engagement that protects American interests while avoiding unnecessary conflict. My opponent has proposed policies that would damage our diplomatic relationships.\\n\\nCandidate D: I respectfully disagree with my colleague. Our current approach has been too soft. We need to stand firm on human rights issues and trade imbalances.\\n\\nCandidate C: I think we actually agree on the goals, if not the methods. The question is how to achieve them without harmful escalation.\\n\\nCandidate D: That's a fair point. Perhaps there's a middle ground that maintains pressure while keeping dialogue open.\\n\\nModerator: Thank you both for that thoughtful exchange. Moving to the Middle East...\",\n    \"_rank\": 2\n  }\n]\n</code></pre></p> <p>This example demonstrates how the Rank operation can semantically sort documents based on complex criteria, providing a ranking that would be difficult to achieve with keyword matching or rule-based approaches.</p>"},{"location":"operators/rank/#algorithm-and-implementation","title":"Algorithm and Implementation","text":"<p>The Rank operation works in these steps:</p> <ol> <li>Initial Ranking:<ol> <li>The algorithm begins with either an embedding-based or Likert-scale rating approach:<ol> <li>Embedding-based: Creates embedding vectors for the ranking criteria and each document, then calculates cosine similarity</li> <li>Likert-based (default): Uses the LLM to rate each document on a 7-point Likert scale based on the criteria. We do this in batches of <code>batch_size</code> documents (defaults to 10), and the prompt includes a random sample of <code>num_calibration_docs</code> (defaults to 10) documents to calibrate the LLM with.</li> </ol> </li> <li>Documents are initially sorted by their similarity scores or ratings (high to low for desc, low to high for asc)</li> </ol> </li> <li>\"Picky Window\" Refinement:<ol> <li>Rather than processing all documents with equal focus, the algorithm employs a \"picky window\" approach</li> <li>Starting from the bottom of the currently ranked documents and working upward:<ol> <li>A large window of documents is presented to the LLM</li> <li>The LLM is asked to identify only the top few documents (configured via <code>num_top_items_per_window</code>)</li> <li>These chosen documents are then moved to the beginning of the window</li> </ol> </li> <li>The window slides upward through the document set with overlapping segments</li> <li>This approach enables the algorithm to process many documents while focusing LLM effort on identifying the best matches</li> </ol> </li> <li> <p>Efficient Resource Utilization:</p> <ol> <li>The window size and step size are calculated based on the call budget to ensure optimal use of LLM calls</li> <li>Overlap between windows ensures robust ranking with minimal redundancy</li> <li>The algorithm tracks document positions using unique identifiers to maintain consistency </li> </ol> </li> <li> <p>Output Preparation:</p> <ol> <li>After all windows have been processed, the algorithm assigns a <code>_rank</code> field to each document (1-indexed)</li> <li>Returns the documents in their final sorted order</li> </ol> </li> </ol>"},{"location":"operators/rank/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"rank\".</li> <li><code>prompt</code>: The prompt specifying the ranking criteria. This does not need to be a Jinja template.</li> <li><code>input_keys</code>: List of document keys to consider for ranking.</li> <li><code>direction</code>: Either \"asc\" (ascending) or \"desc\" (descending).</li> </ul>"},{"location":"operators/rank/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>model</code> The language model to use for LLM-based ranking Falls back to <code>default_model</code> <code>embedding_model</code> The embedding model to use for similarity calculations \"text-embedding-3-small\" <code>batch_size</code> Maximum number of documents to process in a single LLM batch rating (used for the first pass) 10 <code>timeout</code> Timeout for each LLM call in seconds 120 <code>verbose</code> Whether to log detailed LLM call statistics False <code>num_calibration_docs</code> Number of documents to use for calibration (used for the first pass) 10 <code>litellm_completion_kwargs</code> Additional parameters to pass to LiteLLM completion calls {} <code>bypass_cache</code> If true, bypass the cache for this operation False <code>initial_ordering_method</code> Method to use for initial ranking: \"likert\" (default) or \"embedding\" \"likert\" <code>k</code> Number of top items to focus on in the final ranking None (ranks all items) <code>call_budget</code> Maximum number of LLM API calls to make during ranking 10 <code>num_top_items_per_window</code> Number of top items the LLM should select from each window 3 <code>overlap_fraction</code> Fraction of overlap between windows 0.5"},{"location":"operators/rank/#two-step-ranking-approach","title":"Two-Step Ranking Approach","text":"<p>For more complex ranking tasks, a two-step approach can be more effective:</p> <ol> <li>First use a <code>map</code> operation to extract and structure relevant information</li> <li>Then use the <code>rank</code> operation to rank based on the extracted information</li> </ol> Two-Step Ranking Example <pre><code>operations:\n  - name: extract_hostile_exchanges\n    type: map\n    output:\n      schema:\n        meanness_summary: \"str\"\n        hostility_level: \"int\"\n        key_examples: \"list[str]\"\n    prompt: |\n      Analyze the following debate transcript for {{ input.title }} on {{ input.date }}:\n\n      {{ input.content }}\n\n      Extract and summarize exchanges where candidates are mean or hostile to each other.\n      [... prompt details ...]\n\n  - name: rank_by_meanness\n    type: rank\n    prompt: |\n      Order these debate transcripts based on how mean or hostile the candidates are to each other.\n      Focus on the meanness summaries and examples that have been extracted.\n\n      Consider:\n      - The overall hostility level rating\n      - Severity of personal attacks in the key examples\n      [... prompt details ...]\n    input_keys: [\"meanness_summary\", \"hostility_level\", \"key_examples\", \"title\", \"date\"]\n    direction: desc\n    rerank_call_budget: 10\n\npipeline:\n  steps:\n    - name: meanness_analysis\n      input: debates\n      operations:\n        - extract_hostile_exchanges\n        - rank_by_meanness\n</code></pre> <p>This approach: 1. First extracts structured data about hostility in each debate 2. Then ranks debates based on this pre-processed data</p>"},{"location":"operators/rank/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Craft Clear Ranking Criteria: Write clear, specific prompts that guide the LLM to understand the ranking priorities.</p> </li> <li> <p>Choose Appropriate Input Keys: Only include document fields that are relevant to the ranking criteria to reduce noise.</p> </li> <li> <p>Consider Pre-Processing: For complex criteria, use a map operation first to extract structured data that makes ranking more effective.</p> </li> <li> <p>Tune Window Parameters:</p> </li> <li>Adjust <code>num_top_items_per_window</code> based on how selective you need the ranking to be</li> <li>Modify <code>overlap_fraction</code> to balance redundancy and completeness</li> <li> <p>Start with defaults and adjust based on results</p> </li> <li> <p>Use Verbose Mode During Development: Enable the <code>verbose</code> flag during development to understand how the ranking process works and verify the results.</p> </li> <li> <p>Direction Matters: Choose \"asc\" or \"desc\" carefully based on your use case:</p> </li> <li>\"desc\" (descending) ranks the most matching items first</li> <li> <p>\"asc\" (ascending) ranks the least matching items first</p> </li> <li> <p>Mind Cost Considerations: The ranking operation makes multiple LLM calls and embedding requests. For large datasets, consider sampling first to test your approach. Use the embedding based first pass to significantly reduce cost.</p> </li> </ol>"},{"location":"operators/rank/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The rank operation scales with O(n)</li> <li>The <code>verbose</code> flag adds detailed logging but doesn't affect performance or results</li> </ul>"},{"location":"operators/reduce/","title":"Reduce Operation","text":"<p>The Reduce operation in DocETL aggregates data based on a key. It supports both batch reduction and incremental folding for large datasets, making it versatile for various data processing tasks.</p>"},{"location":"operators/reduce/#motivation","title":"Motivation","text":"<p>Reduce operations are essential when you need to summarize or aggregate data across multiple records. For example, you might want to:</p> <ul> <li>Analyze sentiment trends across social media posts</li> <li>Consolidate patient medical records from multiple visits</li> <li>Synthesize key findings from a set of research papers on a specific topic</li> </ul>"},{"location":"operators/reduce/#example-summarizing-customer-feedback","title":"\ud83d\ude80 Example: Summarizing Customer Feedback","text":"<p>Let's look at a practical example of using the Reduce operation to summarize customer feedback by department:</p> <pre><code>- name: summarize_feedback\n  type: reduce\n  reduce_key: department\n  prompt: |\n    Summarize the customer feedback for the {{ inputs[0].department }} department:\n\n    {% for item in inputs %}\n    Feedback {{ loop.index }}: {{ item.feedback }}\n    {% endfor %}\n\n    Provide a concise summary of the main points and overall sentiment.\n  output:\n    schema:\n      summary: string\n      sentiment: string\n</code></pre> <p>This Reduce operation processes customer feedback grouped by department:</p> <ol> <li>Groups all feedback entries by the 'department' key.</li> <li>For each department, it applies the prompt to summarize the feedback and determine overall sentiment.</li> <li>Outputs a summary and sentiment for each department.</li> </ol>"},{"location":"operators/reduce/#configuration","title":"Configuration","text":""},{"location":"operators/reduce/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>type</code>: Must be set to \"reduce\".</li> <li><code>reduce_key</code>: The key (or list of keys) to use for grouping data. Use <code>_all</code> to group all data into one group.</li> <li><code>prompt</code>: The prompt template to use for the reduction operation.</li> <li><code>output</code>: Schema definition for the output from the LLM.</li> </ul>"},{"location":"operators/reduce/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>sample</code> Number of samples to use for the operation None <code>limit</code> Maximum number of groups to process before stopping All groups <code>synthesize_resolve</code> If false, won't synthesize a resolve operation between map and reduce true <code>model</code> The language model to use Falls back to default_model <code>input</code> Specifies the schema or keys to subselect from each item All keys from input items <code>pass_through</code> If true, non-input keys from the first item in the group will be passed through false <code>associative</code> If true, the reduce operation is associative (i.e., order doesn't matter) true <code>fold_prompt</code> A prompt template for incremental folding None <code>fold_batch_size</code> Number of items to process in each fold operation None <code>value_sampling</code> A dictionary specifying the sampling strategy for large groups None <code>verbose</code> If true, enables detailed logging of the reduce operation false <code>persist_intermediates</code> If true, persists the intermediate results for each group to the key <code>_{operation_name}_intermediates</code> false <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>litellm_completion_kwargs</code> Additional parameters to pass to LiteLLM completion calls. {} <code>bypass_cache</code> If true, bypass the cache for this operation. False <code>retriever</code> Name of a retriever to use for RAG. See Retrievers. None <code>save_retriever_output</code> If true, saves the retrieved context to <code>_&lt;operation_name&gt;_retrieved_context</code> in the output. False"},{"location":"operators/reduce/#limiting-group-processing","title":"Limiting group processing","text":"<p>Set <code>limit</code> to short-circuit the reduce phase after N groups have been aggregated. When <code>limit</code> is set, groups are sorted by size (smallest first) and only the N smallest groups are processed. This is useful for previewing results or capping LLM usage while minimizing cost by processing groups with fewer documents. Groups beyond the limit are never scheduled, so you avoid extra fold/merge calls. If a grouped reduce returns more than one record per group, the final output list is truncated to <code>limit</code>.</p>"},{"location":"operators/reduce/#advanced-features","title":"Advanced Features","text":""},{"location":"operators/reduce/#incremental-folding","title":"Incremental Folding","text":"<p>For large datasets, the Reduce operation supports incremental folding. This allows processing of large groups in smaller batches, which can be more efficient and reduce memory usage.</p> <p>To enable incremental folding, provide a <code>fold_prompt</code> and <code>fold_batch_size</code>:</p> <pre><code>- name: large_data_reduce\n  type: reduce\n  reduce_key: category\n  prompt: |\n    Summarize the data for category {{ inputs[0].category }}:\n    {% for item in inputs %}\n    Item {{ loop.index }}: {{ item.data }}\n    {% endfor %}\n  fold_prompt: |\n    Combine the following summaries for category {{ inputs[0].category }}:\n    Current summary: {{ output.summary }}\n    New data:\n    {% for item in inputs %}\n    Item {{ loop.index }}: {{ item.data }}\n    {% endfor %}\n  fold_batch_size: 100\n  output:\n    schema:\n      summary: string\n</code></pre>"},{"location":"operators/reduce/#example-rendered-prompt","title":"Example Rendered Prompt","text":"<p>Rendered Reduce Prompt</p> <p>Let's consider an example of how a reduce operation prompt might look when rendered with actual data. Assume we have a reduce operation that summarizes product reviews, and we're processing reviews for a product with ID \"PROD123\". Here's what the rendered prompt might look like:</p> <pre><code>Summarize the reviews for product PROD123:\n\nReview 1: This laptop is amazing! The battery life is incredible, lasting me a full day of work without needing to charge. The display is crisp and vibrant, perfect for both work and entertainment. The only minor drawback is that it can get a bit warm during intensive tasks.\n\nReview 2: I'm disappointed with this purchase. While the laptop looks sleek, its performance is subpar. It lags when running multiple applications, and the fan noise is quite noticeable. On the positive side, the keyboard is comfortable to type on.\n\nReview 3: Decent laptop for the price. It handles basic tasks well, but struggles with more demanding software. The build quality is solid, and I appreciate the variety of ports. Battery life is average, lasting about 6 hours with regular use.\n\nReview 4: Absolutely love this laptop! It's lightweight yet powerful, perfect for my needs as a student. The touchpad is responsive, and the speakers produce surprisingly good sound. My only wish is that it had a slightly larger screen.\n\nReview 5: Mixed feelings about this product. The speed and performance are great for everyday use and light gaming. However, the webcam quality is poor, which is a letdown for video calls. The design is sleek, but the glossy finish attracts fingerprints easily.\n</code></pre> <p>This example shows how the prompt template is filled with actual review data for a specific product. The language model would then process this prompt to generate a summary of the reviews for the product.</p>"},{"location":"operators/reduce/#scratchpad-technique","title":"Scratchpad Technique","text":"<p>When doing an incremental reduce, the task may require intermediate state that is not represented in the output. For example, if the task is to determine all features more than one person liked about the product, we need some intermediate state to keep track of the features that have been liked once, so if we see the same feature liked again, we can update the output. DocETL maintains an internal \"scratchpad\" to handle this.</p>"},{"location":"operators/reduce/#how-it-works","title":"How it works","text":"<ol> <li>The process starts with an empty accumulator and an internal scratchpad.</li> <li>It sequentially folds in batches of more than one element at a time.</li> <li>The internal scratchpad tracks additional state necessary for accurately solving tasks incrementally. The LLM decides what to write to the scratchpad.</li> <li>During each internal LLM call, the current scratchpad state is used along with the accumulated output and new inputs.</li> <li>The LLM updates both the accumulated output and the internal scratchpad, which are used in the next fold operation.</li> </ol> <p>The scratchpad technique is handled internally by DocETL, allowing users to define complex reduce operations without worrying about the complexities of state management across batches. Users provide their reduce and fold prompts focusing on the desired output, while DocETL uses the scratchpad technique behind the scenes to ensure accurate tracking of trends and efficient processing of large datasets.</p>"},{"location":"operators/reduce/#value-sampling","title":"Value Sampling","text":"<p>For very large groups, you can use value sampling to process a representative subset of the data. This can significantly reduce processing time and costs.</p> <p>The following table outlines the available value sampling methods:</p> Method Description random Randomly select a subset of values first_n Select the first N values cluster Use K-means clustering to select representative samples semantic_similarity Select samples based on semantic similarity to a query <p>To enable value sampling, add a <code>value_sampling</code> configuration to your reduce operation. The configuration should specify the method, sample size, and any additional parameters required by the chosen method.</p> <p>Value Sampling Configuration</p> <pre><code>- name: sampled_reduce\n  type: reduce\n  reduce_key: product_id\n  prompt: |\n    Summarize the reviews for product {{ inputs[0].product_id }}:\n    {% for item in inputs %}\n    Review {{ loop.index }}: {{ item.review }}\n    {% endfor %}\n  value_sampling:\n    enabled: true\n    method: cluster\n    sample_size: 50\n  output:\n    schema:\n      summary: string\n</code></pre> <p>In this example, the Reduce operation will use K-means clustering to select a representative sample of 50 reviews for each product_id.</p> <p>For semantic similarity sampling, you can use a query to select the most relevant samples. This is particularly useful when you want to focus on specific aspects of the data.</p> <p>Semantic Similarity Sampling</p> <pre><code>- name: sampled_reduce_sem_sim\n  type: reduce\n  reduce_key: product_id\n  prompt: |\n    Summarize the reviews for product {{ inputs[0].product_id }}, focusing on comments about battery life and performance:\n    {% for item in inputs %}\n    Review {{ loop.index }}: {{ item.review }}\n    {% endfor %}\n  value_sampling:\n    enabled: true\n    method: sem_sim\n    sample_size: 30\n    embedding_model: text-embedding-3-small\n    embedding_keys:\n      - review\n    query_text: \"Battery life and performance\"\n  output:\n    schema:\n      summary: string\n</code></pre> <p>In this example, the Reduce operation will use semantic similarity to select the 30 reviews most relevant to battery life and performance for each product_id. This allows you to focus the summarization on specific aspects of the product reviews.</p>"},{"location":"operators/reduce/#lineage","title":"Lineage","text":"<p>The Reduce operation supports lineage, which allows you to track the original input data for each output. This can be useful for debugging and auditing. To enable lineage, add a <code>lineage</code> configuration to your reduce operation, specifying the keys to include in the lineage. For example:</p> <pre><code>- name: summarize_reviews_by_category\n  type: reduce\n  reduce_key: category\n  prompt: |\n    Summarize the reviews for category {{ inputs[0].category }}:\n    {% for item in inputs %}\n    Review {{ loop.index }}: {{ item.review }}\n    {% endfor %}\n  output:\n    schema:\n      summary: string\n    lineage:\n      - product_id\n</code></pre> <p>This output will include a list of all product_ids for each category in the lineage, saved under the key <code>summarize_reviews_by_category_lineage</code>.</p>"},{"location":"operators/reduce/#best-practices","title":"Best Practices","text":"<ol> <li>Choose Appropriate Keys: Select <code>reduce_key</code>(s) that logically group your data for the desired aggregation.</li> <li>Design Effective Prompts: Create prompts that clearly instruct the model on how to aggregate or summarize the grouped data.</li> <li>Consider Data Size: For large datasets, use incremental folding and value sampling to manage processing efficiently.</li> <li>Optimize Your Pipeline: Use <code>docetl build pipeline.yaml</code> to optimize your pipeline, which can introduce efficient merge operations and resolve steps if needed.</li> <li>Balance Precision and Efficiency: When dealing with very large groups, consider using value sampling to process a representative subset of the data.</li> </ol>"},{"location":"operators/resolve/","title":"Resolve Operation","text":"<p>The Resolve operation in DocETL identifies and canonicalizes duplicate entities in your data. It's particularly useful when dealing with inconsistencies that can arise from LLM-generated content, or data from multiple sources.</p>"},{"location":"operators/resolve/#motivation","title":"Motivation","text":"<p>Map operations executed by LLMs may sometimes yield inconsistent results, even when referring to the same entity. For example, when extracting patient names from medical transcripts, you might end up with variations like \"Mrs. Smith\" and \"Jane Smith\" for the same person. In such cases, a Resolve operation on the <code>patient_name</code> field can help standardize patient names before conducting further analysis.</p>"},{"location":"operators/resolve/#example-standardizing-patient-names","title":"\ud83d\ude80 Example: Standardizing Patient Names","text":"<p>Let's see a practical example of using the Resolve operation to standardize patient names extracted from medical transcripts.</p> <pre><code>- name: standardize_patient_names\n  type: resolve\n  optimize: true\n  comparison_prompt: |\n    Compare the following two patient name entries:\n\n    Patient 1: {{ input1.patient_name }}\n    Date of Birth 1: {{ input1.date_of_birth }}\n\n    Patient 2: {{ input2.patient_name }}\n    Date of Birth 2: {{ input2.date_of_birth }}\n\n    Are these entries likely referring to the same patient? Consider name similarity and date of birth. Respond with \"True\" if they are likely the same patient, or \"False\" if they are likely different patients.\n  resolution_prompt: |\n    Standardize the following patient name entries into a single, consistent format:\n\n    {% for entry in inputs %}\n    Patient Name {{ loop.index }}: {{ entry.patient_name }}\n    {% endfor %}\n\n    Provide a single, standardized patient name that represents all the matched entries. Use the format \"LastName, FirstName MiddleInitial\" if available.\n  output:\n    schema:\n      patient_name: string\n</code></pre> <p>This Resolve operation processes patient names to identify and standardize duplicates:</p> <ol> <li>Compares all pairs of patient names using the <code>comparison_prompt</code>. In the prompt, you can reference to the documents via <code>input1</code> and <code>input2</code>.</li> <li>For identified duplicates, it applies the <code>resolution_prompt</code> to generate a standardized name. You can reference all matched entries via the <code>inputs</code> variable.</li> </ol> <p>Note: The prompt templates use Jinja2 syntax, allowing you to reference input fields directly (e.g., <code>input1.patient_name</code>).</p> <p>Automatic Blocking</p> <p>If you don't specify any blocking configuration (<code>blocking_threshold</code>, <code>blocking_conditions</code>, or <code>limit_comparisons</code>), the Resolve operation will automatically compute an optimal embedding-based blocking threshold at runtime. It samples pairs from your data, runs LLM comparisons on the sample, and finds a threshold that achieves 95% recall by default. You can adjust this with the <code>blocking_target_recall</code> parameter.</p>"},{"location":"operators/resolve/#blocking","title":"Blocking","text":"<p>To improve efficiency, the Resolve operation supports \"blocking\" - a technique to reduce the number of comparisons by only comparing entries that are likely to be matches. DocETL supports two types of blocking that work together:</p> <ol> <li>Code-based blocking: Apply custom Python expressions to determine if a pair should be compared.</li> <li>Embedding-based blocking: Compare embeddings of specified fields and only process pairs above a certain similarity threshold.</li> </ol>"},{"location":"operators/resolve/#how-blocking-works","title":"How Blocking Works","text":"<p>The Resolve operation creates a union of pairs that pass either blocking method: - First, pairs that satisfy any of the <code>blocking_conditions</code> are selected - Then, pairs that meet the <code>blocking_threshold</code> for embedding similarity are added (if not already included) - When sampling is needed (via <code>limit_comparisons</code>), code-based pairs are prioritized over embedding-based pairs</p> <p>Here's an example of a Resolve operation with both blocking methods:</p> <pre><code>- name: standardize_patient_names\n  type: resolve\n  comparison_prompt: |\n    # (Same as previous example)\n  resolution_prompt: |\n    # (Same as previous example)\n  output:\n    schema:\n      patient_name: string\n  blocking_keys:\n    - last_name\n    - date_of_birth\n  blocking_threshold: 0.8\n  blocking_conditions:\n    - \"input1['last_name'][:2].lower() == input2['last_name'][:2].lower()\"\n    - \"input1['first_name'][:2].lower() == input2['first_name'][:2].lower()\"\n    - \"input1['date_of_birth'] == input2['date_of_birth']\"\n    - \"input1['ssn'][-4:] == input2['ssn'][-4:]\"\n</code></pre> <p>In this example, pairs will be considered for comparison if they satisfy any of the following:</p> <p>Code-based conditions: - The <code>last_name</code> fields start with the same two characters, OR - The <code>first_name</code> fields start with the same two characters, OR - The <code>date_of_birth</code> fields match exactly, OR - The last four digits of the <code>ssn</code> fields match</p> <p>OR</p> <p>Embedding-based condition: - The embedding similarity of their <code>last_name</code> and <code>date_of_birth</code> fields is above 0.8</p> <p>This union approach maximizes recall while maintaining efficiency - exact rule matches are preserved while semantic similarities that might not match the rules are also captured.</p>"},{"location":"operators/resolve/#how-the-comparison-algorithm-works","title":"How the Comparison Algorithm Works","text":"<p>After determining eligible pairs for comparison, the Resolve operation uses a Union-Find (Disjoint Set Union) algorithm to efficiently group similar items. Here's a breakdown of the process:</p> <ol> <li>Initialization: Each item starts in its own cluster.</li> <li>Pair Generation: All possible pairs of items are generated for comparison.</li> <li>Batch Processing: Pairs are processed in batches (controlled by <code>compare_batch_size</code>).</li> <li>Comparison: For each batch:    a. An LLM performs pairwise comparisons to determine if items match.    b. Matching pairs trigger a <code>merge_clusters</code> operation to combine their clusters.</li> <li>Iteration: Steps 3-4 repeat until all pairs are compared.</li> <li>Result Collection: All non-empty clusters are collected as the final result.</li> </ol> <p>Efficiency</p> <p>The batch processing of comparisons allows for efficient, incremental clustering as matches are found, without needing to rebuild the entire cluster structure after each match. This allows for parallelization of LLM calls, improving overall performance. However, this also limits parallelism to the batch size, so choose an appropriate value for <code>compare_batch_size</code> based on your dataset size and system capabilities.</p>"},{"location":"operators/resolve/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>type</code>: Must be set to \"resolve\".</li> <li><code>comparison_prompt</code>: The prompt template to use for comparing potential matches.</li> <li><code>resolution_prompt</code>: The prompt template to use for reducing matched entries.</li> <li><code>output</code>: Schema definition for the output from the LLM.</li> </ul>"},{"location":"operators/resolve/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>embedding_model</code> The model to use for creating embeddings Falls back to <code>default_model</code> <code>resolution_model</code> The language model to use for reducing matched entries Falls back to <code>default_model</code> <code>comparison_model</code> The language model to use for comparing potential matches Falls back to <code>default_model</code> <code>blocking_keys</code> List of keys to use for initial blocking All keys in the input data <code>blocking_threshold</code> Embedding similarity threshold for considering entries as potential matches Auto-computed if not set <code>blocking_target_recall</code> Target recall when auto-computing blocking threshold (0.0 to 1.0) 0.95 <code>blocking_conditions</code> List of conditions for initial blocking [] <code>input</code> Specifies the schema or keys to subselect from each item to pass into the prompts All keys from input items <code>embedding_batch_size</code> The number of entries to send to the embedding model at a time 1000 <code>compare_batch_size</code> The number of entity pairs processed in each batch during the comparison phase 500 <code>limit_comparisons</code> Maximum number of comparisons to perform None <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>sample</code> Number of samples to use for the operation None <code>litellm_completion_kwargs</code> Additional parameters to pass to LiteLLM completion calls. {} <code>bypass_cache</code> If true, bypass the cache for this operation. False"},{"location":"operators/resolve/#best-practices","title":"Best Practices","text":"<ol> <li>Anticipate Resolve Needs: If you anticipate needing a Resolve operation and want to control the prompts, create it in your pipeline and let the optimizer find the appropriate blocking rules and thresholds.</li> <li>Let the Optimizer Help: The optimizer can detect if you need a Resolve operation (e.g., because there's a downstream reduce operation you're optimizing) and can create a Resolve operation with suitable prompts and blocking rules.</li> <li>Effective Comparison Prompts: Design comparison prompts that consider all relevant factors for determining matches.</li> <li>Detailed Resolution Prompts: Create resolution prompts that effectively standardize or combine information from matched records.</li> <li>Appropriate Model Selection: Choose suitable models for embedding (if used) and language tasks.</li> <li>Optimize Batch Size: If you expect to compare a large number of pairs, consider increasing the <code>compare_batch_size</code>. This parameter effectively limits parallelism, so a larger value can improve performance for large datasets.</li> </ol> <p>Balancing Batch Size</p> <p>While increasing <code>compare_batch_size</code> can improve parallelism, be cautious not to set it too high. Extremely large batch sizes might overwhelm system memory or exceed API rate limits. Consider your system's capabilities and the characteristics of your dataset when adjusting this parameter.</p> <p>The Resolve operation is particularly useful for data cleaning, deduplication, and creating standardized records from multiple data sources. It can significantly improve data quality and consistency in your dataset.</p>"},{"location":"operators/sample/","title":"Sample operation","text":"<p>The Sample operation in DocETL samples items from the input. It is meant mostly as a debugging tool:</p> <p>Insert it before the last operation, the one you're currently trying to add to the end of a working pipeline, to limit the amount of data it will be fed, so that the run time is small enough to comfortably debug its prompt. Once it seems to be working, you can remove the sample operation. You can then repeat this for each operation you add while developing your pipeline!</p>"},{"location":"operators/sample/#example","title":"\ud83d\ude80 Example:","text":"<pre><code>- name: sample_concepts\n  type: sample\n  method: uniform\n  samples: 0.1\n  stratify_key: category\n  random_state: 42\n</code></pre> <p>This sample operation will return a pseudo-randomly selected 10% of the samples (samples: 0.1). The random selection will be seeded with a constant (42), meaning the same sample will be returned if you rerun the pipeline (If no random state is given, a different sample will be returned every time). Additionally, the random sampling will sample each value of the category key proportionally.</p>"},{"location":"operators/sample/#required-parameters","title":"Required Parameters","text":"<ul> <li>name: A unique name for the operation.</li> <li>type: Must be set to \"sample\".</li> <li>method: The sampling method to use. Can be \"uniform\", \"outliers\", \"custom\", \"first\", \"top_embedding\", or \"top_fts\".</li> <li>samples: Either a list of key-value pairs representing document ids and values, an integer count of samples, or a float fraction of samples.</li> </ul>"},{"location":"operators/sample/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default random_state An integer to seed the random generator with None stratify_key Key(s) to stratify by. Can be a string or list of strings None samples_per_group When stratifying, sample N items per group vs. proportionally False method_kwargs Additional parameters for specific methods (e.g., outliers) {}"},{"location":"operators/sample/#sampling-methods","title":"Sampling Methods","text":""},{"location":"operators/sample/#uniform-sampling","title":"Uniform Sampling","text":"<p>Randomly samples items from the input data. When combined with stratification, maintains the distribution of the stratified groups.</p> <pre><code>- name: uniform_sample\n  type: sample\n  method: uniform\n  samples: 100\n</code></pre>"},{"location":"operators/sample/#first-sampling","title":"First Sampling","text":"<p>Takes the first N items from the input. When combined with stratification, takes proportionally from each group.</p> <pre><code>- name: first_sample\n  type: sample\n  method: first\n  samples: 50\n</code></pre>"},{"location":"operators/sample/#outlier-sampling","title":"Outlier Sampling","text":"<p>Samples based on distance from a center point in embedding space. Specify the following in method_kwargs:</p> <ul> <li>embedding_keys: A list of keys to use for creating embeddings.</li> <li>std: The number of standard deviations to use as the cutoff for outliers.</li> <li>samples: The number or fraction of samples to consider as outliers.</li> <li>keep: Whether to keep (true) or remove (false) the outliers. Defaults to false.</li> <li>center: (Optional) A dictionary specifying the center point for distance calculations.</li> </ul> <p>You must specify either \"std\" or \"samples\" in the method_kwargs, but not both.</p> <pre><code>- name: remove_outliers\n  type: sample\n  method: outliers\n  method_kwargs:\n    embedding_keys:\n      - concept\n      - description\n    std: 2\n    keep: false\n</code></pre>"},{"location":"operators/sample/#custom-sampling","title":"Custom Sampling","text":"<p>Samples specific items by matching key-value pairs. Stratification is not supported with custom sampling.</p> <pre><code>- name: custom_sample\n  type: sample\n  method: custom\n  samples:\n    - id: 1\n    - id: 5\n</code></pre>"},{"location":"operators/sample/#top-embedding-sampling","title":"Top Embedding Sampling","text":"<p>Retrieves the top N most similar items to a query based on semantic similarity using embeddings. Requires the following in method_kwargs:</p> <ul> <li>keys: A list of keys to use for creating embeddings</li> <li>query: The query string to match against (supports Jinja templates)</li> <li>embedding_model: (Optional) The embedding model to use. Defaults to \"text-embedding-3-small\"</li> </ul> <pre><code>- name: semantic_search\n  type: sample\n  method: top_embedding\n  samples: 10\n  method_kwargs:\n    keys:\n      - title\n      - content\n    query: \"machine learning applications in healthcare\"\n    embedding_model: text-embedding-3-small\n</code></pre> <p>With Jinja template for dynamic queries:</p> <pre><code>- name: personalized_search\n  type: sample\n  method: top_embedding\n  samples: 5\n  method_kwargs:\n    keys:\n      - description\n    query: \"{{ input.user_query }}\"\n</code></pre>"},{"location":"operators/sample/#top-fts-sampling","title":"Top FTS Sampling","text":"<p>Retrieves the top N items using full-text search with BM25 algorithm. Requires the following in method_kwargs:</p> <ul> <li>keys: A list of keys to search within</li> <li>query: The query string for keyword matching (supports Jinja templates)</li> </ul> <pre><code>- name: keyword_search\n  type: sample\n  method: top_fts\n  samples: 20\n  method_kwargs:\n    keys:\n      - title\n      - content\n      - tags\n    query: \"python programming tutorial\"\n</code></pre> <p>With dynamic query:</p> <pre><code>- name: search_products\n  type: sample\n  method: top_fts\n  samples: 0.1  # Top 10% of results\n  method_kwargs:\n    keys:\n      - product_name\n      - description\n    query: \"{{ input.search_terms }}\"\n</code></pre>"},{"location":"operators/sample/#stratification","title":"Stratification","text":"<p>Stratification can be applied to \"uniform\", \"first\", \"outliers\", \"top_embedding\", and \"top_fts\" methods. It ensures that the sample maintains the distribution of specified key(s) in the data or retrieves top items from each stratum.</p>"},{"location":"operators/sample/#single-key-stratification","title":"Single Key Stratification","text":"<pre><code>- name: stratified_sample\n  type: sample\n  method: uniform\n  samples: 0.2\n  stratify_key: category\n</code></pre>"},{"location":"operators/sample/#multiple-key-stratification","title":"Multiple Key Stratification","text":"<p>When using multiple keys, stratification is based on the combination of values:</p> <pre><code>- name: multi_stratified_sample\n  type: sample\n  method: uniform\n  samples: 50\n  stratify_key: \n    - type\n    - size\n</code></pre>"},{"location":"operators/sample/#samples-per-group","title":"Samples Per Group","text":"<p>Instead of proportional sampling, you can sample a fixed number from each stratum:</p> <pre><code>- name: stratified_per_group\n  type: sample\n  method: uniform\n  samples: 10  # Sample 10 items from each group\n  stratify_key: category\n  samples_per_group: true\n</code></pre> <p>This also works with fractions:</p> <pre><code>- name: stratified_fraction_per_group\n  type: sample\n  method: uniform\n  samples: 0.3  # Sample 30% from each group\n  stratify_key: category\n  samples_per_group: true\n</code></pre>"},{"location":"operators/sample/#complete-examples","title":"Complete Examples","text":"<p>Stratified outlier detection:</p> <pre><code>- name: stratified_outliers\n  type: sample\n  method: outliers\n  stratify_key: document_type\n  method_kwargs:\n    embedding_keys:\n      - title\n      - content\n    std: 1.5\n    keep: false\n</code></pre> <p>Stratified first sampling with multiple keys:</p> <pre><code>- name: stratified_first\n  type: sample\n  method: first\n  samples: 100\n  stratify_key:\n    - category\n    - priority\n  samples_per_group: false  # Take proportionally from each combination\n</code></pre> <p>Outlier sampling with a custom center:</p> <pre><code>- name: centered_outliers\n  type: sample\n  method: outliers\n  method_kwargs:\n    embedding_keys:\n      - concept\n      - description\n    center:\n      concept: Tree house\n      description: A small house built among the branches of a tree for children to play in.\n    samples: 20  # Keep the 20 furthest items from the center\n    keep: true\n</code></pre> <p>Stratified semantic search - retrieve top documents from each category:</p> <pre><code>- name: stratified_semantic_search\n  type: sample\n  method: top_embedding\n  samples: 5  # Get top 5 from each category\n  stratify_key: category\n  samples_per_group: true\n  method_kwargs:\n    keys:\n      - title\n      - abstract\n    query: \"recent advances in artificial intelligence\"\n</code></pre> <p>Full-text search with multiple stratification keys:</p> <pre><code>- name: stratified_keyword_search\n  type: sample\n  method: top_fts\n  samples: 3\n  stratify_key:\n    - department\n    - priority\n  samples_per_group: true\n  method_kwargs:\n    keys:\n      - subject\n      - content\n    query: \"urgent customer complaint refund\"\n</code></pre>"},{"location":"operators/sample/#note-on-topk-operation","title":"Note on TopK Operation","text":"<p>For retrieval use cases, consider using the dedicated TopK operation which provides a cleaner interface specifically designed for top-k retrieval with three methods: - <code>embedding</code>: Semantic similarity search - <code>fts</code>: Full-text search using BM25 - <code>llm_compare</code>: LLM-based ranking</p> <p>The TopK operation offers the same functionality as the sample operation's <code>top_embedding</code> and <code>top_fts</code> methods, but with a more intuitive API for retrieval tasks.</p>"},{"location":"operators/split/","title":"Split Operation","text":"<p>The Split operation in DocETL is designed to divide long text content into smaller, manageable chunks. This is particularly useful when dealing with large documents that exceed the token limit of language models or when the LLM's performance degrades with increasing input size for complex tasks.</p>"},{"location":"operators/split/#motivation","title":"Motivation","text":"<p>Some common scenarios where the Split operation is valuable include:</p> <ul> <li>Processing long customer support transcripts to analyze specific sections</li> <li>Dividing extensive research papers or reports for detailed analysis</li> <li>Breaking down large legal documents to extract relevant clauses or sections</li> <li>Preparing long-form content for summarization or topic extraction</li> </ul>"},{"location":"operators/split/#operation-example-splitting-customer-support-transcripts","title":"\ud83d\ude80 Operation Example: Splitting Customer Support Transcripts","text":"<p>Here's an example of using the Split operation to divide customer support transcripts into manageable chunks:</p> <pre><code>- name: split_transcript\n  type: split\n  split_key: transcript\n  method: token_count\n  method_kwargs:\n    num_tokens: 500\n    model: gpt-4o-mini\n</code></pre> <p>This Split operation processes long customer support transcripts:</p> <ol> <li>Splits the 'transcript' field into chunks of approximately 500 tokens each.</li> <li>Uses the gpt-4o-mini model's tokenizer for accurate token counting.</li> <li>Generates multiple output items for each input item, one for each chunk.</li> </ol> <p>Note that chunks will not overlap in content.</p>"},{"location":"operators/split/#configuration","title":"Configuration","text":""},{"location":"operators/split/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>type</code>: Must be set to \"split\".</li> <li><code>split_key</code>: The key of the field containing the text to split.</li> <li><code>method</code>: The method to use for splitting. Options are \"delimiter\" and \"token_count\".</li> <li><code>method_kwargs</code>: A dictionary of keyword arguments for the splitting method.</li> <li>For \"delimiter\" method: <code>delimiter</code> (string) to use for splitting.</li> <li>For \"token_count\" method: <code>num_tokens</code> (integer) specifying the maximum number of tokens per chunk.</li> </ul>"},{"location":"operators/split/#optional-parameters-in-method_kwargs","title":"Optional Parameters in `method_kwargs","text":"Parameter Description Default <code>model</code> The language model's tokenizer to use Falls back to <code>default_model</code> <code>num_splits_to_group</code> Number of splits to group together into one chunk (only for \"delimiter\" method) 1 <code>sample</code> Number of samples to use for the operation None"},{"location":"operators/split/#splitting-methods","title":"Splitting Methods","text":""},{"location":"operators/split/#token-count-method","title":"Token Count Method","text":"<p>The token count method splits the text into chunks based on a specified number of tokens. This is useful when you need to ensure that each chunk fits within the token limit of your language model, or you know that smaller chunks lead to higher performance.</p>"},{"location":"operators/split/#delimiter-method","title":"Delimiter Method","text":"<p>The delimiter method splits the text based on a specified delimiter string. This is particularly useful when you want to split your text at logical boundaries, such as paragraphs or sections.</p> <p>Delimiter Method Example</p> <p>If you set the <code>delimiter</code> to <code>\"\\n\\n\"</code> (double newline) and <code>num_splits_to_group</code> to 3, each chunk will contain 3 paragraphs.</p> <pre><code>- name: split_by_paragraphs\n  type: split\n  split_key: document\n  method: delimiter\n  method_kwargs:\n    delimiter: \"\\n\\n\"\n  num_splits_to_group: 3\n</code></pre>"},{"location":"operators/split/#output","title":"Output","text":"<p>The Split operation generates multiple output items for each input item:</p> <ul> <li>All original key-value pairs from the input item.</li> <li><code>{split_key}_chunk</code>: The content of the split chunk.</li> <li><code>{op_name}_id</code>: A unique identifier for each original document.</li> <li><code>{op_name}_chunk_num</code>: The sequential number of the chunk within its original document.</li> </ul>"},{"location":"operators/split/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Analyzing Customer Frustration:    Split long support transcripts, then use a map operation to identify frustration indicators in each chunk, followed by a reduce operation to summarize frustration points across the chunks (per transcript).</p> </li> <li> <p>Document Summarization:    Split large documents, apply a map operation for section-wise summarization, then use a reduce operation to compile an overall summary.</p> </li> <li> <p>Topic Extraction from Research Papers:    Divide research papers into sections, use a map operation to extract key topics from each section, then apply a reduce operation to synthesize main themes across the entire paper.</p> </li> </ol>"},{"location":"operators/split/#end-to-end-pipeline-example-analyzing-customer-frustration","title":"\ud83d\ude80 End-to-End Pipeline Example: Analyzing Customer Frustration","text":"<p>Let's walk through a complete example of using Split, Map, and Reduce operations to analyze customer frustration in support transcripts.</p>"},{"location":"operators/split/#step-1-split-operation","title":"Step 1: Split Operation","text":"<pre><code>- name: split_transcript\n  type: split\n  split_key: transcript\n  method: token_count\n  method_kwargs:\n    num_tokens: 500\n    model: gpt-4o-mini\n</code></pre>"},{"location":"operators/split/#step-2-map-operation-identify-frustration-indicators","title":"Step 2: Map Operation (Identify Frustration Indicators)","text":"<pre><code>- name: identify_frustration\n  type: map\n  input:\n    - transcript_chunk\n  prompt: |\n    Analyze the following customer support transcript chunk for signs of customer frustration:\n\n    {{ input.transcript_chunk }}\n\n    Identify any indicators of frustration, such as:\n    1. Use of negative language\n    2. Repetition of issues\n    3. Expressions of dissatisfaction\n    4. Requests for escalation\n\n    Provide a list of frustration indicators found, if any.\n  output:\n    schema:\n      frustration_indicators: list[string]\n</code></pre>"},{"location":"operators/split/#step-3-reduce-operation-summarize-frustration-points","title":"Step 3: Reduce Operation (Summarize Frustration Points)","text":"<pre><code>- name: summarize_frustration\n  type: reduce\n  reduce_key: split_transcript_id\n  associative: false\n  prompt: |\n    Summarize the customer frustration points for this support transcript:\n\n    {% for item in inputs %}\n    Chunk {{ item.split_transcript_chunk_num }}:\n    {% for indicator in item.frustration_indicators %}\n    - {{ indicator }}\n    {% endfor %}\n    {% endfor %}\n\n    Provide a concise summary of the main frustration points and their frequency or intensity across the entire transcript.\n  output:\n    schema:\n      frustration_summary: string\n      primary_issues: list[string]\n      frustration_level: string # e.g., \"low\", \"medium\", \"high\"\n</code></pre> <p>Non-Associative Reduce Operation</p> <p>Note the <code>associative: false</code> parameter in the reduce operation. This is crucial when the order of the chunks matters for your analysis. It ensures that the reduce operation processes the chunks in the order they appear in the original transcript, which is often important for understanding the context and progression of customer frustration.</p>"},{"location":"operators/split/#explanation","title":"Explanation","text":"<ol> <li>The Split operation divides long transcripts into 500-token chunks.</li> <li>The Map operation analyzes each chunk for frustration indicators.</li> <li>The Reduce operation combines the frustration indicators from all chunks of a transcript, summarizing the overall frustration points, primary issues, and assessing the overall frustration level. The <code>associative: false</code> setting ensures that the chunks are processed in their original order.</li> </ol> <p>This pipeline allows for detailed analysis of customer frustration in long support transcripts, which would be challenging to process in a single pass due to token limitations or degraded LLM performance on very long inputs.</p>"},{"location":"operators/split/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose the Right Splitting Method: Use the token count method when working with models that have strict token limits. Use the delimiter method when you need to split at logical boundaries in your text.</p> </li> <li> <p>Balance Chunk Size: When using the token count method, choose a chunk size that balances between context preservation and model performance. Smaller chunks may lose context, while larger chunks may degrade model performance. The DocETL optimizer can find the chunk size that works best for your task, if you choose to use the optimizer.</p> </li> <li> <p>Consider Overlap: In some cases, you might want to implement overlap between chunks to maintain context. This isn't built into the Split operation, but you can achieve it by post-processing the split chunks.</p> </li> <li> <p>Use Appropriate Delimiters: When using the delimiter method, choose a delimiter that logically divides your text. Common choices include double newlines for paragraphs, or custom markers for document sections. When using the delimiter method, adjust the <code>num_splits_to_group</code> parameter to create chunks that contain an appropriate amount of context for your task.</p> </li> <li> <p>Mind the Order: If the order of chunks matters for your analysis, always set <code>associative: false</code> in your subsequent reduce operations.</p> </li> <li> <p>Optimize for Performance: For very large documents, consider using a combination of delimiter and token count methods. First split into large sections using delimiters, then apply token count splitting to ensure no chunk exceeds model limits.</p> </li> </ol> <p>By leveraging the Split operation effectively, you can process large documents efficiently and extract meaningful insights using subsequent map and reduce operations.</p>"},{"location":"operators/topk/","title":"TopK Operation","text":"<p>The TopK operation retrieves the most relevant items from your dataset using three different retrieval methods: semantic similarity, full-text search, or LLM-based comparison. It provides a clean, specialized interface for retrieval tasks where you need to find and rank the best matching documents based on specific criteria.</p>"},{"location":"operators/topk/#overview","title":"Overview","text":"<p>TopK is designed for retrieval and ranking use cases such as finding relevant documents for a query, filtering large datasets to the most important items, implementing retrieval-augmented generation (RAG) pipelines, and building recommendation systems. Unlike general sampling operations, TopK focuses specifically on retrieving the best matches according to your chosen method and criteria.</p> <p>The operation supports three distinct retrieval methods, each optimized for different use cases. The embedding method uses semantic similarity to find conceptually related documents, making it ideal when meaning matters more than exact words. The full-text search (FTS) method employs the BM25 algorithm for keyword-based retrieval, perfect for when specific terms are important. The LLM compare method leverages a language model to rank documents based on complex criteria that require reasoning and multi-factor comparisons.</p>"},{"location":"operators/topk/#configuration","title":"Configuration","text":""},{"location":"operators/topk/#core-parameters","title":"Core Parameters","text":"Parameter Type Description Required <code>method</code> <code>\"embedding\"</code> | <code>\"fts\"</code> | <code>\"llm_compare\"</code> Retrieval method to use Yes <code>k</code> <code>int</code> or <code>float</code> Number of items to retrieve (float = percentage) Yes <code>keys</code> <code>list[str]</code> Document fields to use for matching/comparison Yes <code>query</code> <code>str</code> Query or ranking criteria (Jinja templates supported for <code>embedding</code> and <code>fts</code> only) Yes"},{"location":"operators/topk/#method-specific-parameters","title":"Method-Specific Parameters","text":"Parameter Type Methods Description Default <code>embedding_model</code> <code>str</code> <code>embedding</code>, <code>llm_compare</code> Model for embeddings <code>\"text-embedding-3-small\"</code> <code>model</code> <code>str</code> <code>llm_compare</code> LLM model for comparisons Required for <code>llm_compare</code> <code>batch_size</code> <code>int</code> <code>llm_compare</code> Batch size for LLM ranking <code>10</code> <code>stratify_key</code> <code>str</code> or <code>list[str]</code> <code>embedding</code>, <code>fts</code> Keys for stratified retrieval <code>None</code>"},{"location":"operators/topk/#examples","title":"Examples","text":""},{"location":"operators/topk/#semantic-search-with-embeddings","title":"Semantic Search with Embeddings","text":"<p>When you need to find documents based on meaning rather than exact keywords, the embedding method excels. This example finds support tickets semantically similar to payment processing issues:</p> <pre><code>- name: find_relevant_tickets\n  type: topk\n  method: embedding\n  k: 5\n  keys: \n    - subject\n    - description\n    - customer_feedback\n  query: \"payment processing errors with international transactions\"\n  embedding_model: text-embedding-3-small\n</code></pre>"},{"location":"operators/topk/#keyword-search-with-fts","title":"Keyword Search with FTS","text":"<p>For cases where specific terms matter, such as searching a product catalog, the FTS method provides fast, accurate keyword matching without API costs:</p> <pre><code>- name: search_products\n  type: topk\n  method: fts\n  k: 20\n  keys:\n    - product_name\n    - description\n    - category\n    - tags\n  query: \"wireless noise cancelling headphones bluetooth\"\n</code></pre>"},{"location":"operators/topk/#complex-ranking-with-llm-compare","title":"Complex Ranking with LLM Compare","text":"<p>When you need to rank items based on multiple factors or subjective criteria, the LLM compare method allows you to specify complex ranking logic. Note that this method requires consistent criteria across all documents and doesn't support Jinja templates:</p> <pre><code>- name: screen_resumes\n  type: topk\n  method: llm_compare\n  k: 10\n  keys:\n    - skills\n    - experience\n    - education\n  query: |\n    Rank candidates based on their fit for a Senior Backend Engineer role requiring:\n    - 5+ years Python experience\n    - Distributed systems expertise\n    - Strong knowledge of PostgreSQL and Redis\n    - Experience with microservices architecture\n    - Leadership experience is a plus\n\n    Prioritize hands-on technical experience over academic credentials.\n  model: gpt-4o\n  batch_size: 5\n</code></pre>"},{"location":"operators/topk/#dynamic-queries-with-templates","title":"Dynamic Queries with Templates","text":"<p>The embedding and FTS methods support Jinja templates for dynamic queries that adapt based on input data. This enables personalized search experiences:</p> <pre><code>- name: personalized_search\n  type: topk\n  method: embedding\n  k: 10\n  keys:\n    - content\n    - tags\n  query: |\n    {{ input.user_preferences }} \n    Focus on {{ input.topic_of_interest }}\n    Exclude anything related to {{ input.blocked_topics }}\n</code></pre>"},{"location":"operators/topk/#stratified-retrieval","title":"Stratified Retrieval","text":"<p>Both embedding and FTS methods support stratification, which ensures you retrieve top items from each group. This is useful for maintaining diversity in results:</p> <pre><code>- name: recommendations_by_category\n  type: topk\n  method: fts\n  k: 3  # Get top 3 from each category\n  keys:\n    - product_name\n    - description\n  query: \"premium quality bestseller\"\n  stratify_key: category\n</code></pre>"},{"location":"operators/topk/#common-patterns","title":"Common Patterns","text":""},{"location":"operators/topk/#single-document-rag-pipeline","title":"Single-Document RAG Pipeline","text":"<p>A retrieval-augmented generation pipeline for answering questions about a single document typically retrieves the most relevant chunks, then synthesizes them into a coherent answer using reduce:</p> <pre><code># Step 1: Retrieve most relevant document chunks\n- name: retrieve_context\n  type: topk\n  method: embedding\n  k: 5\n  keys: [content]\n  query: \"{{ input.user_question }}\"\n\n# Step 2: Generate comprehensive answer from all retrieved chunks\n- name: generate_answer\n  type: reduce\n  reduce_key: user_question  # Group by the question\n  prompt: |\n    Based on the following document excerpts, provide a comprehensive answer to the question.\n\n    Question: {{ inputs[0].user_question }}\n\n    Retrieved context from document:\n    {% for chunk in inputs %}\n    - {{ chunk.content }}\n    {% endfor %}\n\n    Synthesize the information from all excerpts into a single, coherent answer.\n  output_schema:\n    answer: string\n</code></pre>"},{"location":"operators/topk/#multi-stage-filtering","title":"Multi-Stage Filtering","text":"<p>For complex retrieval tasks, you might combine multiple TopK operations with different methods, progressively refining your results:</p> <pre><code># Cast a wide net with keyword search\n- name: initial_search\n  type: topk\n  method: fts\n  k: 100\n  keys: [title, content]\n  query: \"machine learning\"\n\n# Refine with semantic search\n- name: refine_results\n  type: topk\n  method: embedding\n  k: 20\n  keys: [title, content]\n  query: \"practical applications of deep learning in healthcare\"\n\n# Final ranking with LLM\n- name: final_ranking\n  type: topk\n  method: llm_compare\n  k: 5\n  keys: [title, abstract, impact_factor]\n  query: \"Rank by potential clinical impact and implementation feasibility\"\n  model: gpt-4o\n</code></pre>"},{"location":"operators/topk/#performance-considerations","title":"Performance Considerations","text":"<p>Each retrieval method has different performance characteristics that should guide your choice. The FTS method is the fastest and has no API costs since it uses local BM25 scoring, making it ideal for high-volume or cost-sensitive applications. The embedding method requires API calls to generate embeddings for both documents and queries, but once computed, similarity matching is very fast. It provides excellent semantic understanding but at a moderate cost. The LLM compare method has the highest cost and slowest performance due to multiple LLM calls, but offers unmatched flexibility for complex ranking criteria.</p> <p>When optimizing performance, consider preprocessing embeddings offline for the embedding method, using FTS for initial filtering before applying more expensive methods, and carefully tuning the batch_size parameter for LLM compare to balance speed and accuracy.</p>"},{"location":"operators/topk/#implementation-details","title":"Implementation Details","text":""},{"location":"operators/topk/#embedding-method","title":"Embedding Method","text":"<p>The embedding method converts both your documents and query into high-dimensional vectors using the specified embedding model (defaulting to OpenAI's text-embedding-3-small). It then calculates cosine similarity between the query vector and each document vector, returning the k documents with highest similarity scores. The method handles text normalization and truncation automatically, and when stratification is enabled, it performs this process independently for each stratum.</p>"},{"location":"operators/topk/#fts-method","title":"FTS Method","text":"<p>The full-text search method uses the BM25 algorithm, the same ranking function used by search engines like Elasticsearch. Documents are tokenized and lowercase-normalized, with special characters removed. The BM25 scoring function considers term frequency (with saturation to prevent overweighting repeated terms), inverse document frequency (giving more weight to rare terms), and document length normalization. The implementation uses the rank-bm25 library for efficient scoring.</p>"},{"location":"operators/topk/#llm-compare-method","title":"LLM Compare Method","text":"<p>The LLM compare method delegates to the rank operation, which implements sophisticated comparison algorithms from human-powered sorting research. It starts with an initial ordering using embeddings, then applies sliding windows of documents for pairwise comparison by the LLM. The LLM evaluates documents in batches (controlled by batch_size) and produces rankings based on the specified criteria. This method requires consistent ranking criteria across all documents, which is why Jinja templates are not supported\u2014the LLM needs to compare all documents using the same criteria for fair ranking.</p>"},{"location":"operators/topk/#error-handling","title":"Error Handling","text":"<p>The TopK operation is designed to handle edge cases gracefully. If k exceeds the number of available documents, it returns all available items rather than failing. When specified keys are missing from some documents, it uses whatever fields are available. For the FTS method, documents with empty text after normalization are handled appropriately, and for the embedding method, API failures include automatic retries with exponential backoff.</p>"},{"location":"operators/unnest/","title":"Unnest Operation","text":"<p>The Unnest operation in DocETL is designed to expand an array field or a dictionary in the input data into multiple items. This operation is particularly useful when you need to process or analyze individual elements of an array or specific fields of a nested dictionary separately.</p> <p>How Unnest Works</p> <p>The Unnest operation behaves differently depending on the type of data being unnested:</p> <ul> <li>For list-type unnesting: It replaces the original key with each individual element from the list.</li> <li>For dictionary-type unnesting: It adds new keys to the parent dictionary based on the <code>expand_fields</code> parameter.</li> </ul> <p>Unnest does not have an output schema. It modifies the structure of your data in place.</p>"},{"location":"operators/unnest/#motivation","title":"Motivation","text":"<p>The Unnest operation is valuable in scenarios where you need to:</p> <ul> <li>Process individual items from a list of products in an order</li> <li>Analyze separate entries in a list of comments or reviews</li> <li>Expand nested data structures for more granular processing</li> <li>Flatten complex data structures for easier analysis</li> </ul>"},{"location":"operators/unnest/#configuration","title":"Configuration","text":""},{"location":"operators/unnest/#required-parameters","title":"Required Parameters","text":"Parameter Description type Must be set to \"unnest\" name A unique name for the operation unnest_key The key of the array field to unnest"},{"location":"operators/unnest/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default keep_empty If true, empty arrays being exploded will be kept in the output (with value None) false expand_fields A list of fields to expand from the nested dictionary into the parent dictionary, if unnesting a dict [] recursive If true, the unnest operation will be applied recursively to nested arrays false depth The maximum depth for recursive unnesting (only applicable if recursive is true) inf sample Number of samples to use for the operation None"},{"location":"operators/unnest/#output","title":"Output","text":"<p>The Unnest operation modifies the structure of your data:</p> <ul> <li>For list-type unnesting: It generates multiple output items for each input item, replacing the original array in the <code>unnest_key</code> field with individual elements.</li> <li>For dictionary-type unnesting: It expands the specified fields into the parent dictionary.</li> </ul> <p>All other original key-value pairs from the input item are preserved in the output.</p> <p>Note</p> <p>When unnesting dictionaries, the original nested dictionary is preserved in the output, and the specified fields are expanded into the parent dictionary.</p>"},{"location":"operators/unnest/#use-cases","title":"Use Cases","text":"<ol> <li>Product Analysis in Orders: Unnest a list of products in each order, then use a map operation to analyze each product individually.</li> <li>Comment Sentiment Analysis: Unnest a list of comments for each post, enabling sentiment analysis on individual comments.</li> <li>Nested Data Structure Flattening: Unnest complex nested data structures to create a flattened dataset for easier analysis or processing.</li> <li>Processing Time Series Data: Unnest time series data stored in arrays to analyze individual time points.</li> </ol>"},{"location":"operators/unnest/#example-analyzing-product-reviews","title":"Example: Analyzing Product Reviews","text":"<p>Let's walk through an example of using the Unnest operation to prepare product reviews for detailed analysis.</p> <pre><code>- name: extract_salient_quotes\n  type: map\n  prompt: |\n    For the following product review, extract up to 3 salient quotes that best represent the reviewer's opinion:\n\n    {{ input.review_text }}\n\n    For each quote, provide the text and its sentiment (positive, negative, or neutral).\n  output:\n    schema:\n      salient_quotes: list[string]\n\n- name: unnest_quotes\n  type: unnest\n  unnest_key: salient_quotes\n\n- name: analyze_quote\n  type: map\n  prompt: |\n    Analyze the following quote from a product review:\n\n    Quote &amp; information: {{ input.salient_quotes }}\n    Review text: {{ input.review_text }}\n\n    Provide a detailed analysis of the quote, including:\n    1. The specific aspect of the product being discussed\n    2. The strength of the sentiment (-5 to 5, where -5 is extremely negative and 5 is extremely positive)\n    3. Any key terms or phrases that stand out\n\n  output:\n    schema:\n      product_aspect: string\n      sentiment_strength: number\n      key_terms: list[string]\n</code></pre> <p>This example demonstrates how the Unnest operation fits into a pipeline for analyzing product reviews:</p> <ol> <li>The first Map operation extracts salient quotes from each review.</li> <li>The Unnest operation expands the 'salient_quotes' array, creating individual items for each quote. Each quote can now be accessed via <code>input.salient_quotes</code>.</li> <li>The second Map operation performs a detailed analysis on each individual quote.</li> </ol> <p>By unnesting the quotes, we enable more granular analysis that wouldn't be possible if we processed the entire review as a single unit.</p>"},{"location":"operators/unnest/#advanced-features","title":"Advanced Features","text":""},{"location":"operators/unnest/#recursive-unnesting","title":"Recursive Unnesting","text":"<p>When dealing with deeply nested structures, you can use the <code>recursive</code> parameter to apply the unnest operation at multiple levels:</p> <pre><code>- name: recursive_unnest\n  type: unnest\n  unnest_key: nested_data\n  recursive: true\n  depth: 3 # Limit recursion to 3 levels deep\n</code></pre>"},{"location":"operators/unnest/#dictionary-expansion","title":"Dictionary Expansion","text":"<p>When unnesting dictionaries, you can use the <code>expand_fields</code> parameter to flatten specific fields into the parent structure:</p> <pre><code>- name: expand_user_data\n  type: unnest\n  unnest_key: user_info\n  expand_fields:\n    - name\n    - age\n    - location\n</code></pre> <p>In this case, <code>name</code>, <code>age</code>, and <code>location</code> would be added as new keys in the parent dictionary, alongside the original <code>user_info</code> key.</p>"},{"location":"operators/unnest/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose the Right Unnest Key: Ensure you're unnesting the correct field that contains the array or nested structure you want to expand.</p> </li> <li> <p>Consider Data Volume: Unnesting can significantly increase the number of items in your data stream. Be mindful of this when designing subsequent operations in your pipeline.</p> </li> <li> <p>Use Expand Fields Wisely: When unnesting dictionaries, use the <code>expand_fields</code> parameter to flatten your data structure if needed, but be cautious of potential key conflicts.</p> </li> <li> <p>Handle Empty Arrays: Decide whether empty arrays should be kept (using <code>keep_empty</code>) based on your specific use case and how subsequent operations should handle null values.</p> </li> <li> <p>Preserve Context: When unnesting, consider whether you need to carry forward any context from the parent item. The unnest operation preserves all other fields, which helps maintain context.</p> </li> </ol>"},{"location":"optimization/configuration/","title":"Advanced: Customizing Optimization","text":"<p>You can customize the optimization process for specific operations using the ``optimizer_config in your pipeline.</p>"},{"location":"optimization/configuration/#global-configuration","title":"Global Configuration","text":"<p>The following options can be applied globally to all operations in your pipeline during optimization:</p> <ul> <li> <p><code>num_retries</code>: The number of times to retry optimizing if the LLM agent fails. Default is 1.</p> </li> <li> <p><code>sample_sizes</code>: Override the default sample sizes for each operator type. Specify as a dictionary with operator types as keys and integer sample sizes as values.</p> </li> </ul> <p>Default sample sizes:</p> <pre><code>SAMPLE_SIZE_MAP = {\n    \"reduce\": 40,\n    \"map\": 5,\n    \"resolve\": 100,\n    \"equijoin\": 100,\n    \"filter\": 5,\n}\n</code></pre> <ul> <li> <p><code>judge_agent_model</code>: Specify the model to use for the judge agent. Default is <code>gpt-4o-mini</code>.</p> </li> <li> <p><code>rewrite_agent_model</code>: Specify the model to use for the rewrite agent. Default is <code>gpt-4o</code>.</p> </li> <li> <p><code>litellm_kwargs</code>: Specify the litellm kwargs to use for the optimization. Default is <code>{}</code>.</p> </li> </ul>"},{"location":"optimization/configuration/#equijoin-configuration","title":"Equijoin Configuration","text":"<ul> <li><code>target_recall</code>: Change the default target recall (default is 0.95).</li> </ul>"},{"location":"optimization/configuration/#resolve-configuration","title":"Resolve Configuration","text":"<ul> <li><code>target_recall</code>: Specify the target recall for the resolve operation.</li> </ul>"},{"location":"optimization/configuration/#reduce-configuration","title":"Reduce Configuration","text":"<ul> <li><code>synthesize_resolve</code>: Set to <code>False</code> if you definitely don't want a resolve operation synthesized or want to turn off this rewrite rule.</li> </ul>"},{"location":"optimization/configuration/#map-configuration","title":"Map Configuration","text":"<ul> <li><code>force_chunking_plan</code>: Set to <code>True</code> if you want the the optimizer to force plan that breaks up the input documents into chunks.</li> <li><code>plan_types</code>: Specify the plan types to consider for the map operation. The available plan types are:</li> <li><code>chunk</code>: Breaks up the input documents into chunks (i.e., data decomposition).</li> <li><code>proj_synthesis</code>: Synthesizes 1+ projections (i.e., task decomposition).</li> <li><code>glean</code>: Synthesizes a glean plan (i.e., uses LLM as a judge to refine the output).</li> </ul>"},{"location":"optimization/configuration/#example-configuration","title":"Example Configuration","text":"<p>Here's an example of how to use the <code>optimizer_config</code> in your pipeline:</p> <pre><code>optimizer_config:\n  rewrite_agent_model: gpt-4o-mini\n  judge_agent_model: gpt-4o-mini\n  litellm_kwargs:\n    temperature: 0.5\n  num_retries: 2\n  sample_sizes:\n    map: 10\n    reduce: 50\n  reduce:\n    synthesize_resolve: false\n  map:\n    plan_types: # Considers all these plan types\n      - chunk\n      - proj_synthesis\n      - glean\n\noperations:\n  - name: extract_medications\n    type: map\n    optimize: true\n    recursively_optimize: true # Recursively optimize the map operation (i.e., optimize any new operations that are synthesized)\n    # ... other configuration ...\n\n  - name: summarize_prescriptions\n    type: reduce\n    optimize: true\n    # ... other configuration ...\n# ... rest of the pipeline configuration ...\n</code></pre> <p>This configuration will:</p> <ol> <li>Retry optimization up to 2 times for each operation if the LLM agent fails.</li> <li>Use custom sample sizes for map (10) and reduce (50) operations.</li> <li>Prevent the synthesis of resolve operations for reduce operations.</li> <li>Consider all plan types for map operations.</li> </ol>"},{"location":"optimization/example/","title":"Running the Optimizer","text":"<p>Optimizer Stability</p> <p>The optimization process can be unstable, as well as resource-intensive (we've seen it take up to 10 minutes to optimize a single operation, spending up to ~$50 in API costs for end-to-end pipelines). We recommend optimizing one operation at a time and retrying if necessary, as results may vary between runs. This approach also allows you to confidently verify that each optimized operation is performing as expected before moving on to the next.</p> <p>See the API for more details on how to resume the optimizer from a failed run, by rerunning <code>docetl build pipeline.yaml --resume</code> (with the <code>--resume</code> flag).</p> <p>Also, you can use gpt-4o-mini for cheaper optimizations (rather than the default gpt-4o), which you can do via <code>docetl build pipeline.yaml --model=gpt-4o-mini</code>.</p> <p>To optimize your pipeline, start with your initial configuration and follow these steps:</p> <ol> <li> <p>Set <code>optimize: True</code> for the operation you want to optimize (start with the first operation, if you're not sure which one).</p> </li> <li> <p>Run the optimizer using the command <code>docetl build pipeline.yaml</code>. This will generate an optimized version in <code>pipeline_opt.yaml</code>.</p> </li> <li> <p>Review the optimized operation in <code>pipeline_opt.yaml</code>. If you're satisfied with the changes, copy the optimized operation back into your original <code>pipeline.yaml</code>.</p> </li> <li> <p>Move on to the next LLM-powered operation and repeat steps 1-3.</p> </li> <li> <p>Once all operations are optimized, your <code>pipeline.yaml</code> will contain the fully optimized pipeline.</p> </li> </ol> <p>When optimizing a resolve operation, the optimizer will also set blocking configurations and thresholds, saving you from manual configuration.</p> <p>Feeling Ambitious?</p> <p>You can run the optimizer on your entire pipeline by setting <code>optimize: True</code> for each operation you want to optimize. But sometimes the agent fails to find a better plan, and you'll need to manually intervene. We are exploring human-in-the-loop optimization, where the optimizer can ask for human feedback to improve its plans.</p>"},{"location":"optimization/example/#example-optimizing-a-medical-transcripts-pipeline","title":"Example: Optimizing a Medical Transcripts Pipeline","text":"<p>Let's walk through optimizing a pipeline for extracting medication information from medical transcripts. We'll start with an initial pipeline and optimize it step by step.</p>"},{"location":"optimization/example/#initial-pipeline","title":"Initial Pipeline","text":"<pre><code>datasets:\n  transcripts:\n    path: medical_transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_medications\n    type: map\n    optimize: true\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the transcript: {{ input.src }}\n      List all medications mentioned.\n\n  - name: unnest_medications\n    type: unnest\n    unnest_key: medication\n\n  - name: summarize_prescriptions\n    type: reduce\n    optimize: true\n    reduce_key:\n      - medication\n    output:\n      schema:\n        side_effects: str\n        uses: str\n    prompt: |\n      Summarize side effects and uses of {{ reduce_key }} from:\n      {% for value in inputs %}\n      Transcript {{ loop.index }}: {{ value.src }}\n      {% endfor %}\n\npipeline:\n  output:\n    path: medication_summaries.json\n    type: file\n  steps:\n    - input: transcripts\n      name: medical_info_extraction\n      operations:\n        - extract_medications\n        - unnest_medications\n        - summarize_prescriptions\n</code></pre>"},{"location":"optimization/example/#optimization-steps","title":"Optimization Steps","text":"<p>First, we'll optimize the <code>extract_medications</code> operation. Set <code>optimize: True</code> for this operation and run the optimizer. Review the changes and integrate them into your pipeline.</p> <p>Then, optimize the <code>summarize_prescriptions</code> operation by setting <code>optimize: True</code> and running <code>docetl build pipeline.yaml</code> again. The optimizer may suggest adding a resolve operation at this point, and will automatically configure blocking and thresholds. After completing all steps, your optimized pipeline might look like this:</p>"},{"location":"optimization/example/#optimized-pipeline","title":"Optimized Pipeline","text":"<pre><code>datasets:\n  transcripts:\n    path: medical_transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the transcript: {{ input.src }}\n      List all medications mentioned.\n    gleaning:\n      num_rounds: 1\n      validation_prompt: |\n        Evaluate the extraction for completeness and accuracy:\n        1. Are all medications, dosages, and symptoms from the transcript included?\n        2. Is the extracted information correct and relevant?\n\n  - name: unnest_medications\n    type: unnest\n    unnest_key: medication\n\n  - name: resolve_medications\n    type: resolve\n    blocking_keys:\n      - medication\n    blocking_threshold: 0.7\n    comparison_prompt: |\n      Compare medications:\n      1: {{ input1.medication }}\n      2: {{ input2.medication }}\n      Are these the same or closely related?\n    resolution_prompt: |\n      Standardize the name for:\n      {% for entry in inputs %}\n      - {{ entry.medication }}\n      {% endfor %}\n\n  - name: summarize_prescriptions\n    type: reduce\n    reduce_key:\n      - medication\n    output:\n      schema:\n        side_effects: str\n        uses: str\n    prompt: |\n      Summarize side effects and uses of {{ reduce_key }} from:\n      {% for value in inputs %}\n      Transcript {{ loop.index }}: {{ value.src }}\n      {% endfor %}\n    fold_batch_size: 10\n    fold_prompt: |\n      Update the existing summary of side effects and uses for {{ reduce_key }} based on the following additional transcripts:\n      {% for value in inputs %}\n      Transcript {{ loop.index }}: {{ value.src }}\n      {% endfor %}\n\n      Existing summary:\n      Side effects: {{ output.side_effects }}\n      Uses: {{ output.uses }}\n\n      Provide an updated and comprehensive summary, incorporating both the existing information and any new insights from the additional transcripts.\n\npipeline:\n  output:\n    path: medication_summaries.json\n    type: file\n  steps:\n    - input: transcripts\n      name: medical_info_extraction\n      operations:\n        - extract_medications\n        - unnest_medications\n        - resolve_medications\n        - summarize_prescriptions\n</code></pre> <p>This optimized pipeline now includes improved prompts, a resolve operation, and additional output fields for more comprehensive medication information extraction.</p> <p>Feedback Welcome</p> <p>We're continually improving the optimizer. Your feedback on its performance and usability is invaluable. Please share your experiences and suggestions!</p>"},{"location":"optimization/example/#optimizer-api","title":"Optimizer API","text":""},{"location":"optimization/example/#docetl.cli.build","title":"<code>docetl.cli.build(yaml_file=typer.Argument(..., help='Path to the YAML file containing the pipeline configuration'), optimizer=typer.Option('moar', '--optimizer', '-o', help=\"Optimizer to use: 'moar' (default) or 'v1' (deprecated)\"), max_threads=typer.Option(None, help='Maximum number of threads to use for running operations'), resume=typer.Option(False, help='Resume optimization from a previous build that may have failed'), save_path=typer.Option(None, help='Path to save the optimized pipeline configuration'))</code>","text":"<p>Build and optimize the configuration specified in the YAML file. Any arguments passed here will override the values in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>optimizer</code> <code>str</code> <p>Optimizer to use - 'moar' or 'v1' (required).</p> <code>Option('moar', '--optimizer', '-o', help=\"Optimizer to use: 'moar' (default) or 'v1' (deprecated)\")</code> <code>max_threads</code> <code>int | None</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous run. Defaults to False.</p> <code>Option(False, help='Resume optimization from a previous build that may have failed')</code> <code>save_path</code> <code>Path</code> <p>Path to save the optimized pipeline configuration.</p> <code>Option(None, help='Path to save the optimized pipeline configuration')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef build(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    optimizer: str = typer.Option(\n        \"moar\",\n        \"--optimizer\",\n        \"-o\",\n        help=\"Optimizer to use: 'moar' (default) or 'v1' (deprecated)\",\n    ),\n    max_threads: int | None = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n    resume: bool = typer.Option(\n        False, help=\"Resume optimization from a previous build that may have failed\"\n    ),\n    save_path: Path = typer.Option(\n        None, help=\"Path to save the optimized pipeline configuration\"\n    ),\n):\n    \"\"\"\n    Build and optimize the configuration specified in the YAML file.\n    Any arguments passed here will override the values in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        optimizer (str): Optimizer to use - 'moar' or 'v1' (required).\n        max_threads (int | None): Maximum number of threads to use for running operations.\n        resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n        save_path (Path): Path to save the optimized pipeline configuration.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    # Validate optimizer choice\n    if optimizer not in [\"moar\", \"v1\"]:\n        typer.echo(\n            f\"Error: optimizer must be 'moar' or 'v1', got '{optimizer}'\", err=True\n        )\n        raise typer.Exit(1)\n\n    # Load YAML to check for optimizer_config\n    import yaml as yaml_lib\n\n    with open(yaml_file, \"r\") as f:\n        config = yaml_lib.safe_load(f)\n\n    if optimizer == \"moar\":\n        optimizer_config = config.get(\"optimizer_config\", {})\n        if not optimizer_config:\n            example_yaml = \"\"\"optimizer_config:\n  type: moar\n  save_dir: ./moar_results\n  available_models:\n    - gpt-5\n    - gpt-4o\n  evaluation_file: workloads/medical/evaluate_medications.py\n  metric_key: medication_extraction_score\n  max_iterations: 40\n  model: gpt-5\"\"\"\n\n            error_panel = Panel(\n                f\"[bold red]Error:[/bold red] optimizer_config section is required in YAML for MOAR optimizer.\\n\\n\"\n                f\"[bold]Example:[/bold]\\n\"\n                f\"[dim]{example_yaml}[/dim]\\n\\n\"\n                f\"[yellow]Note:[/yellow] dataset_name is inferred from the 'datasets' section. \"\n                f\"dataset_path can optionally be specified in optimizer_config, otherwise it's inferred from the 'datasets' section.\",\n                title=\"[bold red]Missing optimizer_config[/bold red]\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            raise typer.Exit(1)\n\n        if optimizer_config.get(\"type\") != \"moar\":\n            error_panel = Panel(\n                f\"[bold red]Error:[/bold red] optimizer_config.type must be 'moar', got '[yellow]{optimizer_config.get('type')}[/yellow]'\",\n                title=\"[bold red]Invalid optimizer type[/bold red]\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            raise typer.Exit(1)\n\n        # Validate required fields in optimizer_config\n        required_fields = {\n            \"save_dir\": \"Output directory for MOAR results\",\n            \"available_models\": \"List of model names to use\",\n            \"evaluation_file\": \"Path to evaluation function file\",\n            \"metric_key\": \"Key to extract from evaluation results\",\n            \"max_iterations\": \"Number of MOARSearch iterations\",\n            \"model\": \"LLM model name for directive instantiation\",\n        }\n\n        missing_fields = [\n            field for field in required_fields if not optimizer_config.get(field)\n        ]\n        if missing_fields:\n            # Create a table for required fields\n            fields_table = Table(\n                show_header=True, header_style=\"bold cyan\", box=None, padding=(0, 2)\n            )\n            fields_table.add_column(\"Field\", style=\"yellow\")\n            fields_table.add_column(\"Description\", style=\"dim\")\n\n            for field, desc in required_fields.items():\n                style = \"bold red\" if field in missing_fields else \"dim\"\n                fields_table.add_row(f\"[{style}]{field}[/{style}]\", desc)\n\n            # Create example YAML\n            example_yaml = \"\"\"optimizer_config:\n  type: moar\n  save_dir: ./moar_results\n  available_models:\n    - gpt-5\n    - gpt-4o\n  evaluation_file: workloads/medical/evaluate_medications.py\n  metric_key: medication_extraction_score\n  max_iterations: 40\n  model: gpt-5\"\"\"\n\n            missing_list = \", \".join(\n                [f\"[bold red]{f}[/bold red]\" for f in missing_fields]\n            )\n\n            # Build error content with table rendered separately\n            from rich.console import Group\n\n            error_group = Group(\n                f\"[bold red]Missing required fields:[/bold red] {missing_list}\\n\",\n                \"[bold]Required fields:[/bold]\",\n                fields_table,\n                f\"\\n[bold]Example:[/bold]\\n[dim]{example_yaml}[/dim]\\n\",\n                \"[yellow]Note:[/yellow] dataset_name is inferred from the 'datasets' section. \"\n                \"dataset_path can optionally be specified in optimizer_config, otherwise it's inferred from the 'datasets' section.\",\n            )\n\n            error_panel = Panel(\n                error_group,\n                title=\"[bold red]Missing Required Fields[/bold red]\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            raise typer.Exit(1)\n\n        # Run MOAR optimization\n        from docetl.moar.cli_helpers import run_moar_optimization\n\n        try:\n            results = run_moar_optimization(\n                yaml_path=str(yaml_file),\n                optimizer_config=optimizer_config,\n            )\n            typer.echo(\"\\n\u2705 MOAR optimization completed successfully!\")\n            typer.echo(f\"   Results saved to: {optimizer_config.get('save_dir')}\")\n            if results.get(\"evaluation_file\"):\n                typer.echo(f\"   Evaluation: {results['evaluation_file']}\")\n        except Exception as e:\n            typer.echo(f\"Error running MOAR optimization: {e}\", err=True)\n            raise typer.Exit(1)\n\n    else:  # v1 optimizer (deprecated)\n        console.print(\n            Panel(\n                \"[bold yellow]Warning:[/bold yellow] The V1 optimizer is deprecated. \"\n                \"Please use MOAR optimizer instead: [bold]docetl build pipeline.yaml --optimizer moar[/bold]\",\n                title=\"[bold yellow]Deprecated Optimizer[/bold yellow]\",\n                border_style=\"yellow\",\n            )\n        )\n        runner = DSLRunner.from_yaml(str(yaml_file), max_threads=max_threads)\n        runner.optimize(\n            save=True,\n            return_pipeline=False,\n            resume=resume,\n            save_path=save_path,\n        )\n</code></pre>"},{"location":"optimization/fallback-models/","title":"Fallback Models","text":"<p>Fallback models provide automatic failover to alternative models when API errors or content errors occur with your primary model. This feature improves pipeline reliability and reduces failures due to temporary API issues or model unavailability.</p>"},{"location":"optimization/fallback-models/#overview","title":"Overview","text":"<p>When configured, DocETL will automatically try fallback models in sequence if:</p> <ul> <li>The primary model encounters an API error (rate limits, service unavailability, content warning errors, etc.)</li> <li>The primary model returns invalid content that cannot be parsed</li> <li>The primary model fails to respond within expected timeframes</li> </ul> <p>This ensures your pipelines continue running even when individual models experience issues.</p>"},{"location":"optimization/fallback-models/#configuration","title":"Configuration","text":"<p>Fallback models are configured at the global level in your pipeline YAML file. You can configure separate fallback models for:</p> <ul> <li>Completion/Chat operations: Used by <code>map</code>, <code>reduce</code>, <code>resolve</code>, <code>filter</code>, and other LLM-powered operations</li> <li>Embedding operations: Used by operations that generate embeddings (e.g., <code>cluster</code>, <code>rank</code>)</li> </ul>"},{"location":"optimization/fallback-models/#basic-configuration","title":"Basic Configuration","text":"<p>The simplest way to configure fallback models is to provide a list of model names:</p> <pre><code># Default language model for all operations\ndefault_model: gpt-4o-mini\n\n# Fallback models for completion/chat operations\nfallback_models:\n  - gpt-3.5-turbo\n  - claude-3-haiku-20240307\n\n# Fallback models for embedding operations\nfallback_embedding_models:\n  - text-embedding-3-small\n  - text-embedding-ada-002\n</code></pre> <p>Models will be tried in the order specified. If the primary model fails, DocETL will automatically try the first fallback model, then the second, and so on.</p>"},{"location":"optimization/fallback-models/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more control, you can specify additional LiteLLM parameters for each fallback model:</p> <pre><code>default_model: gpt-4o-mini\n\n# Fallback models with custom parameters\nfallback_models:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      temperature: 0.0\n      max_tokens: 2000\n  - model_name: claude-3-haiku-20240307\n    litellm_params:\n      temperature: 0.0\n\n# Fallback embedding models\nfallback_embedding_models:\n  - model_name: text-embedding-3-small\n    litellm_params: {}\n  - model_name: text-embedding-ada-002\n    litellm_params: {}\n</code></pre>"},{"location":"optimization/fallback-models/#how-it-works","title":"How It Works","text":"<p>When an operation uses a model (either the <code>default_model</code> or an operation-specific model), DocETL will:</p> <ol> <li>Try the primary model first: The operation's specified model (or <code>default_model</code>) is attempted first</li> <li>Fallback on error: If an API error or content parsing error occurs, DocETL automatically tries the first fallback model</li> <li>Continue through fallbacks: If the first fallback also fails, it tries the next fallback model in sequence</li> <li>Fail only if all models fail: The operation only fails if all models (primary + all fallbacks) fail</li> </ol>"},{"location":"optimization/fallback-models/#example-complete-pipeline-with-fallback-models","title":"Example: Complete Pipeline with Fallback Models","text":"<p>Here's a complete example showing how to use fallback models in a pipeline:</p> <pre><code>datasets:\n  example_dataset:\n    type: file\n    path: example_data/example.json\n\n# Default language model for all operations unless overridden\ndefault_model: gpt-4o-mini\n\n# Fallback models for completion/chat operations\n# Models will be tried in order when API errors or content errors occur\nfallback_models:\n  # First fallback model\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      temperature: 0.0\n  # Second fallback model\n  - model_name: claude-3-haiku-20240307\n    litellm_params:\n      temperature: 0.0\n\n# Fallback models for embedding operations\n# Separate configuration for embedding model fallbacks\nfallback_embedding_models:\n  - model_name: text-embedding-3-small\n    litellm_params: {}\n  - model_name: text-embedding-ada-002\n    litellm_params: {}\n\noperations:\n  - name: example_map\n    type: map\n    prompt: \"Extract key information from: {{ input.contents }}\"\n    output:\n      schema:\n        extracted_info: \"str\"\n\npipeline:\n  steps:\n    - name: process_data\n      input: example_dataset\n      operations:\n        - example_map\n\n  output:\n    type: file\n    path: example_output.json\n</code></pre>"},{"location":"optimization/moar/","title":"MOAR Optimizer","text":"<p>The MOAR (Multi-Objective Agentic Rewrites) optimizer explores different ways to optimize your pipeline, finding solutions that balance accuracy and cost.</p>"},{"location":"optimization/moar/#what-is-moar","title":"What is MOAR?","text":"<p>When optimizing pipelines, you trade off cost and accuracy. MOAR explores many different pipeline configurations (like changing models, adding validation steps, combining operations, etc.) and evaluates each one to find the best trade-offs. It returns a frontier of plans that balance cost and accuracy, giving you multiple optimized options to choose from based on your budget and accuracy requirements.</p>"},{"location":"optimization/moar/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Getting Started - Step-by-step guide to run your first MOAR optimization</li> <li>Configuration - Complete reference for all configuration options</li> <li>Evaluation Functions - How to write and use evaluation functions</li> <li>Understanding Results - What MOAR outputs and how to interpret it</li> <li>Examples - Complete working examples</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"optimization/moar/#when-to-use-moar","title":"When to Use MOAR","text":"<p>Good for</p> <ul> <li>Finding cost-accuracy trade-offs across different models</li> <li>When you want multiple optimization options to choose from</li> <li>Custom evaluation metrics specific to your use case</li> <li>Exploring different pipeline configurations automatically</li> </ul>"},{"location":"optimization/moar/#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Create your pipeline YAML - Define your DocETL pipeline</li> <li>Write an evaluation function - Create a Python function to measure accuracy</li> <li>Configure MOAR - Set up <code>optimizer_config</code> in your YAML</li> <li>Run optimization - Execute <code>docetl build pipeline.yaml --optimizer moar</code></li> <li>Review results - Choose from the cost-accuracy frontier</li> </ol> <p>Ready to get started? Head to the Getting Started guide.</p>"},{"location":"optimization/overview/","title":"DocETL Optimizer","text":"<p>DocETL provides two optimizer options to improve your document processing pipelines:</p>"},{"location":"optimization/overview/#moar-optimizer-recommended","title":"MOAR Optimizer (Recommended)","text":"<p>The MOAR (Multi-Objective Agentic Rewrites) optimizer uses Monte Carlo Tree Search to explore optimization space and find Pareto-optimal solutions that balance accuracy and cost. It's the recommended optimizer for most use cases.</p> <p>Key Features:</p> <ul> <li>Multi-objective optimization (accuracy + cost)</li> <li>Returns multiple Pareto-optimal solutions</li> <li>Automatic model exploration</li> <li>Custom evaluation functions</li> <li>Intelligent search using MCTS</li> </ul> <p>See the MOAR Optimizer Guide for detailed documentation and examples.</p>"},{"location":"optimization/overview/#v1-optimizer-deprecated","title":"V1 Optimizer (Deprecated)","text":"<p>Deprecated</p> <p>The V1 optimizer is deprecated and no longer recommended. Use MOAR instead for all new optimizations.</p> <p>The V1 optimizer uses a greedy approach with validation to find improved pipeline configurations. It's still available for backward compatibility but should not be used for new projects.</p> <p>The rest of this page describes the general optimization concepts that apply to both optimizers.</p>"},{"location":"optimization/overview/#key-features","title":"Key Features","text":"<ul> <li>Automatically decomposes complex operations into more efficient sub-pipelines</li> <li>Inserts resolve operations before reduce operations when beneficial</li> <li>Optimizes for large documents that exceed context limits</li> <li>Improves accuracy in high-volume reduce operations with incremental reduce</li> </ul>"},{"location":"optimization/overview/#how-it-works","title":"How It Works","text":"<p>The optimizer employs AI agents to generate and validate potential optimizations:</p> <ol> <li>Generation Agents: Create alternative plans for operations, potentially breaking them down into multiple steps.</li> <li>Validation Agents: Evaluate and compare the outputs of different plans to determine the most effective approach.</li> </ol> <pre><code>graph TB\n    A[User-Defined Operation] --&gt; B[Validation Agent]\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    B --&gt;|Synthesize| C[Validator Prompt]\n    C --&gt; D[Evaluate on Sample Data]\n    D --&gt; E{Needs Optimization?}\n    E --&gt;|Yes| F[Generation Agent]\n    E --&gt;|No| J[Optimized Operation]\n    style F fill:#bbf,stroke:#333,stroke-width:2px\n    F --&gt;|Create| G[Candidate Plans]\n    G --&gt; H[Validation Agent]\n    style H fill:#f9f,stroke:#333,stroke-width:2px\n    H --&gt;|Rank/Compare| I[Select Best Plan]\n    I --&gt; J</code></pre>"},{"location":"optimization/overview/#should-i-use-the-optimizer","title":"Should I Use the Optimizer?","text":"<p>While any pipeline can potentially benefit from optimization, there are specific scenarios where using the optimizer can significantly improve your pipeline's performance and accuracy. When should you use the optimizer?</p> <p>Large Documents</p> <p>If you have documents that approach or exceed context limits and a map operation that transforms these documents using an LLM, the optimizer can help:</p> <ul> <li>Improve accuracy</li> <li>Enable processing of entire documents</li> <li>Optimize for large-scale data handling</li> </ul> <p>Entity Resolution</p> <p>The optimizer is particularly useful when:</p> <pre><code>- You need a resolve operation before your reduce operation\n- You've defined a resolve operation but want to optimize it for speed using blocking\n</code></pre> <p>High-Volume Reduce Operations</p> <p>Consider using the optimizer when:</p> <pre><code>- You have many documents feeding into a reduce operation for a given key\n- You're concerned about the accuracy of the reduce operation due to high volume\n- You want to optimize for better accuracy in complex reductions\n</code></pre> <p>Even if your pipeline doesn't fall into these specific categories, optimization can still be beneficial. For example, the optimizer can enhance your operations by adding gleaning to an operation, which uses an LLM-powered validator to ensure operation correctness. Learn more about gleaning.</p>"},{"location":"optimization/overview/#example-optimizing-legal-contract-analysis","title":"Example: Optimizing Legal Contract Analysis","text":"<p>Let's consider a pipeline for analyzing legal contracts, extracting clauses, and summarizing them by type. Initially, you might have a single map operation to extract and tag clauses, followed by a reduce operation to summarize them. However, this approach might not be accurate enough for long contracts.</p>"},{"location":"optimization/overview/#initial-pipeline","title":"Initial Pipeline","text":"<p>In the initial pipeline, you might have a single map operation that attempts to extract all clauses and tag them with their types in one go. This is followed by a reduce operation that summarizes the clauses by type. Maybe the reduce operation accurately summarizes the clauses in a single LLM call per clause type, but the map operation might not be able to accurately extract and tag the clauses in a single LLM call.</p>"},{"location":"optimization/overview/#optimized-pipeline","title":"Optimized Pipeline","text":"<p>After applying the optimizer, your pipeline could be transformed into a more efficient and accurate sub-pipeline:</p> <ol> <li>Split Operation: Breaks down each long contract into manageable chunks.</li> <li>Map Operation: Processes each chunk to extract and tag clauses.</li> <li>Reduce Operation: For each contract, combine the extracted and tagged clauses from each chunk.</li> </ol> <p>The goal of the DocETL optimizer is to try many ways of rewriting your pipeline and then select the best one. This may take some time (20-30 minutes for very complex tasks and large documents). But the optimizer's ability to break down complex tasks into more manageable sub-steps can lead to more accurate and reliable results.</p>"},{"location":"optimization/overview/#choosing-an-optimizer","title":"Choosing an Optimizer","text":"<p>Use MOAR if:</p> <ul> <li>You want to explore cost-accuracy trade-offs</li> <li>You need multiple solution options (Pareto frontier)</li> <li>You have custom evaluation metrics</li> <li>You want automatic model exploration</li> </ul> <p>V1 Optimizer Deprecated</p> <p>The V1 optimizer is deprecated. Use MOAR instead. If you have existing V1-optimized pipelines, they will continue to work, but new optimizations should use MOAR.</p> <p>For detailed MOAR usage, see the MOAR Optimizer Guide.</p>"},{"location":"optimization/python-api/","title":"Optimizing Pipelines with the Python API","text":"<p>You may have your pipelines defined in Python instead of YAML and want to optimize them. Here's an example of how to use the Python API to define, optimize, and run a document processing pipeline similar to the medical transcripts example we saw earlier.</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, UnnestOp, ResolveOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define datasets\ndatasets = {\n    \"transcripts\": Dataset(type=\"file\", path=\"medical_transcripts.json\"),\n}\n\n# Define operations\noperations = [\n    MapOp(\n        name=\"extract_medications\",\n        type=\"map\",\n        optimize=True,  # This operation will be optimized\n        output={\"schema\": {\"medication\": \"list[str]\"}},\n        prompt=\"Analyze the transcript: {{ input.src }}\\nList all medications mentioned.\",\n    ),\n    UnnestOp(\n        name=\"unnest_medications\",\n        type=\"unnest\",\n        unnest_key=\"medication\"\n    ),\n    ResolveOp(\n        name=\"resolve_medications\",\n        type=\"resolve\",\n        blocking_keys=[\"medication\"],\n        optimize=True,  # This operation will be optimized\n        output={\"schema\": {\"medication\": \"str\"}},\n        comparison_prompt=\"Compare medications:\\n1: {{ input1.medication }}\\n2: {{ input2.medication }}\\nAre these the same or closely related?\",\n        resolution_prompt=\"Standardize the name for:\\n{% for entry in inputs %}\\n- {{ entry.medication }}\\n{% endfor %}\"\n    ),\n    ReduceOp(\n        name=\"summarize_prescriptions\",\n        type=\"reduce\",\n        reduce_key=[\"medication\"],\n        output={\"schema\": {\"side_effects\": \"str\", \"uses\": \"str\"}},\n        prompt=\"Summarize side effects and uses of {{ reduce_key }} from:\\n{% for value in inputs %}\\nTranscript {{ loop.index }}: {{ value.src }}\\n{% endfor %}\",\n        optimize=True,  # This operation will be optimized\n    )\n]\n\n# Define pipeline steps\nsteps = [\n    PipelineStep(name=\"medical_info_extraction\", input=\"transcripts\", operations=[\"extract_medications\", \"unnest_medications\", \"resolve_medications\", \"summarize_prescriptions\"])\n]\n\n# Define pipeline output\noutput = PipelineOutput(type=\"file\", path=\"medication_summaries.json\")\n\n# Create the pipeline\npipeline = Pipeline(\n    name=\"medical_transcripts_pipeline\",\n    datasets=datasets,\n    operations=operations,\n    steps=steps,\n    output=output,\n    default_model=\"gpt-4o-mini\",\n    system_prompt={\n        \"dataset_description\": \"a collection of medical conversation transcripts\",\n        \"persona\": \"a healthcare analyst extracting and summarizing medication information\",\n    }\n)\n\n# Optimize the pipeline\noptimized_pipeline = pipeline.optimize(model=\"gpt-4o-mini\")\n\n# Run the optimized pipeline\nresult = optimized_pipeline.run()\n\nprint(f\"Pipeline execution completed. Total cost: ${result:.2f}\")\n</code></pre> <p>This example demonstrates how to create a pipeline that processes medical transcripts, extracts medication information, resolves similar medications, and summarizes prescription details.</p> <p>Optimization</p> <p>Notice that some operations have <code>optimize=True</code> set. DocETL will only optimize operations with this flag set to <code>True</code>. In this example, the <code>extract_medications</code>, <code>resolve_medications</code>, and <code>summarize_prescriptions</code> operations will be optimized.</p> <p>Optimization Model</p> <p>We use <code>pipeline.optimize(model=\"gpt-4o-mini\")</code> to optimize the pipeline using the GPT-4o-mini model for the agents. This allows you to specify which model to use for optimization, which can be particularly useful when you want to balance between performance and cost.</p> <p>The pipeline is optimized before execution to improve performance and accuracy. By setting <code>optimize=True</code> for specific operations, you have fine-grained control over which parts of your pipeline undergo optimization.</p>"},{"location":"optimization/moar/configuration/","title":"MOAR Configuration Reference","text":"<p>Complete reference for all MOAR configuration options.</p>"},{"location":"optimization/moar/configuration/#required-fields","title":"Required Fields","text":"<p>All fields in <code>optimizer_config</code> are required (no defaults):</p> Field Type Description <code>type</code> <code>str</code> Must be <code>\"moar\"</code> <code>save_dir</code> <code>str</code> Directory where MOAR results will be saved <code>available_models</code> <code>list[str]</code> List of LiteLLM model names to explore (e.g., <code>[\"gpt-4o-mini\", \"gpt-4o\"]</code>). Make sure your API keys are set in your environment for these models. <code>evaluation_file</code> <code>str</code> Path to Python file containing <code>@register_eval</code> decorated function <code>metric_key</code> <code>str</code> Key in evaluation results dictionary to use as accuracy metric <code>max_iterations</code> <code>int</code> Maximum number of MOARSearch iterations to run <code>rewrite_agent_model</code> <code>str</code> LLM model to use for directive instantiation during search <p>All Fields Required</p> <p>MOAR will error if any required field is missing. There are no defaults.</p>"},{"location":"optimization/moar/configuration/#optional-fields","title":"Optional Fields","text":"Field Type Default Description <code>dataset_path</code> <code>str</code> Inferred from <code>datasets</code> Path to dataset file to use for optimization. Use a sample/hold-out dataset to avoid optimizing on your test set. <code>exploration_weight</code> <code>float</code> <code>1.414</code> UCB exploration constant (higher = more exploration) <code>build_first_layer</code> <code>bool</code> <code>False</code> Whether to build initial model-specific nodes <code>ground_truth_path</code> <code>str</code> <code>None</code> Path to ground truth file (for evaluation)"},{"location":"optimization/moar/configuration/#dataset-path","title":"Dataset Path","text":""},{"location":"optimization/moar/configuration/#automatic-inference","title":"Automatic Inference","text":"<p>If <code>dataset_path</code> is not specified, MOAR will automatically infer it from the <code>datasets</code> section of your YAML:</p> <pre><code>datasets:\n  transcripts:\n    path: data/full_dataset.json  # This will be used if dataset_path not specified\n    type: file\n\noptimizer_config:\n  # dataset_path not specified - will use data/full_dataset.json\n  # ... other config ...\n</code></pre>"},{"location":"optimization/moar/configuration/#using-samplehold-out-datasets","title":"Using Sample/Hold-Out Datasets","text":"<p>Best Practice</p> <p>Use a sample or hold-out dataset for optimization to avoid optimizing on your test set.</p> <pre><code>optimizer_config:\n  dataset_path: data/sample_dataset.json  # Use sample/hold-out for optimization\n  # ... other config ...\n\ndatasets:\n  transcripts:\n    path: data/full_dataset.json  # Full dataset for final pipeline\n</code></pre> <p>The optimizer will use the sample dataset, but your final pipeline uses the full dataset. This ensures you don't overfit to your test set during optimization.</p>"},{"location":"optimization/moar/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"optimization/moar/configuration/#available-models","title":"Available Models","text":"<p>LiteLLM Model Names</p> <p>Use LiteLLM model names (e.g., <code>gpt-4o-mini</code>, <code>gpt-4o</code>, <code>gpt-5.1</code>). Make sure your API keys are set in your environment.</p> <pre><code>available_models:  # LiteLLM model names - ensure API keys are set\n  - gpt-5.1-nano      # Cheapest, lower accuracy\n  - gpt-5.1-mini      # Low cost, decent accuracy\n  - gpt-5.1           # Balanced\n  - gpt-4o             # Higher cost, better accuracy\n</code></pre>"},{"location":"optimization/moar/configuration/#model-for-directive-instantiation","title":"Model for Directive Instantiation","text":"<p>The <code>rewrite_agent_model</code> field specifies which LLM to use for generating optimization directives during the search process. This doesn't affect the models tested in <code>available_models</code>.</p> <p>Cost Consideration</p> <p>Use a cheaper model (like <code>gpt-4o-mini</code>) for directive instantiation to reduce search costs.</p>"},{"location":"optimization/moar/configuration/#iteration-count","title":"Iteration Count","text":"<p>The <code>max_iterations</code> parameter controls how many pipeline configurations MOAR explores:</p> <ul> <li>10-20 iterations: Quick exploration, good for testing</li> <li>40 iterations: Recommended for most use cases</li> <li>100+ iterations: For complex pipelines or when you need the absolute best results</li> </ul> <p>Time vs Quality</p> <p>More iterations give better results but take longer and cost more.</p>"},{"location":"optimization/moar/configuration/#complete-example","title":"Complete Example","text":"<pre><code>optimizer_config:\n  type: moar\n  save_dir: results/moar_optimization\n  available_models:\n    - gpt-4o-mini\n    - gpt-4o\n    - gpt-5.1-mini\n    - gpt-5.1\n  evaluation_file: evaluate_medications.py\n  metric_key: medication_extraction_score\n  max_iterations: 40\n  rewrite_agent_model: gpt-5.1\n  dataset_path: data/sample.json  # Optional\n  exploration_weight: 1.414  # Optional\n</code></pre>"},{"location":"optimization/moar/evaluation/","title":"Evaluation Functions","text":"<p>How to write evaluation functions for MOAR optimization.</p>"},{"location":"optimization/moar/evaluation/#how-evaluation-functions-work","title":"How Evaluation Functions Work","text":"<p>Your evaluation function receives the pipeline output and computes metrics by comparing it to the original dataset. MOAR uses one specific metric from your returned dictionary (specified by <code>metric_key</code>) to optimize for accuracy.</p> <p>Function Signature</p> <p>Your function must have exactly this signature: <pre><code>def evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n</code></pre></p>"},{"location":"optimization/moar/evaluation/#what-you-receive","title":"What You Receive","text":"<ul> <li><code>results_file_path</code>: Path to JSON file containing your pipeline's output</li> <li><code>dataset_file_path</code>: Path to JSON file containing the original dataset</li> </ul>"},{"location":"optimization/moar/evaluation/#what-you-return","title":"What You Return","text":"<p>A dictionary with numeric metrics. The key specified in <code>optimizer_config.metric_key</code> will be used as the accuracy metric for optimization.</p> <p>Using Original Input Data</p> <p>Pipeline output includes the original input data. For example, if your dataset has a <code>src</code> attribute, it will be available in the output. You can use this directly for comparison without loading the dataset file separately.</p>"},{"location":"optimization/moar/evaluation/#basic-example","title":"Basic Example","text":"<pre><code>import json\nfrom typing import Any, Dict\nfrom docetl.utils_evaluation import register_eval\n\n@register_eval\ndef evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n    # Load pipeline output\n    with open(results_file_path, 'r') as f:\n        output = json.load(f)\n\n    total_correct = 0\n    for result in output:\n        # For example, if your dataset has a 'src' attribute, it's available in the output\n        original_text = result.get(\"src\", \"\").lower()\n        # Replace \"your_extraction_key\" with the actual key from your pipeline output\n        extracted_items = result.get(\"your_extraction_key\", [])\n\n        # Check if extracted items appear in original text\n        for item in extracted_items:\n            if str(item).lower() in original_text:\n                total_correct += 1\n\n    return {\n        \"extraction_score\": total_correct,  # This key is used if metric_key=\"extraction_score\"\n        \"total_extracted\": sum(len(r.get(\"your_extraction_key\", [])) for r in output),\n    }\n</code></pre>"},{"location":"optimization/moar/evaluation/#requirements","title":"Requirements","text":"<p>Critical Requirements</p> <ul> <li>The function must be decorated with <code>@docetl.register_eval</code></li> <li>It must take exactly two arguments: <code>dataset_file_path</code> and <code>results_file_path</code></li> <li>It must return a dictionary with numeric metrics</li> <li>The <code>metric_key</code> in your <code>optimizer_config</code> must match one of the keys in this dictionary</li> <li>Only one function per file can be decorated with <code>@register_eval</code></li> </ul>"},{"location":"optimization/moar/evaluation/#performance-considerations","title":"Performance Considerations","text":"<p>Keep It Fast</p> <p>Your evaluation function will be called many times during optimization. Make sure it's efficient:</p> <ul> <li>Avoid expensive computations</li> <li>Cache results if possible</li> <li>Keep the function simple and fast</li> </ul>"},{"location":"optimization/moar/evaluation/#common-evaluation-patterns","title":"Common Evaluation Patterns","text":""},{"location":"optimization/moar/evaluation/#pattern-1-extraction-verification-with-recall","title":"Pattern 1: Extraction Verification with Recall","text":"<p>Check if extracted items appear in the document text and compute recall:</p> <pre><code>@register_eval\ndef evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n    with open(results_file_path, 'r') as f:\n        output = json.load(f)\n\n    # For example, if your dataset has a 'src' attribute, it's available in the output\n    total_correct = 0\n    total_extracted = 0\n    total_expected = 0\n\n    for result in output:\n        # Replace \"src\" with the actual key from your dataset\n        original_text = result.get(\"src\", \"\").lower()\n        extracted_items = result.get(\"your_extraction_key\", [])  # Replace with your key\n\n        # Count correct extractions (items that appear in text)\n        for item in extracted_items:\n            total_extracted += 1\n            if str(item).lower() in original_text:\n                total_correct += 1\n\n        # Count expected items (if you have ground truth)\n        # total_expected += len(expected_items)\n\n    precision = total_correct / total_extracted if total_extracted &gt; 0 else 0.0\n    recall = total_correct / total_expected if total_expected &gt; 0 else 0.0\n\n    return {\n        \"extraction_score\": total_correct,  # Use this as metric_key\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n</code></pre>"},{"location":"optimization/moar/evaluation/#pattern-2-comparing-against-ground-truth","title":"Pattern 2: Comparing Against Ground Truth","text":"<p>Load ground truth from the dataset file and compare:</p> <pre><code>@register_eval\ndef evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n    with open(results_file_path, 'r') as f:\n        predictions = json.load(f)\n\n    with open(dataset_file_path, 'r') as f:\n        ground_truth = json.load(f)\n\n    # Compare predictions with ground truth\n    # Adjust keys based on your data structure\n    correct = 0\n    total = len(predictions)\n\n    for pred, truth in zip(predictions, ground_truth):\n        # Example: compare classification labels\n        if pred.get(\"predicted_label\") == truth.get(\"true_label\"):\n            correct += 1\n\n    return {\n        \"accuracy\": correct / total if total &gt; 0 else 0.0,\n        \"correct\": correct,\n        \"total\": total,\n    }\n</code></pre>"},{"location":"optimization/moar/evaluation/#pattern-3-external-evaluation-file-or-api","title":"Pattern 3: External Evaluation (File or API)","text":"<p>Load additional data or call an API for evaluation:</p> <pre><code>import requests\nfrom pathlib import Path\n\n@register_eval\ndef evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n    with open(results_file_path, 'r') as f:\n        output = json.load(f)\n\n    # Option A: Load ground truth from a separate file\n    ground_truth_path = Path(dataset_file_path).parent / \"ground_truth.json\"\n    with open(ground_truth_path, 'r') as f:\n        ground_truth = json.load(f)\n\n    # Option B: Call an API for evaluation\n    # response = requests.post(\"https://api.example.com/evaluate\", json=output)\n    # api_score = response.json()[\"score\"]\n\n    # Evaluate using ground truth\n    scores = []\n    for result, truth in zip(output, ground_truth):\n        # Your evaluation logic here\n        score = compute_score(result, truth)\n        scores.append(score)\n\n    return {\n        \"average_score\": sum(scores) / len(scores) if scores else 0.0,\n        \"scores\": scores,\n    }\n</code></pre>"},{"location":"optimization/moar/evaluation/#testing-your-function","title":"Testing Your Function","text":"<p>Test Before Running</p> <p>Test your evaluation function independently before running MOAR:</p> <pre><code>result = evaluate_results(\"dataset.json\", \"results.json\")\nprint(result)  # Check that your metric_key is present\n</code></pre> <p>This helps catch errors early and ensures your function works correctly.</p>"},{"location":"optimization/moar/examples/","title":"MOAR Examples","text":"<p>Complete working examples for MOAR optimization.</p>"},{"location":"optimization/moar/examples/#medication-extraction-example","title":"Medication Extraction Example","text":"<p>This example extracts medications from medical transcripts and evaluates extraction accuracy.</p> <p>Metric Key</p> <p>The <code>metric_key</code> in the <code>optimizer_config</code> section specifies which key from your evaluation function's return dictionary will be used as the accuracy metric. In this example, <code>metric_key: medication_extraction_score</code> means MOAR will optimize using the <code>medication_extraction_score</code> value returned by the evaluation function.</p>"},{"location":"optimization/moar/examples/#pipelineyaml","title":"pipeline.yaml","text":"<pre><code>datasets:\n  transcripts:\n    path: workloads/medical/raw.json\n    type: file\n\ndefault_model: gpt-4o-mini\nbypass_cache: true\n\noptimizer_config:\n  type: moar\n  dataset_path: workloads/medical/raw_sample.json  # Use sample for faster optimization\n  save_dir: workloads/medical/moar_results\n  available_models:  # LiteLLM model names - ensure API keys are set in your environment\n    - gpt-5.1-nano\n    - gpt-5.1-mini\n    - gpt-5.1\n    - gpt-4o\n    - gpt-4o-mini\n  evaluation_file: workloads/medical/evaluate_medications.py\n  metric_key: medication_extraction_score\n  max_iterations: 40\n  rewrite_agent_model: gpt-5.1\n\nsystem_prompt:\n  dataset_description: a collection of transcripts of doctor visits\n  persona: a medical practitioner analyzing patient symptoms and reactions to medications\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the following transcript of a conversation between a doctor and a patient:\n      {{ input.src }}\n      Extract and list all medications mentioned in the transcript.\n      If no medications are mentioned, return an empty list.\n\npipeline:\n  steps:\n    - name: medication_extraction\n      input: transcripts\n      operations:\n        - extract_medications\n  output:\n    type: file\n    path: workloads/medical/extracted_medications_results.json\n</code></pre>"},{"location":"optimization/moar/examples/#evaluate_medicationspy","title":"evaluate_medications.py","text":"<pre><code>import json\nfrom typing import Any, Dict\nfrom docetl.utils_evaluation import register_eval\n\n@register_eval\ndef evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate medication extraction results.\n\n    Checks if each extracted medication appears verbatim in the original transcript.\n    In this example, the dataset has a 'src' attribute with the original input text.\n    \"\"\"\n    # Load pipeline output\n    with open(results_file_path, 'r') as f:\n        output = json.load(f)\n\n    total_correct_medications = 0\n    total_extracted_medications = 0\n\n    # Evaluate each result\n    for result in output:\n        # In this example, the dataset has a 'src' attribute with the original transcript\n        original_transcript = result.get(\"src\", \"\").lower()\n        extracted_medications = result.get(\"medication\", [])\n\n        # Check each extracted medication\n        for medication in extracted_medications:\n            total_extracted_medications += 1\n            medication_lower = str(medication).lower().strip()\n\n            # Check if medication appears in transcript\n            if medication_lower in original_transcript:\n                total_correct_medications += 1\n\n    # Calculate metrics\n    precision = total_correct_medications / total_extracted_medications if total_extracted_medications &gt; 0 else 0.0\n\n    return {\n        \"medication_extraction_score\": total_correct_medications,  # This is used as the accuracy metric\n        \"total_correct_medications\": total_correct_medications,\n        \"total_extracted_medications\": total_extracted_medications,\n        \"precision\": precision,\n    }\n</code></pre>"},{"location":"optimization/moar/examples/#running-the-optimization","title":"Running the Optimization","text":"<pre><code>docetl build workloads/medical/pipeline_medication_extraction.yaml --optimizer moar\n</code></pre> <p>Using Sample Datasets</p> <p>Notice that <code>dataset_path</code> points to <code>raw_sample.json</code> for optimization, while the main pipeline uses <code>raw.json</code>. This prevents optimizing on your test set.</p>"},{"location":"optimization/moar/examples/#key-points","title":"Key Points","text":"<p>Evaluation Function</p> <ul> <li>In this example, uses the <code>src</code> attribute from output items (no need to load dataset separately)</li> <li>Checks if extracted medications appear verbatim in the transcript</li> <li>Returns multiple metrics, with <code>medication_extraction_score</code> as the primary one</li> </ul> <p>Configuration</p> <ul> <li>Uses a sample dataset for optimization (<code>dataset_path</code>)</li> <li>Includes multiple models in <code>available_models</code> to explore trade-offs</li> <li>Sets <code>max_iterations</code> to 40 for a good balance of exploration and time</li> </ul>"},{"location":"optimization/moar/getting-started/","title":"Getting Started with MOAR","text":"<p>This guide walks you through running your first MOAR optimization step by step.</p>"},{"location":"optimization/moar/getting-started/#step-1-create-your-pipeline-yaml","title":"Step 1: Create Your Pipeline YAML","text":"<p>Start with a standard DocETL pipeline YAML file:</p> <pre><code>datasets:\n  transcripts:\n    path: data/transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract all medications mentioned in: {{ input.src }}\n\npipeline:\n  steps:\n    - name: medication_extraction\n      input: transcripts\n      operations:\n        - extract_medications\n  output:\n    type: file\n    path: results.json\n</code></pre> <p>Standard Pipeline</p> <p>Your pipeline doesn't need any special configuration for MOAR. Just create a normal DocETL pipeline.</p>"},{"location":"optimization/moar/getting-started/#step-2-create-an-evaluation-function","title":"Step 2: Create an Evaluation Function","text":"<p>Create a Python file with an evaluation function. This function will be called for each pipeline configuration that MOAR explores.</p> <p>How Evaluation Works</p> <ul> <li>Your function receives the pipeline output and the original dataset</li> <li>You compute evaluation metrics by comparing the output to the dataset</li> <li>You return a dictionary of metrics</li> <li>MOAR uses one specific key from this dictionary (specified by <code>metric_key</code>) as the accuracy metric to optimize</li> </ul> <pre><code># evaluate_medications.py\nimport json\nfrom typing import Any, Dict\nfrom docetl.utils_evaluation import register_eval\n\n@register_eval\ndef evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate pipeline output against the original dataset.\n    \"\"\"\n    # Load pipeline output\n    with open(results_file_path, 'r') as f:\n        output = json.load(f)\n\n    # Load original dataset for comparison\n    with open(dataset_file_path, 'r') as f:\n        dataset = json.load(f)\n\n    # Compute your evaluation metrics\n    correct_count = 0\n    total_count = len(output)\n\n    for idx, result in enumerate(output):\n        # Compare result with original data\n        # For example, if your dataset has a 'src' attribute, it's available in the output\n        original_text = result.get(\"src\", \"\").lower()\n        extracted_items = result.get(\"medication\", [])\n\n        # Check if extracted items appear in original text\n        for item in extracted_items:\n            if item.lower() in original_text:\n                correct_count += 1\n\n    # Return dictionary of metrics\n    return {\n        \"medication_extraction_score\": correct_count,  # This key will be used if metric_key matches\n        \"total_extracted\": total_count,\n        \"precision\": correct_count / total_count if total_count &gt; 0 else 0.0,\n    }\n</code></pre> <p>Important Requirements</p> <ul> <li>The function must be decorated with <code>@docetl.register_eval</code></li> <li>It must take exactly two arguments: <code>dataset_file_path</code> and <code>results_file_path</code></li> <li>It must return a dictionary with numeric metrics</li> <li>The <code>metric_key</code> in your <code>optimizer_config</code> must match one of the keys in this dictionary</li> <li>Only one function per file can be decorated with <code>@register_eval</code></li> </ul> <p>For more details on evaluation functions, see the Evaluation Functions guide.</p>"},{"location":"optimization/moar/getting-started/#step-3-configure-the-optimizer","title":"Step 3: Configure the Optimizer","text":"<p>Add an <code>optimizer_config</code> section to your YAML. The <code>metric_key</code> specifies which key from your evaluation function's return dictionary will be used as the accuracy metric for optimization:</p> <pre><code>optimizer_config:\n  type: moar\n  save_dir: results/moar_optimization\n  available_models:  # LiteLLM model names - ensure API keys are set in your environment\n    - gpt-4o-mini\n    - gpt-4o\n    - gpt-5.1-mini\n    - gpt-5.1\n  evaluation_file: evaluate_medications.py\n  metric_key: medication_extraction_score  # This must match a key in your evaluation function's return dictionary\n  max_iterations: 40\n  rewrite_agent_model: gpt-5.1\n  dataset_path: data/transcripts_sample.json  # Optional: use sample/hold-out dataset\n</code></pre> <p>Using Sample Datasets</p> <p>Use <code>dataset_path</code> to specify a sample or hold-out dataset for optimization. This prevents optimizing on your test set. The main pipeline will still use the full dataset from the <code>datasets</code> section.</p> <p>For complete configuration details, see the Configuration Reference.</p>"},{"location":"optimization/moar/getting-started/#step-4-run-the-optimizer","title":"Step 4: Run the Optimizer","text":"<p>Run MOAR optimization using the CLI:</p> <pre><code>docetl build pipeline.yaml --optimizer moar\n</code></pre> <p>What Happens Next</p> <p>MOAR will: 1. Explore different pipeline configurations 2. Evaluate each configuration using your evaluation function 3. Build a cost-accuracy frontier of optimal solutions 4. Save results to your <code>save_dir</code></p>"},{"location":"optimization/moar/getting-started/#step-5-review-results","title":"Step 5: Review Results","text":"<p>After optimization completes, check your <code>save_dir</code> for:</p> <ul> <li><code>experiment_summary.json</code> - High-level summary of the run</li> <li><code>pareto_frontier.json</code> - List of optimal solutions</li> <li><code>evaluation_metrics.json</code> - Detailed evaluation results</li> <li><code>pipeline_*.yaml</code> - Optimized pipeline configurations</li> </ul> <p>For details on interpreting results, see Understanding Results.</p>"},{"location":"optimization/moar/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about configuration options</li> <li>See complete examples</li> <li>Read troubleshooting tips</li> </ul>"},{"location":"optimization/moar/results/","title":"Understanding MOAR Results","text":"<p>What MOAR outputs and how to interpret the results.</p>"},{"location":"optimization/moar/results/#output-files","title":"Output Files","text":"<p>After running MOAR optimization, you'll find several files in your <code>save_dir</code>:</p> <ul> <li><code>experiment_summary.json</code> - High-level summary</li> <li><code>pareto_frontier.json</code> - Optimal solutions</li> <li><code>evaluation_metrics.json</code> - Detailed evaluation results</li> <li><code>pipeline_*.yaml</code> - Optimized pipeline configurations</li> </ul>"},{"location":"optimization/moar/results/#experiment_summaryjson","title":"experiment_summary.json","text":"<p>High-level summary of the optimization run:</p> <pre><code>{\n  \"optimizer\": \"moar\",\n  \"input_pipeline\": \"pipeline.yaml\",\n  \"rewrite_agent_model\": \"gpt-5.1\",\n  \"max_iterations\": 40,\n  \"save_dir\": \"results/moar_optimization\",\n  \"dataset\": \"transcripts\",\n  \"start_time\": \"2024-01-15T10:30:00\",\n  \"end_time\": \"2024-01-15T11:15:00\",\n  \"duration_seconds\": 2700,\n  \"num_best_nodes\": 5,\n  \"total_nodes_explored\": 120,\n  \"total_search_cost\": 15.50\n}\n</code></pre> <p>Key Metrics</p> <ul> <li><code>num_best_nodes</code>: Number of solutions on the Pareto frontier</li> <li><code>total_nodes_explored</code>: Total configurations tested</li> <li><code>total_search_cost</code>: Total cost of the optimization search</li> </ul>"},{"location":"optimization/moar/results/#pareto_frontierjson","title":"pareto_frontier.json","text":"<p>List of Pareto-optimal solutions (the cost-accuracy frontier):</p> <pre><code>[\n  {\n    \"node_id\": 5,\n    \"yaml_path\": \"results/moar_optimization/pipeline_5.yaml\",\n    \"cost\": 0.05,\n    \"accuracy\": 0.92\n  },\n  {\n    \"node_id\": 12,\n    \"yaml_path\": \"results/moar_optimization/pipeline_12.yaml\",\n    \"cost\": 0.08,\n    \"accuracy\": 0.95\n  }\n]\n</code></pre> <p>Choosing a Solution</p> <p>Review the Pareto frontier to find solutions that match your priorities:</p> <ul> <li>Low cost priority: Choose solutions with lower cost</li> <li>High accuracy priority: Choose solutions with higher accuracy</li> <li>Balanced: Choose solutions in the middle</li> </ul> <p>Each solution includes a <code>yaml_path</code> pointing to the optimized pipeline configuration.</p>"},{"location":"optimization/moar/results/#evaluation_metricsjson","title":"evaluation_metrics.json","text":"<p>Detailed evaluation results for all explored configurations. This file contains comprehensive metrics for every pipeline configuration tested during optimization.</p>"},{"location":"optimization/moar/results/#pipeline-configurations","title":"Pipeline Configurations","text":"<p>Each solution on the Pareto frontier has a corresponding YAML file (e.g., <code>pipeline_5.yaml</code>) containing the optimized pipeline configuration. You can:</p> <ol> <li>Review the changes MOAR made</li> <li>Test the pipeline on your full dataset</li> <li>Use it in production</li> </ol>"},{"location":"optimization/moar/results/#next-steps","title":"Next Steps","text":"<p>After reviewing the results:</p> <ol> <li>Review the Pareto frontier - See available options</li> <li>Choose a solution - Based on your accuracy/cost priorities</li> <li>Test the chosen pipeline - Run it on your full dataset</li> <li>Integrate into production - Use the optimized configuration</li> </ol> <p>Success</p> <p>You now have multiple optimized pipeline options to choose from, each representing a different point on the cost-accuracy trade-off curve.</p>"},{"location":"optimization/moar/troubleshooting/","title":"Troubleshooting MOAR","text":"<p>Common issues and solutions when using MOAR optimization.</p>"},{"location":"optimization/moar/troubleshooting/#error-missing-required-accuracy-metric","title":"Error: Missing required accuracy metric","text":"<p>Error Message</p> <p><code>KeyError: Missing required accuracy metric 'your_metric_key'</code></p> <p>Solution:</p> <p>Check that:</p> <ol> <li>Your evaluation function returns a dictionary with the <code>metric_key</code> you specified</li> <li>The <code>metric_key</code> in <code>optimizer_config</code> matches the key in your evaluation results</li> <li>Your evaluation function is working correctly (test it independently)</li> </ol> <pre><code># Test your function\nresult = evaluate_results(\"dataset.json\", \"results.json\")\nprint(result)  # Verify your metric_key is present\n</code></pre>"},{"location":"optimization/moar/troubleshooting/#error-evaluation-function-takes-wrong-number-of-arguments","title":"Error: Evaluation function takes wrong number of arguments","text":"<p>Error Message</p> <p><code>TypeError: evaluate_results() takes 1 positional argument but 2 were given</code></p> <p>Solution:</p> <p>Make sure your evaluation function has exactly this signature:</p> <pre><code>def evaluate_results(dataset_file_path: str, results_file_path: str) -&gt; Dict[str, Any]:\n</code></pre> <p>And that it's decorated with <code>@docetl.register_eval</code>.</p>"},{"location":"optimization/moar/troubleshooting/#all-accuracies-showing-as-00","title":"All accuracies showing as 0.0","text":"<p>Symptom</p> <p>All solutions show 0.0 accuracy in the Pareto frontier.</p> <p>Possible causes:</p> <ol> <li>Evaluation function failing silently - Check the error logs</li> <li>Result files don't exist - Make sure pipelines are executing successfully</li> <li>Metric key doesn't match - Verify <code>metric_key</code> matches what your function returns</li> </ol> <p>Solution:</p> <p>Test your evaluation function independently and check MOAR logs for errors.</p>"},{"location":"optimization/moar/troubleshooting/#optimization-taking-too-long","title":"Optimization taking too long","text":"<p>Speed Up Optimization</p> <p>If optimization is taking too long, try:</p> <ul> <li>Reduce <code>max_iterations</code> (e.g., from 40 to 20)</li> <li>Use a smaller sample dataset via <code>dataset_path</code></li> <li>Reduce the number of models in <code>available_models</code></li> <li>Use a faster model for directive instantiation (<code>model</code> parameter)</li> </ul>"},{"location":"optimization/moar/troubleshooting/#best-practices","title":"Best Practices","text":""},{"location":"optimization/moar/troubleshooting/#using-samplehold-out-datasets","title":"Using Sample/Hold-Out Datasets","text":"<p>Avoid Overfitting</p> <p>Always use a sample or hold-out dataset for optimization to avoid optimizing on your test set:</p> <pre><code>optimizer_config:\n  dataset_path: data/sample_100.json  # Use sample/hold-out for optimization\n</code></pre>"},{"location":"optimization/moar/troubleshooting/#choosing-models","title":"Choosing Models","text":"<p>Model Selection</p> <p>Include a range of models in <code>available_models</code> to explore cost-accuracy trade-offs:</p> <pre><code>available_models:\n  - gpt-5.1-nano      # Cheapest, lower accuracy\n  - gpt-5.1-mini      # Low cost, decent accuracy\n  - gpt-5.1           # Balanced\n  - gpt-4o             # Higher cost, better accuracy\n</code></pre>"},{"location":"optimization/moar/troubleshooting/#iteration-count","title":"Iteration Count","text":"<p>Iteration Guidelines</p> <ul> <li>10-20 iterations: Quick exploration, good for testing</li> <li>40 iterations: Recommended for most use cases</li> <li>100+ iterations: For complex pipelines or when you need the absolute best results</li> </ul>"},{"location":"optimization/moar/troubleshooting/#evaluation-function-performance","title":"Evaluation Function Performance","text":"<p>Keep Functions Fast</p> <p>Your evaluation function will be called many times. Make sure it's efficient:</p> <ul> <li>Avoid expensive computations</li> <li>Cache results if possible</li> <li>Keep the function simple and fast</li> </ul>"},{"location":"optimization/moar/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still experiencing issues:</p> <ol> <li>Check the MOAR logs for detailed error messages</li> <li>Verify your evaluation function works independently</li> <li>Test with a smaller <code>max_iterations</code> to isolate issues</li> <li>Review the Configuration Reference to ensure all required fields are set</li> </ol>"},{"location":"pandas/","title":"Pandas Integration","text":"<p>DocETL provides seamless integration for several operators (map, filter, merge, agg, split, gather, unnest) with pandas through a dataframe accessor. This idea was proposed by LOTUS<sup>1</sup>. </p>"},{"location":"pandas/#installation","title":"Installation","text":"<p>The pandas integration is included in the main DocETL package:</p> <pre><code>pip install docetl\n</code></pre>"},{"location":"pandas/#overview","title":"Overview","text":"<p>The pandas integration provides a <code>.semantic</code> accessor that enables:</p> <ul> <li>Semantic mapping with LLMs (<code>df.semantic.map()</code>)</li> <li>Intelligent filtering (<code>df.semantic.filter()</code>)</li> <li>Fuzzy merging of DataFrames (<code>df.semantic.merge()</code>)</li> <li>Semantic aggregation (<code>df.semantic.agg()</code>)</li> <li>Content splitting into chunks (<code>df.semantic.split()</code>)</li> <li>Contextual information gathering (<code>df.semantic.gather()</code>)</li> <li>Data structure unnesting (<code>df.semantic.unnest()</code>)</li> <li>Cost tracking and operation history</li> </ul>"},{"location":"pandas/#quick-example","title":"Quick Example","text":"<pre><code>import pandas as pd\nfrom docetl import SemanticAccessor\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    \"text\": [\n        \"Apple released the iPhone 15 with USB-C port\",\n        \"Microsoft's new Surface laptops feature AI capabilities\",\n        \"Google announces Pixel 8 with enhanced camera features\"\n    ]\n})\n\n# Configure the semantic accessor\ndf.semantic.set_config(default_model=\"gpt-4o-mini\")\n\n# Extract structured information\nresult = df.semantic.map(\n    prompt=\"Extract company and product from: {{input.text}}\",\n    output={\n        \"schema\": {\n            \"company\": \"str\",\n            \"product\": \"str\",\n            \"features\": \"list[str]\"\n        }\n    }\n)\n\n# Track costs\nprint(f\"Operation cost: ${result.semantic.total_cost}\")\n</code></pre>"},{"location":"pandas/#configuration","title":"Configuration","text":"<p>Configure the semantic accessor with your preferred settings:</p> <pre><code>df.semantic.set_config(\n    default_model=\"gpt-4o-mini\",  # Default LLM to use\n    max_threads=64,              # Maximum concurrent threads,\n    rate_limits={\n        \"embedding_call\": [\n            {\"count\": 1000, \"per\": 1, \"unit\": \"second\"}\n        ],\n        \"llm_call\": [\n            {\"count\": 1, \"per\": 1, \"unit\": \"second\"},\n            {\"count\": 10, \"per\": 5, \"unit\": \"hour\"}\n        ]\n    } \n)\n</code></pre> <p>Pipeline Optimization</p> <p>While individual semantic operations are optimized internally, pipelines created through the pandas <code>.semantic</code> accessor (sequences of operations like <code>map</code> \u2192 <code>filter</code> \u2192 <code>merge</code>) cannot be optimized as a whole. For pipeline-level optimizations like operation rewriting and automatic resolve operation insertion, you must use either:</p> <ul> <li>The YAML configuration interface</li> <li>The Python API</li> </ul> <p>For detailed configuration options and best practices, refer to:</p> <ul> <li>DocETL Best Practices</li> <li>Pipeline Configuration</li> <li>Output Schemas</li> <li>Rate Limiting </li> </ul>"},{"location":"pandas/#output-modes","title":"Output Modes","text":"<p>DocETL supports two output modes for LLM calls:</p>"},{"location":"pandas/#tools-mode-default","title":"Tools Mode (Default)","text":"<p>Uses function calling to ensure structured outputs: <pre><code>result = df.semantic.map(\n    prompt=\"Extract data from: {{input.text}}\",\n    output={\n        \"schema\": {\"name\": \"str\", \"age\": \"int\"},\n        \"mode\": \"tools\"  # Default mode\n    }\n)\n</code></pre></p>"},{"location":"pandas/#structured-output-mode","title":"Structured Output Mode","text":"<p>Uses native JSON schema validation for supported models (like GPT-4o): <pre><code>result = df.semantic.map(\n    prompt=\"Extract data from: {{input.text}}\",\n    output={\n        \"schema\": {\"name\": \"str\", \"age\": \"int\"},\n        \"mode\": \"structured_output\"  # Better JSON schema support\n    }\n)\n</code></pre></p> <p>When to Use Structured Output Mode</p> <p>Use <code>\"structured_output\"</code> mode when: - You're using models that support native JSON schema (like GPT-4o) - You need stricter adherence to complex JSON schemas - You want potentially better performance for structured data extraction</p> <p>The default <code>\"tools\"</code> mode works with all models and is more widely compatible.</p>"},{"location":"pandas/#backward-compatibility","title":"Backward Compatibility","text":"<p>The old <code>output_schema</code> parameter is still supported for backward compatibility: <pre><code># This still works (automatically uses tools mode)\nresult = df.semantic.map(\n    prompt=\"Extract data from: {{input.text}}\",\n    output_schema={\"name\": \"str\", \"age\": \"int\"}\n)\n</code></pre></p>"},{"location":"pandas/#cost-tracking","title":"Cost Tracking","text":"<p>All semantic operations track their LLM usage costs:</p> <pre><code># Get total cost of operations\ntotal_cost = df.semantic.total_cost\n\n# Get operation history\nhistory = df.semantic.history\nfor op in history:\n    print(f\"Operation: {op.op_type}\")\n    print(f\"Modified columns: {op.output_columns}\")\n</code></pre>"},{"location":"pandas/#implementation","title":"Implementation","text":"<p>This implementation is inspired by LOTUS, a system introduced by Patel et al. <sup>1</sup>. Our implementation has a few differences:</p> <ul> <li>We use DocETL's query engine to run the LLM operations. This allows us to use retries, validation, well-defined output schemas, and other features described in our documentation.</li> <li>Our aggregation operator combines the <code>resolve</code> and <code>reduce</code> operators, so you can get a fuzzy groupby.</li> <li>Our merge operator is based on our equijoin operator implementation, which optimizes LLM call usage by generating blocking rules before running the LLM. See the Equijoin Operator for more details.</li> <li>We do not implement LOTUS's <code>sem_extract</code>, <code>sem_topk</code>, <code>sem_sim_join</code>, and <code>sem_search</code> operators. However, <code>sem_extract</code> can effectively be implemented by running the <code>map</code> operator with a prompt that describes the extraction.</li> </ul> <ol> <li> <p>Patel, L., Jha, S., Asawa, P., Pan, M., Guestrin, C., &amp; Zaharia, M. (2024). Semantic Operators: A Declarative Model for Rich, AI-based Analytics Over Text Data. arXiv preprint arXiv:2407.11418. https://arxiv.org/abs/2407.11418 \u21a9\u21a9</p> </li> </ol>"},{"location":"pandas/examples/","title":"Examples","text":"<p>Here are some demonstrating how to use DocETL's pandas integration for various tasks.</p> <p>Note that caching is enabled, but intermediate outputs are not persisted like in DocETL's YAML interface.</p>"},{"location":"pandas/examples/#example-1-analyzing-customer-reviews","title":"Example 1: Analyzing Customer Reviews","text":"<p>Extract structured insights from customer reviews:</p> <pre><code>import pandas as pd\nfrom docetl import SemanticAccessor\n\n# Load customer reviews\ndf = pd.DataFrame({\n    \"review\": [\n        \"Great laptop, fast processor but battery life could be better\",\n        \"The camera quality is amazing, especially in low light\",\n        \"Keyboard feels cheap and the screen is too dim\"\n    ]\n})\n\n# Configure semantic accessor\ndf.semantic.set_config(default_model=\"gpt-4o-mini\")\n\n# Extract structured insights\nresult = df.semantic.map(\n    prompt=\"\"\"Analyze this product review and extract:\n    1. Mentioned features\n    2. Sentiment per feature\n    3. Overall sentiment\n\n    Review: {{input.review}}\"\"\",\n    output={\n        \"schema\": {\n            \"features\": \"list[str]\",\n            \"feature_sentiments\": \"dict[str, str]\",\n            \"overall_sentiment\": \"str\"\n        }\n    }\n)\n\n# Filter for negative reviews\nnegative_reviews = result.semantic.filter(\n    prompt=\"Is this review predominantly negative? Consider the overall sentiment and feature sentiments.\\n{{input}}\"\n)\n</code></pre>"},{"location":"pandas/examples/#example-2-deduplicating-customer-records","title":"Example 2: Deduplicating Customer Records","text":"<p>Identify and merge duplicate customer records using fuzzy matching:</p> <pre><code># Customer records from two sources\ndf1 = pd.DataFrame({\n    \"name\": [\"John Smith\", \"Mary Johnson\"],\n    \"email\": [\"john@email.com\", \"mary.j@email.com\"],\n    \"address\": [\"123 Main St\", \"456 Oak Ave\"]\n})\n\ndf2 = pd.DataFrame({\n    \"name\": [\"John A Smith\", \"Mary Johnson\"],\n    \"email\": [\"john@email.com\", \"mary.johnson@email.com\"],\n    \"address\": [\"123 Main Street\", \"456 Oak Avenue\"]\n})\n\n# Merge records with fuzzy matching\nmerged = df1.semantic.merge(\n    df2,\n    comparison_prompt=\"\"\"Compare these customer records and determine if they represent the same person.\n    Consider name variations, email patterns, and address formatting.\n\n    Record 1:\n    Name: {{input1.name}}\n    Email: {{input1.email}}\n    Address: {{input1.address}}\n\n    Record 2:\n    Name: {{input2.name}}\n    Email: {{input2.email}}\n    Address: {{input2.address}}\"\"\",\n    fuzzy=True, # This will automatically invoke optimization\n)\n</code></pre>"},{"location":"pandas/examples/#example-3-topic-analysis-of-news-articles","title":"Example 3: Topic Analysis of News Articles","text":"<p>Group and summarize news articles by topic:</p> <pre><code># News articles\ndf = pd.DataFrame({\n    \"title\": [\"Apple's New iPhone Launch\", \"Tech Giants Face Regulation\", \"AI Advances in Healthcare\"],\n    \"content\": [\"Apple announced...\", \"Lawmakers propose...\", \"Researchers develop...\"]\n})\n\n# First, use a semantic map to extract the topic from each article\ndf = df.semantic.map(\n    prompt=\"Extract the topic from this article: {{input.content}}\",\n    output={\"schema\": {\"topic\": \"str\"}}\n)\n\n# Group similar articles and generate summaries\nsummaries = df.semantic.agg(\n    # First, group similar articles\n    fuzzy=True,\n    reduce_keys=[\"topic\"],\n    comparison_prompt=\"\"\"Are these articles about the same topic or closely related topics?\n\n    Article 1:\n    Title: {{input1.title}}\n    Content: {{input1.content}}\n\n    Article 2:\n    Title: {{input2.title}}\n    Content: {{input2.content}}\"\"\",\n\n    # Then, generate a summary for each group\n    reduce_prompt=\"\"\"Summarize these related articles into a comprehensive overview:\n\n    Articles:\n    {{inputs}}\"\"\",\n\n    output={\n        \"schema\": {\n            \"summary\": \"str\",\n            \"key_points\": \"list[str]\"\n        }\n    }\n)\n\n# Summaries will be a df with the following columns:\n# - topic: str (because this was the reduce_keys)\n# - summary: str\n# - key_points: list[str]\n</code></pre>"},{"location":"pandas/examples/#example-4-multi-step-analysis-pipeline","title":"Example 4: Multi-step Analysis Pipeline","text":"<p>Combine multiple operations for complex analysis:</p> <pre><code># Social media posts\nposts = pd.DataFrame({\n    \"text\": [\"Just tried the new iPhone 15!\", \"Having issues with iOS 17\", \"Android is better\"],\n    \"timestamp\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"]\n})\n\n# 1. Extract structured information\nanalyzed = posts.semantic.map(\n    prompt=\"\"\"Analyze this social media post and extract:\n    1. Product mentioned\n    2. Sentiment\n    3. Issues/Praise points\n\n    Post: {{input.text}}\"\"\",\n    output={\n        \"schema\": {\n            \"product\": \"str\",\n            \"sentiment\": \"str\",\n            \"points\": \"list[str]\"\n        }\n    }\n)\n\n# 2. Filter relevant posts\nrelevant = analyzed.semantic.filter(\n    prompt=\"Is this post about Apple products? {{input}}\"\n)\n\n\n# 3. Group by issue and summarize\nsummaries = relevant.semantic.agg(\n    fuzzy=True,\n    reduce_keys=[\"product\"],\n    comparison_prompt=\"Do these posts discuss the same product?\",\n    reduce_prompt=\"Summarize the feedback about this product\",\n    output={\n        \"schema\": {\n            \"summary\": \"str\",\n            \"frequency\": \"int\",\n            \"severity\": \"str\"\n        }\n    }\n)\n\n# Summaries will be a df with the following columns:\n# - product: str (because this was the reduce_keys)\n# - summary: str\n# - frequency: int\n# - severity: str\n\n# Track total cost\nprint(f\"Total analysis cost: ${summaries.semantic.total_cost}\")\n</code></pre>"},{"location":"pandas/examples/#example-5-error-handling-and-validation","title":"Example 5: Error Handling and Validation","text":"<p>Implement robust error handling and validation:</p> <pre><code># Product descriptions\ndf = pd.DataFrame({\n    \"description\": [\"High-performance laptop...\", \"Wireless earbuds...\", \"Invalid/\"]\n})\n\ntry:\n    result = df.semantic.map(\n        prompt=\"Extract product specifications from: {{input.description}}. There should be at least one feature.\",\n        output={\n            \"schema\": {\n                \"category\": \"str\",\n                \"features\": \"list[str]\",\n                \"price_range\": \"enum[budget, mid-range, premium, luxury]\"\n            }\n        },\n        # Validation rules\n        validate=[\n            \"len(output['features']) &gt;= 1\",\n        ],\n        # Retry configuration\n        num_retries_on_validate_failure=2,\n    )\n\n    # Check operation history\n    for op in result.semantic.history:\n        print(f\"Operation: {op.op_type}\")\n        print(f\"Modified columns: {op.output_columns}\")\n\nexcept Exception as e:\n    print(f\"Error during processing: {e}\")\n</code></pre>"},{"location":"pandas/examples/#example-6-pdf-analysis","title":"Example 6: PDF Analysis","text":"<p>DocETL supports native PDF handling with Claude and Gemini, in map and filter operations. Suppose you have a column in your pandas dataframe with PDF paths (1 path per row), and you want the LLM to do some analysis for each PDF. You can do this by setting the <code>pdf_url_key</code> parameter in the map or filter operation.</p> <pre><code>df = pd.DataFrame({\n    \"PdfPath\": [\"https://docetlcloudbank.blob.core.windows.net/ntsb-reports/Report_N617GC.pdf\", \"https://docetlcloudbank.blob.core.windows.net/ntsb-reports/Report_CEN25LA075.pdf\"]\n})\n\nresult_df = df.semantic.map(\n    prompt=\"Summarize the air crash report and determine any contributing factors\",\n    output={\n        \"schema\": {\"summary\": \"str\", \"contributing_factors\": \"list[str]\"}\n    },\n    pdf_url_key=\"PdfPath\", # This is the column with the PDF paths\n)\n\nprint(result_df.head()) # The result will have the same number of rows as the input dataframe, with the summary and contributing factors added\n</code></pre>"},{"location":"pandas/examples/#example-7-synthetic-data-generation","title":"Example 7: Synthetic Data Generation","text":"<p>DocETL supports generating multiple outputs for each input using the <code>n</code> parameter in the map operation. This is useful for synthetic data generation, A/B testing content variations, or creating multiple alternatives for each input.</p> <pre><code># Starter concepts for content generation\ndf = pd.DataFrame({\n    \"product\": [\"Smart Watch\", \"Wireless Earbuds\", \"Home Security Camera\"],\n    \"target_audience\": [\"Fitness Enthusiasts\", \"Music Lovers\", \"Homeowners\"]\n})\n\n# Generate 5 marketing headlines for each product-audience combination\nvariations = df.semantic.map(\n    prompt=\"\"\"Generate a compelling marketing headline for the following product and target audience:\n\n    Product: {{input.product}}\n    Target Audience: {{input.target_audience}}\n\n    The headline should:\n    - Be attention-grabbing and memorable\n    - Speak directly to the target audience's needs or desires\n    - Highlight the key benefit of the product\n    - Be between 5-10 words\n    \"\"\",\n    output={\"schema\": {\"headline\": \"str\"}, \"n\": 5}  # Generate 5 variations for each input row\n)\n\nprint(f\"Original dataframe rows: {len(df)}\")\nprint(f\"Generated variations: {len(variations)}\")  # Should be 5x the original count\n\n# Variations dataframe will contain:\n# - All original columns (product, target_audience)\n# - The new headline column\n# - 5 rows for each original row (15 total for this example)\n\n# You can also combine this with other operations\n# Filter to only keep the best headlines\nbest_headlines = variations.semantic.filter(\n    prompt=\"\"\"Is this headline exceptional and likely to drive high engagement?\n\n    Product: {{input.product}}\n    Target Audience: {{input.target_audience}}\n    Headline: {{input.headline}}\n\n    Consider catchiness, emotional appeal, and clarity.\n    \"\"\"\n)\n</code></pre> <p>This example will generate 15 total variations (3 products \u00d7 5 variations each). You can adjust the <code>n</code> parameter to generate more or fewer variations as needed.</p>"},{"location":"pandas/examples/#example-8-document-processing-with-split-and-gather","title":"Example 8: Document Processing with Split and Gather","text":"<p>Process long documents by splitting them into chunks and adding contextual information:</p> <pre><code># Long documents that need to be processed in chunks\ndf = pd.DataFrame({\n    \"document_id\": [\"doc1\", \"doc2\"],\n    \"title\": [\"Technical Manual\", \"Research Paper\"],\n    \"content\": [\n        \"Chapter 1: Introduction\\n\\nThis manual provides comprehensive guidance for system installation and configuration. The installation process involves several critical steps that must be followed in order.\\n\\nChapter 2: Prerequisites\\n\\nBefore beginning installation, ensure all system requirements are met. This includes hardware specifications, software dependencies, and network connectivity.\\n\\nChapter 3: Installation\\n\\nThe installation wizard guides users through the setup process. Select appropriate configuration options based on your deployment environment.\",\n        \"Abstract\\n\\nThis research investigates the impact of machine learning on data processing efficiency. Our methodology involved testing multiple algorithms across diverse datasets.\\n\\nIntroduction\\n\\nMachine learning has revolutionized data processing across industries. Previous studies have shown significant improvements in processing speed and accuracy.\\n\\nMethodology\\n\\nWe conducted experiments using three different ML algorithms: neural networks, decision trees, and support vector machines. Each algorithm was tested on datasets ranging from 1,000 to 1,000,000 records.\"\n    ]\n})\n\n# Step 1: Split documents into manageable chunks\nchunks = df.semantic.split(\n    split_key=\"content\",\n    method=\"delimiter\",\n    method_kwargs={\"delimiter\": \"\\n\\n\", \"num_splits_to_group\": 1}\n)\n\nprint(f\"Original documents: {len(df)}\")\nprint(f\"Generated chunks: {len(chunks)}\")\n\n# Step 2: Add contextual information to each chunk\nenhanced_chunks = chunks.semantic.gather(\n    content_key=\"content_chunk\",\n    doc_id_key=\"semantic_split_0_id\",\n    order_key=\"semantic_split_0_chunk_num\",\n    peripheral_chunks={\n        \"previous\": {\"head\": {\"count\": 1}},  # Include 1 previous chunk\n        \"next\": {\"head\": {\"count\": 1}}       # Include 1 next chunk\n    }\n)\n\n# Step 3: Extract structured information from each chunk with context\nanalyzed_chunks = enhanced_chunks.semantic.map(\n    prompt=\"\"\"Analyze this document chunk with its surrounding context and extract:\n    1. Main topic or section\n    2. Key concepts discussed\n    3. Action items or requirements (if any)\n\n    Document chunk with context:\n    {{input.content_chunk_rendered}}\"\"\",\n    output={\n        \"schema\": {\n            \"section_topic\": \"str\",\n            \"key_concepts\": \"list[str]\",\n            \"action_items\": \"list[str]\"\n        }\n    }\n)\n\n# Step 4: Aggregate insights by document\ndocument_summaries = analyzed_chunks.semantic.agg(\n    reduce_keys=[\"document_id\"],\n    reduce_prompt=\"\"\"Create a comprehensive summary of this document based on all its chunks:\n\n    Document chunks:\n    {% for chunk in inputs %}\n    Section: {{chunk.section_topic}}\n    Key concepts: {{chunk.key_concepts | join(', ')}}\n    Action items: {{chunk.action_items | join(', ')}}\n    ---\n    {% endfor %}\"\"\",\n    output={\n        \"schema\": {\n            \"document_summary\": \"str\",\n            \"all_key_concepts\": \"list[str]\",\n            \"all_action_items\": \"list[str]\"\n        }\n    }\n)\n\nprint(f\"Final document summaries: {len(document_summaries)}\")\nprint(f\"Total processing cost: ${document_summaries.semantic.total_cost}\")\n</code></pre>"},{"location":"pandas/examples/#example-9-data-structure-processing-with-unnest","title":"Example 9: Data Structure Processing with Unnest","text":"<p>Handle complex nested data structures commonly found in JSON APIs or survey responses:</p> <pre><code># Survey data with nested responses\nsurvey_df = pd.DataFrame({\n    \"respondent_id\": [1, 2, 3],\n    \"demographics\": [\n        {\"age\": 25, \"location\": \"NYC\", \"education\": \"Bachelor's\"},\n        {\"age\": 34, \"location\": \"SF\", \"education\": \"Master's\"},\n        {\"age\": 28, \"location\": \"Chicago\", \"education\": \"PhD\"}\n    ],\n    \"interests\": [\n        [\"technology\", \"sports\", \"music\"],\n        [\"science\", \"reading\"],\n        [\"art\", \"travel\", \"cooking\", \"photography\"]\n    ],\n    \"ratings\": [\n        {\"product_quality\": 4, \"customer_service\": 5, \"value\": 3},\n        {\"product_quality\": 5, \"customer_service\": 4, \"value\": 4},\n        {\"product_quality\": 3, \"customer_service\": 3, \"value\": 5}\n    ]\n})\n\n# Step 1: Unnest demographics into separate columns\nwith_demographics = survey_df.semantic.unnest(\n    unnest_key=\"demographics\",\n    expand_fields=[\"age\", \"location\", \"education\"]\n)\n\n# Step 2: Unnest interests (each interest becomes a separate row)\nindividual_interests = with_demographics.semantic.unnest(\n    unnest_key=\"interests\"\n)\n\n# Step 3: For ratings, expand all fields \nwith_ratings = individual_interests.semantic.unnest(\n    unnest_key=\"ratings\",\n    expand_fields=[\"product_quality\", \"customer_service\", \"value\"]\n)\n\nprint(f\"Original survey responses: {len(survey_df)}\")\nprint(f\"Individual interest entries: {len(individual_interests)}\")\nprint(f\"Final flattened dataset: {len(with_ratings)}\")\n\n# Now you can analyze individual interests by demographics\ninterest_analysis = with_ratings.semantic.map(\n    prompt=\"\"\"Analyze this person's interest in the context of their demographics:\n\n    Person: {{input.age}} years old, {{input.education}}, from {{input.location}}\n    Interest: {{input.interests}}\n    Product ratings: Quality={{input.product_quality}}, Service={{input.customer_service}}, Value={{input.value}}\n\n    Provide insights about how this interest might relate to their demographics and satisfaction.\"\"\",\n    output={\n        \"schema\": {\n            \"demographic_insight\": \"str\",\n            \"interest_category\": \"str\",\n            \"satisfaction_correlation\": \"str\"\n        }\n    }\n)\n\n# Aggregate insights by interest category\ncategory_insights = interest_analysis.semantic.agg(\n    reduce_keys=[\"interest_category\"],\n    reduce_prompt=\"\"\"Summarize insights about people interested in {{inputs[0].interest_category}}:\n\n    {% for person in inputs %}\n    - {{person.age}} year old {{person.education}} from {{person.location}}: {{person.demographic_insight}}\n    {% endfor %}\"\"\",\n    output={\n        \"schema\": {\n            \"category_summary\": \"str\",\n            \"typical_demographics\": \"str\",\n            \"satisfaction_patterns\": \"str\"\n        }\n    }\n)\n</code></pre>"},{"location":"pandas/examples/#example-10-combined-document-workflow","title":"Example 10: Combined Document Workflow","text":"<p>A comprehensive example combining multiple operations for end-to-end document processing:</p> <pre><code># Research papers with metadata\npapers_df = pd.DataFrame({\n    \"paper_id\": [\"P001\", \"P002\", \"P003\"],\n    \"title\": [\n        \"Deep Learning for Natural Language Processing\",\n        \"Quantum Computing Applications in Cryptography\", \n        \"Sustainable Energy Storage Solutions\"\n    ],\n    \"abstract\": [\"This paper explores...\", \"We investigate...\", \"This study examines...\"],\n    \"full_text\": [\n        \"Introduction\\n\\nDeep learning has revolutionized NLP...\\n\\nMethodology\\n\\nWe employed transformer architectures...\\n\\nResults\\n\\nOur experiments show significant improvements...\\n\\nConclusion\\n\\nThe findings demonstrate...\",\n        \"Introduction\\n\\nQuantum computing promises...\\n\\nBackground\\n\\nClassical cryptography relies...\\n\\nQuantum Algorithms\\n\\nShor's algorithm can factor...\\n\\nConclusion\\n\\nQuantum computing will require...\",\n        \"Introduction\\n\\nRenewable energy adoption...\\n\\nCurrent Challenges\\n\\nEnergy storage remains...\\n\\nProposed Solutions\\n\\nWe propose novel battery...\\n\\nResults\\n\\nOur testing shows...\"\n    ],\n    \"authors\": [\n        [\"Dr. Smith\", \"Prof. Johnson\", \"Dr. Lee\"],\n        [\"Prof. Chen\", \"Dr. Wilson\"],\n        [\"Dr. Brown\", \"Prof. Davis\", \"Dr. Taylor\", \"Prof. Anderson\"]\n    ]\n})\n\n# Step 1: Unnest authors for author-level analysis\nauthor_papers = papers_df.semantic.unnest(unnest_key=\"authors\")\n\n# Step 2: Split full text into sections\npaper_sections = papers_df.semantic.split(\n    split_key=\"full_text\",\n    method=\"delimiter\",\n    method_kwargs={\"delimiter\": \"\\n\\n\", \"num_splits_to_group\": 1}\n)\n\n# Step 3: Add context to each section\ncontextual_sections = paper_sections.semantic.gather(\n    content_key=\"full_text_chunk\",\n    doc_id_key=\"semantic_split_0_id\",\n    order_key=\"semantic_split_0_chunk_num\",\n    peripheral_chunks={\n        \"previous\": {\"head\": {\"count\": 1}},\n        \"next\": {\"head\": {\"count\": 1}}\n    }\n)\n\n# Step 4: Extract insights from each section\nsection_insights = contextual_sections.semantic.map(\n    prompt=\"\"\"Analyze this paper section in context:\n\nPaper: {{input.title}}\nSection content: {{input.full_text_chunk_rendered}}\n\nExtract:\n1. Section type (Introduction, Methodology, Results, etc.)\n2. Key findings or claims\n3. Technical concepts mentioned\n4. Research gaps or future work mentioned\"\"\",\n    output={\n        \"schema\": {\n            \"section_type\": \"str\",\n            \"key_findings\": \"list[str]\",\n            \"technical_concepts\": \"list[str]\",\n            \"future_work\": \"list[str]\"\n        }\n    }\n)\n\n# Step 5: Aggregate insights by paper\npaper_summaries = section_insights.semantic.agg(\n    reduce_keys=[\"paper_id\"],\n    reduce_prompt=\"\"\"Create a comprehensive analysis of this research paper:\n\nPaper: {{inputs[0].title}}\n\nSections analyzed:\n{% for section in inputs %}\n{{section.section_type}}: {{section.key_findings | join(', ')}}\nTechnical concepts: {{section.technical_concepts | join(', ')}}\n{% endfor %}\n\nProvide a structured summary.\"\"\",\n    output={\n        \"schema\": {\n            \"comprehensive_summary\": \"str\",\n            \"main_contributions\": \"list[str]\",\n            \"methodology_type\": \"str\",\n            \"research_field\": \"str\"\n        }\n    }\n)\n\n# Step 6: Cross-paper analysis\nfield_analysis = paper_summaries.semantic.agg(\n    reduce_keys=[\"research_field\"],\n    fuzzy=True,  # Group similar research fields\n    reduce_prompt=\"\"\"Analyze research trends in this field based on these papers:\n\n{% for paper in inputs %}\nPaper: {{paper.title}}\nSummary: {{paper.comprehensive_summary}}\nContributions: {{paper.main_contributions | join(', ')}}\n---\n{% endfor %}\"\"\",\n    output={\n        \"schema\": {\n            \"field_trends\": \"str\",\n            \"common_methodologies\": \"list[str]\",\n            \"emerging_themes\": \"list[str]\"\n        }\n    }\n)\n\nprint(f\"Papers processed: {len(papers_df)}\")\nprint(f\"Authors identified: {len(author_papers)}\")\nprint(f\"Sections analyzed: {len(section_insights)}\")\nprint(f\"Research fields identified: {len(field_analysis)}\")\nprint(f\"Total processing cost: ${field_analysis.semantic.total_cost}\")\n</code></pre>"},{"location":"pandas/operations/","title":"Semantic Operations","text":"<p>The pandas integration provides several semantic operations through the <code>.semantic</code> accessor. Each operation is designed to handle specific types of transformations and analyses using LLMs.</p> <p>All semantic operations return a new DataFrame that preserves the original columns and adds new columns based on the output schema. For example, if your original DataFrame has a column <code>text</code> and you use <code>map</code> with an <code>output={\"schema\": {\"sentiment\": \"str\", \"keywords\": \"list[str]\"}}</code>, the resulting DataFrame will have three columns: <code>text</code>, <code>sentiment</code>, and <code>keywords</code>. This makes it easy to chain operations and maintain data lineage.</p>"},{"location":"pandas/operations/#map-operation","title":"Map Operation","text":"<p>Apply semantic mapping to each row using a language model.</p> <p>Documentation: https://ucbepic.github.io/docetl/operators/map/</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Jinja template string for generating prompts. Use {{input.column_name}}    to reference input columns.</p> required <code>output</code> <code>dict[str, Any]</code> <p>Dictionary containing output configuration with keys:    - \"schema\": Dictionary defining the expected output structure and types.               Example: {\"entities\": \"list[str]\", \"sentiment\": \"str\"}    - \"mode\": Optional output mode. Either \"tools\" (default) or \"structured_output\".             \"structured_output\" uses native JSON schema mode for supported models.    - \"n\": Optional number of outputs to generate for each input (synthetic data generation)</p> <code>None</code> <code>output_schema</code> <code>dict[str, Any]</code> <p>DEPRECATED. Use 'output' parameter instead.           Dictionary defining the expected output structure for backward compatibility.</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration options: - model: LLM model to use (default: from config) - batch_prompt: Template for processing multiple documents in a single prompt - max_batch_size: Maximum number of documents to process in a single batch - optimize: Flag to enable operation optimization (default: True) - recursively_optimize: Flag to enable recursive optimization (default: False) - sample: Number of samples to use for the operation - tools: List of tool definitions for LLM use - validate: List of Python expressions to validate output - num_retries_on_validate_failure: Number of retry attempts (default: 0) - gleaning: Configuration for LLM-based refinement - drop_keys: List of keys to drop from input - timeout: Timeout for each LLM call in seconds (default: 120) - max_retries_per_timeout: Maximum retries per timeout (default: 2) - litellm_completion_kwargs: Additional parameters for LiteLLM - skip_on_error: Skip operation if LLM returns error (default: False) - bypass_cache: Bypass cache for this operation (default: False) - n: Number of outputs to generate for each input (synthetic data generation)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A new DataFrame containing the transformed data with columns          matching the output schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Extract entities and sentiment\n&gt;&gt;&gt; df.semantic.map(\n...     prompt=\"Analyze this text: {{input.text}}\",\n...     output={\n...         \"schema\": {\n...             \"entities\": \"list[str]\",\n...             \"sentiment\": \"str\"\n...         }\n...     },\n...     validate=[\"len(output['entities']) &lt;= 5\"],\n...     num_retries_on_validate_failure=2\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Generate synthetic data with multiple variations per input\n&gt;&gt;&gt; df.semantic.map(\n...     prompt=\"Create a headline for: {{input.topic}}\",\n...     output={\"schema\": {\"headline\": \"str\"}, \"n\": 5}\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Use structured output mode for better JSON schema support\n&gt;&gt;&gt; df.semantic.map(\n...     prompt=\"Extract structured data: {{input.text}}\",\n...     output={\n...         \"schema\": {\"name\": \"str\", \"age\": \"int\", \"tags\": \"list[str]\"},\n...         \"mode\": \"structured_output\"\n...     }\n... )\n</code></pre> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>def map(\n    self,\n    prompt: str,\n    output: dict[str, Any] = None,\n    *,\n    output_schema: dict[str, Any] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply semantic mapping to each row using a language model.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/map/\n\n    Args:\n        prompt: Jinja template string for generating prompts. Use {{input.column_name}}\n               to reference input columns.\n        output: Dictionary containing output configuration with keys:\n               - \"schema\": Dictionary defining the expected output structure and types.\n                          Example: {\"entities\": \"list[str]\", \"sentiment\": \"str\"}\n               - \"mode\": Optional output mode. Either \"tools\" (default) or \"structured_output\".\n                        \"structured_output\" uses native JSON schema mode for supported models.\n               - \"n\": Optional number of outputs to generate for each input (synthetic data generation)\n        output_schema: DEPRECATED. Use 'output' parameter instead.\n                      Dictionary defining the expected output structure for backward compatibility.\n        **kwargs: Additional configuration options:\n            - model: LLM model to use (default: from config)\n            - batch_prompt: Template for processing multiple documents in a single prompt\n            - max_batch_size: Maximum number of documents to process in a single batch\n            - optimize: Flag to enable operation optimization (default: True)\n            - recursively_optimize: Flag to enable recursive optimization (default: False)\n            - sample: Number of samples to use for the operation\n            - tools: List of tool definitions for LLM use\n            - validate: List of Python expressions to validate output\n            - num_retries_on_validate_failure: Number of retry attempts (default: 0)\n            - gleaning: Configuration for LLM-based refinement\n            - drop_keys: List of keys to drop from input\n            - timeout: Timeout for each LLM call in seconds (default: 120)\n            - max_retries_per_timeout: Maximum retries per timeout (default: 2)\n            - litellm_completion_kwargs: Additional parameters for LiteLLM\n            - skip_on_error: Skip operation if LLM returns error (default: False)\n            - bypass_cache: Bypass cache for this operation (default: False)\n            - n: Number of outputs to generate for each input (synthetic data generation)\n\n    Returns:\n        pd.DataFrame: A new DataFrame containing the transformed data with columns\n                     matching the output schema.\n\n    Examples:\n        &gt;&gt;&gt; # Extract entities and sentiment\n        &gt;&gt;&gt; df.semantic.map(\n        ...     prompt=\"Analyze this text: {{input.text}}\",\n        ...     output={\n        ...         \"schema\": {\n        ...             \"entities\": \"list[str]\",\n        ...             \"sentiment\": \"str\"\n        ...         }\n        ...     },\n        ...     validate=[\"len(output['entities']) &lt;= 5\"],\n        ...     num_retries_on_validate_failure=2\n        ... )\n\n        &gt;&gt;&gt; # Generate synthetic data with multiple variations per input\n        &gt;&gt;&gt; df.semantic.map(\n        ...     prompt=\"Create a headline for: {{input.topic}}\",\n        ...     output={\"schema\": {\"headline\": \"str\"}, \"n\": 5}\n        ... )\n\n        &gt;&gt;&gt; # Use structured output mode for better JSON schema support\n        &gt;&gt;&gt; df.semantic.map(\n        ...     prompt=\"Extract structured data: {{input.text}}\",\n        ...     output={\n        ...         \"schema\": {\"name\": \"str\", \"age\": \"int\", \"tags\": \"list[str]\"},\n        ...         \"mode\": \"structured_output\"\n        ...     }\n        ... )\n    \"\"\"\n    # Convert DataFrame to list of dicts for DocETL\n    input_data = self._df.to_dict(\"records\")\n\n    # Handle backward compatibility: if output_schema is provided, convert it to output format\n    if output_schema is not None and output is None:\n        output = {\"schema\": output_schema}\n        if \"n\" in kwargs:\n            output[\"n\"] = kwargs.pop(\"n\")\n    elif output is None and output_schema is None:\n        raise ValueError(\"Either 'output' or 'output_schema' must be provided\")\n    elif output is not None and output_schema is not None:\n        raise ValueError(\n            \"Cannot provide both 'output' and 'output_schema' parameters\"\n        )\n\n    # Validate output parameter\n    if not isinstance(output, dict):\n        raise ValueError(\"output must be a dictionary\")\n\n    if \"schema\" not in output:\n        raise ValueError(\"output must contain a 'schema' key\")\n\n    # Validate output mode if provided\n    output_mode = output.get(\"mode\", \"tools\")\n    if output_mode not in [\"tools\", \"structured_output\"]:\n        raise ValueError(\n            f\"output mode must be 'tools' or 'structured_output', got '{output_mode}'\"\n        )\n\n    # Create map operation config\n    map_config = {\n        \"type\": \"map\",\n        \"name\": f\"semantic_map_{len(self._history)}\",\n        \"prompt\": prompt,\n        \"output\": output,\n        **kwargs,\n    }\n\n    # Create and execute map operation\n    map_op = MapOperation(\n        runner=self.runner,\n        config=map_config,\n        default_model=self.runner.config[\"default_model\"],\n        max_threads=self.runner.max_threads,\n        console=self.runner.console,\n        status=self.runner.status,\n    )\n    results, cost = map_op.execute(input_data)\n\n    return self._record_operation(results, \"map\", map_config, cost)\n</code></pre> <p>Example usage: <pre><code># Basic map operation\ndf.semantic.map(\n    prompt=\"Extract sentiment and key points from: {{input.text}}\",\n    output={\n        \"schema\": {\n            \"sentiment\": \"str\",\n            \"key_points\": \"list[str]\"\n        }\n    },\n    validate=[\"len(output['key_points']) &lt;= 5\"],\n    num_retries_on_validate_failure=2\n)\n\n# Using structured output mode for better JSON schema support\ndf.semantic.map(\n    prompt=\"Extract detailed information from: {{input.text}}\",\n    output={\n        \"schema\": {\n            \"company\": \"str\",\n            \"product\": \"str\",\n            \"features\": \"list[str]\"\n        },\n        \"mode\": \"structured_output\"\n    }\n)\n\n# Backward compatible syntax (still supported)\ndf.semantic.map(\n    prompt=\"Extract sentiment from: {{input.text}}\",\n    output_schema={\"sentiment\": \"str\"}\n)\n</code></pre></p>"},{"location":"pandas/operations/#filter-operation","title":"Filter Operation","text":"<p>Filter DataFrame rows based on semantic conditions.</p> <p>Documentation: https://ucbepic.github.io/docetl/operators/filter/</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Jinja template string for generating prompts</p> required <code>output</code> <code>dict[str, Any] | None</code> <p>Output configuration with keys: - \"schema\": Dictionary defining the expected output structure and types   For filtering, this must be a single boolean field (e.g., {\"keep\": \"bool\"}) - \"mode\": Optional output mode. Either \"tools\" (default) or \"structured_output\"</p> <code>None</code> <code>output_schema</code> <code>dict[str, Any] | None</code> <p>DEPRECATED. Use 'output' parameter instead. Backward-compatible schema dict.</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration options: - model: LLM model to use - validate: List of validation expressions - num_retries_on_validate_failure: Number of retries - timeout: Timeout in seconds (default: 120) - max_retries_per_timeout: Max retries per timeout (default: 2) - skip_on_error: Skip rows on LLM error (default: False) - bypass_cache: Bypass cache for this operation (default: False)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Filtered DataFrame containing only rows where the model          returned True</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple filtering\n&gt;&gt;&gt; df.semantic.filter(\n...     prompt=\"Is this about technology? {{input.text}}\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom output schema\n&gt;&gt;&gt; df.semantic.filter(\n...     prompt=\"Analyze if this is relevant: {{input.text}}\",\n...     output={\"schema\": {\"keep\": \"bool\", \"reason\": \"str\"}}\n... )\n</code></pre> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>def filter(\n    self,\n    prompt: str,\n    *,\n    output: dict[str, Any] | None = None,\n    output_schema: dict[str, Any] | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter DataFrame rows based on semantic conditions.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/filter/\n\n    Args:\n        prompt: Jinja template string for generating prompts\n        output: Output configuration with keys:\n            - \"schema\": Dictionary defining the expected output structure and types\n              For filtering, this must be a single boolean field (e.g., {\"keep\": \"bool\"})\n            - \"mode\": Optional output mode. Either \"tools\" (default) or \"structured_output\"\n        output_schema: DEPRECATED. Use 'output' parameter instead. Backward-compatible schema dict.\n        **kwargs: Additional configuration options:\n            - model: LLM model to use\n            - validate: List of validation expressions\n            - num_retries_on_validate_failure: Number of retries\n            - timeout: Timeout in seconds (default: 120)\n            - max_retries_per_timeout: Max retries per timeout (default: 2)\n            - skip_on_error: Skip rows on LLM error (default: False)\n            - bypass_cache: Bypass cache for this operation (default: False)\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame containing only rows where the model\n                     returned True\n\n    Examples:\n        &gt;&gt;&gt; # Simple filtering\n        &gt;&gt;&gt; df.semantic.filter(\n        ...     prompt=\"Is this about technology? {{input.text}}\"\n        ... )\n\n        &gt;&gt;&gt; # Custom output schema\n        &gt;&gt;&gt; df.semantic.filter(\n        ...     prompt=\"Analyze if this is relevant: {{input.text}}\",\n        ...     output={\"schema\": {\"keep\": \"bool\", \"reason\": \"str\"}}\n        ... )\n    \"\"\"\n    # Convert DataFrame to list of dicts\n    input_data = self._df.to_dict(\"records\")\n\n    # Backward compatibility and defaults\n    if output is None and output_schema is not None:\n        output = {\"schema\": output_schema}\n    if output is None and output_schema is None:\n        output = {\"schema\": {\"keep\": \"bool\"}}\n\n    # Validate output\n    if not isinstance(output, dict):\n        raise ValueError(\"output must be a dictionary\")\n    if \"schema\" not in output:\n        raise ValueError(\"output must contain a 'schema' key\")\n    output_mode = output.get(\"mode\", \"tools\")\n    if output_mode not in [\"tools\", \"structured_output\"]:\n        raise ValueError(\n            f\"output mode must be 'tools' or 'structured_output', got '{output_mode}'\"\n        )\n\n    # Create map operation config for filtering\n    filter_config = {\n        \"type\": \"map\",\n        \"name\": f\"semantic_filter_{len(self._history)}\",\n        \"prompt\": prompt,\n        \"output\": output,\n        **kwargs,\n    }\n\n    # Create and execute filter operation\n    filter_op = FilterOperation(\n        runner=self.runner,\n        config=filter_config,\n        default_model=self.runner.config[\"default_model\"],\n        max_threads=self.runner.max_threads,\n        console=self.runner.console,\n        status=self.runner.status,\n    )\n    results, cost = filter_op.execute(input_data)\n\n    return self._record_operation(results, \"filter\", filter_config, cost)\n</code></pre> <p>Example usage: <pre><code># Simple filtering\ndf.semantic.filter(\n    prompt=\"Is this text about technology? {{input.text}}\"\n)\n\n# Custom output with reasons\ndf.semantic.filter(\n    prompt=\"Analyze if this is relevant: {{input.text}}\",\n    output={\n        \"schema\": {\n            \"keep\": \"bool\",\n            \"reason\": \"str\"\n        }\n    }\n)\n</code></pre></p>"},{"location":"pandas/operations/#merge-operation-experimental","title":"Merge Operation (Experimental)","text":"<p>Note: The merge operation is an experimental feature based on our equijoin operator. It provides a pandas-like interface for semantic record matching and deduplication. When <code>fuzzy=True</code>, it automatically invokes optimization to improve performance while maintaining accuracy.</p> <p>Semantically merge two DataFrames based on flexible matching criteria.</p> <p>Documentation: https://ucbepic.github.io/docetl/operators/equijoin/</p> <p>When fuzzy=True and no blocking parameters are provided, this method automatically invokes the JoinOptimizer to generate efficient blocking conditions. The optimizer will suggest blocking thresholds and conditions to improve performance while maintaining the target recall. The optimized configuration will be displayed for future reuse.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>DataFrame</code> <p>Right DataFrame to merge with</p> required <code>comparison_prompt</code> <code>str</code> <p>Prompt template for comparing records</p> required <code>fuzzy</code> <code>bool</code> <p>Whether to use fuzzy matching with optimization (default: False)</p> <code>False</code> <code>**kwargs</code> <p>Additional configuration options: - model: LLM model to use - blocking_threshold: Threshold for blocking optimization - blocking_conditions: Custom blocking conditions - target_recall: Target recall for optimization (default: 0.95) - estimated_selectivity: Estimated match rate - validate: List of validation expressions - num_retries_on_validate_failure: Number of retries - timeout: Timeout in seconds (default: 120) - max_retries_per_timeout: Max retries per timeout (default: 2)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Merged DataFrame containing matched records</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple merge\n&gt;&gt;&gt; merged_df = df1.semantic.merge(\n...     df2,\n...     comparison_prompt=\"Are these records about the same entity? {{input1}} vs {{input2}}\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Fuzzy merge with automatic optimization\n&gt;&gt;&gt; merged_df = df1.semantic.merge(\n...     df2,\n...     comparison_prompt=\"Compare: {{input1}} vs {{input2}}\",\n...     fuzzy=True,\n...     target_recall=0.9\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Fuzzy merge with manual blocking parameters\n&gt;&gt;&gt; merged_df = df1.semantic.merge(\n...     df2,\n...     comparison_prompt=\"Compare: {{input1}} vs {{input2}}\",\n...     fuzzy=False,\n...     blocking_threshold=0.8,\n...     blocking_conditions=[\"input1.category == input2.category\"]\n... )\n</code></pre> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>def merge(\n    self,\n    right: pd.DataFrame,\n    comparison_prompt: str,\n    *,\n    fuzzy: bool = False,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Semantically merge two DataFrames based on flexible matching criteria.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/equijoin/\n\n    When fuzzy=True and no blocking parameters are provided, this method automatically\n    invokes the JoinOptimizer to generate efficient blocking conditions. The optimizer\n    will suggest blocking thresholds and conditions to improve performance while\n    maintaining the target recall. The optimized configuration will be displayed\n    for future reuse.\n\n    Args:\n        right: Right DataFrame to merge with\n        comparison_prompt: Prompt template for comparing records\n        fuzzy: Whether to use fuzzy matching with optimization (default: False)\n        **kwargs: Additional configuration options:\n            - model: LLM model to use\n            - blocking_threshold: Threshold for blocking optimization\n            - blocking_conditions: Custom blocking conditions\n            - target_recall: Target recall for optimization (default: 0.95)\n            - estimated_selectivity: Estimated match rate\n            - validate: List of validation expressions\n            - num_retries_on_validate_failure: Number of retries\n            - timeout: Timeout in seconds (default: 120)\n            - max_retries_per_timeout: Max retries per timeout (default: 2)\n\n    Returns:\n        pd.DataFrame: Merged DataFrame containing matched records\n\n    Examples:\n        &gt;&gt;&gt; # Simple merge\n        &gt;&gt;&gt; merged_df = df1.semantic.merge(\n        ...     df2,\n        ...     comparison_prompt=\"Are these records about the same entity? {{input1}} vs {{input2}}\"\n        ... )\n\n        &gt;&gt;&gt; # Fuzzy merge with automatic optimization\n        &gt;&gt;&gt; merged_df = df1.semantic.merge(\n        ...     df2,\n        ...     comparison_prompt=\"Compare: {{input1}} vs {{input2}}\",\n        ...     fuzzy=True,\n        ...     target_recall=0.9\n        ... )\n\n        &gt;&gt;&gt; # Fuzzy merge with manual blocking parameters\n        &gt;&gt;&gt; merged_df = df1.semantic.merge(\n        ...     df2,\n        ...     comparison_prompt=\"Compare: {{input1}} vs {{input2}}\",\n        ...     fuzzy=False,\n        ...     blocking_threshold=0.8,\n        ...     blocking_conditions=[\"input1.category == input2.category\"]\n        ... )\n    \"\"\"\n    # Convert DataFrames to lists of dicts\n    left_data = self._df.to_dict(\"records\")\n    right_data = right.to_dict(\"records\")\n\n    # Create equijoin operation config\n    join_config = {\n        \"type\": \"equijoin\",\n        \"name\": f\"semantic_merge_{len(self._history)}\",\n        \"comparison_prompt\": comparison_prompt,\n        **kwargs,\n    }\n\n    # If fuzzy matching and no blocking params provided, use JoinOptimizer\n    if (\n        fuzzy\n        and not kwargs.get(\"blocking_threshold\")\n        and not kwargs.get(\"blocking_conditions\")\n    ):\n        join_optimizer = JoinOptimizer(\n            self.runner,\n            join_config,\n            target_recall=kwargs.get(\"target_recall\", 0.95),\n            estimated_selectivity=kwargs.get(\"estimated_selectivity\", None),\n        )\n        optimized_config, optimizer_cost, _ = join_optimizer.optimize_equijoin(\n            left_data, right_data, skip_map_gen=True, skip_containment_gen=True\n        )\n\n        # Print optimized config for reuse\n        self.runner.console.log(\n            Panel.fit(\n                \"[bold cyan]Optimized Configuration for Merge[/bold cyan]\\n\"\n                \"[yellow]Consider adding these parameters to avoid re-running optimization:[/yellow]\\n\\n\"\n                f\"blocking_threshold: {optimized_config.get('blocking_threshold')}\\n\"\n                f\"blocking_keys: {optimized_config.get('blocking_keys')}\\n\"\n                f\"blocking_conditions: {optimized_config.get('blocking_conditions', [])}\\n\",\n                title=\"Optimization Results\",\n            )\n        )\n        join_config = optimized_config\n        optimizer_cost_value = optimizer_cost\n    else:\n        optimizer_cost_value = 0.0\n\n    # Create and execute equijoin operation\n    join_op = EquijoinOperation(\n        runner=self.runner,\n        config=join_config,\n        default_model=self.runner.config[\"default_model\"],\n        max_threads=self.runner.max_threads,\n        console=self.runner.console,\n        status=self.runner.status,\n    )\n    results, cost = join_op.execute(left_data, right_data)\n\n    return self._record_operation(\n        results, \"equijoin\", join_config, cost + optimizer_cost_value\n    )\n</code></pre> <p>Example usage: <pre><code># Simple merge\nmerged_df = df1.semantic.merge(\n    df2,\n    comparison_prompt=\"Are these records about the same entity? {{input1}} vs {{input2}}\"\n)\n\n# Fuzzy merge with optimization\nmerged_df = df1.semantic.merge(\n    df2,\n    comparison_prompt=\"Compare: {{input1}} vs {{input2}}\",\n    fuzzy=True,\n    target_recall=0.9\n)\n</code></pre></p>"},{"location":"pandas/operations/#aggregate-operation","title":"Aggregate Operation","text":"<p>Semantically aggregate data with optional fuzzy matching.</p> <p>Documentation: - Resolve Operation: https://ucbepic.github.io/docetl/operators/resolve/ - Reduce Operation: https://ucbepic.github.io/docetl/operators/reduce/</p> <p>When fuzzy=True and no blocking parameters are provided in resolve_kwargs, this method automatically invokes the JoinOptimizer to generate efficient blocking conditions for the resolve phase. The optimizer will suggest blocking thresholds and conditions to improve performance while maintaining the target recall. The optimized configuration will be displayed for future reuse.</p> <p>The resolve phase is skipped if: - fuzzy=False - reduce_keys=[\"_all\"] - input data has 5 or fewer rows</p> <p>Parameters:</p> Name Type Description Default <code>reduce_prompt</code> <code>str</code> <p>Prompt template for the reduction phase</p> required <code>output</code> <code>dict[str, Any] | None</code> <p>Output configuration with keys: - \"schema\": Dictionary defining the expected output structure and types - \"mode\": Optional output mode. Either \"tools\" (default) or \"structured_output\"</p> <code>None</code> <code>output_schema</code> <code>dict[str, Any] | None</code> <p>DEPRECATED. Use 'output' parameter instead. Backward-compatible schema dict</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Whether to use fuzzy matching for resolution (default: False)</p> <code>False</code> <code>comparison_prompt</code> <code>str | None</code> <p>Prompt template for comparing records during resolution</p> <code>None</code> <code>resolution_prompt</code> <code>str | None</code> <p>Prompt template for resolving conflicts</p> <code>None</code> <code>resolution_output</code> <code>dict[str, Any] | None</code> <p>Output configuration for resolution (new API with schema key)</p> <code>None</code> <code>resolution_output_schema</code> <code>dict[str, Any] | None</code> <p>Schema for resolution output (deprecated, use resolution_output)</p> <code>None</code> <code>reduce_keys</code> <code>str | list[str]</code> <p>Keys to group by for reduction (default: [\"_all\"])</p> <code>['_all']</code> <code>resolve_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs for resolve operation: - model: LLM model to use - blocking_threshold: Threshold for blocking optimization - blocking_conditions: Custom blocking conditions - target_recall: Target recall for optimization (default: 0.95) - estimated_selectivity: Estimated match rate - validate: List of validation expressions - num_retries_on_validate_failure: Number of retries - timeout: Timeout in seconds (default: 120) - max_retries_per_timeout: Max retries per timeout (default: 2)</p> <code>{}</code> <code>reduce_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs for reduce operation: - model: LLM model to use - validate: List of validation expressions - num_retries_on_validate_failure: Number of retries - timeout: Timeout in seconds (default: 120) - max_retries_per_timeout: Max retries per timeout (default: 2)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Aggregated DataFrame with columns matching output['schema']</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple aggregation\n&gt;&gt;&gt; df.semantic.agg(\n...     reduce_prompt=\"Summarize these items: {{input.text}}\",\n...     output={\"schema\": {\"summary\": \"str\"}}\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Fuzzy matching with automatic optimization\n&gt;&gt;&gt; df.semantic.agg(\n...     reduce_prompt=\"Combine these items: {{input.text}}\",\n...     output={\"schema\": {\"combined\": \"str\"}},\n...     fuzzy=True,\n...     comparison_prompt=\"Are these items similar: {{input1.text}} vs {{input2.text}}\",\n...     resolution_prompt=\"Resolve conflicts between: {{items}}\",\n...     resolution_output={\"schema\": {\"resolved\": \"str\"}}\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Fuzzy matching with manual blocking parameters\n&gt;&gt;&gt; df.semantic.agg(\n...     reduce_prompt=\"Combine these items: {{input.text}}\",\n...     output={\"schema\": {\"combined\": \"str\"}},\n...     fuzzy=False,\n...     comparison_prompt=\"Compare items: {{input1.text}} vs {{input2.text}}\",\n...     resolve_kwargs={\n...         \"blocking_threshold\": 0.8,\n...         \"blocking_conditions\": [\"input1.category == input2.category\"]\n...     }\n... )\n</code></pre> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>    def agg(\n        self,\n        *,\n        # Reduction phase params (required)\n        reduce_prompt: str,\n        output: dict[str, Any] | None = None,\n        output_schema: dict[str, Any] | None = None,\n        # Resolution and reduce phase params (optional)\n        fuzzy: bool = False,\n        comparison_prompt: str | None = None,\n        resolution_prompt: str | None = None,\n        resolution_output: dict[str, Any] | None = None,\n        resolution_output_schema: dict[str, Any] | None = None,\n        reduce_keys: str | list[str] = [\"_all\"],\n        resolve_kwargs: dict[str, Any] = {},\n        reduce_kwargs: dict[str, Any] = {},\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Semantically aggregate data with optional fuzzy matching.\n\n        Documentation:\n        - Resolve Operation: https://ucbepic.github.io/docetl/operators/resolve/\n        - Reduce Operation: https://ucbepic.github.io/docetl/operators/reduce/\n\n        When fuzzy=True and no blocking parameters are provided in resolve_kwargs,\n        this method automatically invokes the JoinOptimizer to generate efficient\n        blocking conditions for the resolve phase. The optimizer will suggest\n        blocking thresholds and conditions to improve performance while maintaining\n        the target recall. The optimized configuration will be displayed for future reuse.\n\n        The resolve phase is skipped if:\n        - fuzzy=False\n        - reduce_keys=[\"_all\"]\n        - input data has 5 or fewer rows\n\n        Args:\n            reduce_prompt: Prompt template for the reduction phase\n            output: Output configuration with keys:\n                - \"schema\": Dictionary defining the expected output structure and types\n                - \"mode\": Optional output mode. Either \"tools\" (default) or \"structured_output\"\n            output_schema: DEPRECATED. Use 'output' parameter instead. Backward-compatible schema dict\n            fuzzy: Whether to use fuzzy matching for resolution (default: False)\n            comparison_prompt: Prompt template for comparing records during resolution\n            resolution_prompt: Prompt template for resolving conflicts\n            resolution_output: Output configuration for resolution (new API with schema key)\n            resolution_output_schema: Schema for resolution output (deprecated, use resolution_output)\n            reduce_keys: Keys to group by for reduction (default: [\"_all\"])\n            resolve_kwargs: Additional kwargs for resolve operation:\n                - model: LLM model to use\n                - blocking_threshold: Threshold for blocking optimization\n                - blocking_conditions: Custom blocking conditions\n                - target_recall: Target recall for optimization (default: 0.95)\n                - estimated_selectivity: Estimated match rate\n                - validate: List of validation expressions\n                - num_retries_on_validate_failure: Number of retries\n                - timeout: Timeout in seconds (default: 120)\n                - max_retries_per_timeout: Max retries per timeout (default: 2)\n            reduce_kwargs: Additional kwargs for reduce operation:\n                - model: LLM model to use\n                - validate: List of validation expressions\n                - num_retries_on_validate_failure: Number of retries\n                - timeout: Timeout in seconds (default: 120)\n                - max_retries_per_timeout: Max retries per timeout (default: 2)\n\n        Returns:\n            pd.DataFrame: Aggregated DataFrame with columns matching output['schema']\n\n        Examples:\n            &gt;&gt;&gt; # Simple aggregation\n            &gt;&gt;&gt; df.semantic.agg(\n            ...     reduce_prompt=\"Summarize these items: {{input.text}}\",\n            ...     output={\"schema\": {\"summary\": \"str\"}}\n            ... )\n\n            &gt;&gt;&gt; # Fuzzy matching with automatic optimization\n            &gt;&gt;&gt; df.semantic.agg(\n            ...     reduce_prompt=\"Combine these items: {{input.text}}\",\n            ...     output={\"schema\": {\"combined\": \"str\"}},\n            ...     fuzzy=True,\n            ...     comparison_prompt=\"Are these items similar: {{input1.text}} vs {{input2.text}}\",\n            ...     resolution_prompt=\"Resolve conflicts between: {{items}}\",\n            ...     resolution_output={\"schema\": {\"resolved\": \"str\"}}\n            ... )\n\n            &gt;&gt;&gt; # Fuzzy matching with manual blocking parameters\n            &gt;&gt;&gt; df.semantic.agg(\n            ...     reduce_prompt=\"Combine these items: {{input.text}}\",\n            ...     output={\"schema\": {\"combined\": \"str\"}},\n            ...     fuzzy=False,\n            ...     comparison_prompt=\"Compare items: {{input1.text}} vs {{input2.text}}\",\n            ...     resolve_kwargs={\n            ...         \"blocking_threshold\": 0.8,\n            ...         \"blocking_conditions\": [\"input1.category == input2.category\"]\n            ...     }\n            ... )\n        \"\"\"\n        # Convert DataFrame to list of dicts\n        input_data = self._df.to_dict(\"records\")\n\n        # Handle backward compatibility: if output_schema is provided, convert it to output format\n        if output_schema is not None and output is None:\n            output = {\"schema\": output_schema}\n        elif output is None and output_schema is None:\n            raise ValueError(\"Either 'output' or 'output_schema' must be provided\")\n        elif output is not None and output_schema is not None:\n            raise ValueError(\n                \"Cannot provide both 'output' and 'output_schema' parameters\"\n            )\n\n        # Validate output parameter\n        if not isinstance(output, dict):\n            raise ValueError(\"output must be a dictionary\")\n        if \"schema\" not in output:\n            raise ValueError(\"output must contain a 'schema' key\")\n\n        # Handle backward compatibility for resolution_output_schema\n        if resolution_output_schema is not None and resolution_output is None:\n            resolution_output = {\"schema\": resolution_output_schema}\n        elif resolution_output is not None and resolution_output_schema is not None:\n            raise ValueError(\n                \"Cannot provide both 'resolution_output' and 'resolution_output_schema' parameters\"\n            )\n\n        # Validate output mode if provided\n        output_mode = output.get(\"mode\", \"tools\")\n        if output_mode not in [\"tools\", \"structured_output\"]:\n            raise ValueError(\n                f\"output mode must be 'tools' or 'structured_output', got '{output_mode}'\"\n            )\n\n        # Change keys to list\n        if isinstance(reduce_keys, str):\n            reduce_keys = [reduce_keys]\n\n        # Skip resolution if using _all or fuzzy is False\n        if reduce_keys == [\"_all\"] or not fuzzy or len(input_data) &lt;= 5:\n            resolved_data = input_data\n        else:\n            # Synthesize comparison prompt if not provided\n            if comparison_prompt is None:\n                # Build record template from reduce_keys\n                record_template = \", \".join(\n                    f\"{key}: {{{{ input{0}.{key} }}}}\" for key in reduce_keys\n                )\n\n                # Add context about how fields were created\n                context = self._synthesize_comparison_context(reduce_keys)\n\n                comparison_prompt = f\"\"\"Do the following two records represent the same concept? Your answer should be true or false.{context}\n\nRecord 1: {record_template.replace('input0', 'input1')}\nRecord 2: {record_template.replace('input0', 'input2')}\"\"\"\n\n            # Configure resolution\n            resolve_config = {\n                \"type\": \"resolve\",\n                \"name\": f\"semantic_resolve_{len(self._history)}\",\n                \"comparison_prompt\": comparison_prompt,\n                **resolve_kwargs,\n            }\n\n            # Add resolution prompt and schema if provided\n            if resolution_prompt:\n                resolve_config[\"resolution_prompt\"] = resolution_prompt\n                if resolution_output:\n                    # Use the new resolution_output format\n                    resolve_config[\"output\"] = resolution_output\n                    if \"keys\" not in resolve_config[\"output\"]:\n                        # Add keys from schema if not explicitly provided\n                        resolve_config[\"output\"][\"keys\"] = list(\n                            resolution_output[\"schema\"].keys()\n                        )\n                else:\n                    # No resolution output provided, use reduce_keys\n                    resolve_config[\"output\"] = {\"keys\": reduce_keys}\n            else:\n                resolve_config[\"output\"] = {\"keys\": reduce_keys}\n\n            # If blocking params not provided, use JoinOptimizer to synthesize them\n            if not resolve_kwargs or (\n                \"blocking_threshold\" not in resolve_kwargs\n                and \"blocking_conditions\" not in resolve_kwargs\n            ):\n                join_optimizer = JoinOptimizer(\n                    self.runner,\n                    resolve_config,\n                    target_recall=(\n                        resolve_kwargs.get(\"target_recall\", 0.95)\n                        if resolve_kwargs\n                        else 0.95\n                    ),\n                    estimated_selectivity=(\n                        resolve_kwargs.get(\"estimated_selectivity\", None)\n                        if resolve_kwargs\n                        else None\n                    ),\n                )\n                optimized_config, optimizer_cost = join_optimizer.optimize_resolve(\n                    input_data\n                )\n\n                # Print optimized config for reuse\n                self.runner.console.log(\n                    Panel.fit(\n                        \"[bold cyan]Optimized Configuration for Resolve[/bold cyan]\\n\"\n                        \"[yellow]Consider adding these parameters to avoid re-running optimization:[/yellow]\\n\\n\"\n                        f\"blocking_threshold: {optimized_config.get('blocking_threshold')}\\n\"\n                        f\"blocking_keys: {optimized_config.get('blocking_keys')}\\n\"\n                        f\"blocking_conditions: {optimized_config.get('blocking_conditions', [])}\\n\",\n                        title=\"Optimization Results\",\n                    )\n                )\n            else:\n                # Use provided blocking params\n                optimized_config = resolve_config.copy()\n                optimizer_cost = 0.0\n\n            # Execute resolution with optimized config\n            resolve_op = ResolveOperation(\n                runner=self.runner,\n                config=optimized_config,\n                default_model=self.runner.config[\"default_model\"],\n                max_threads=self.runner.max_threads,\n                console=self.runner.console,\n                status=self.runner.status,\n            )\n            resolved_data, resolve_cost = resolve_op.execute(input_data)\n            _ = self._record_operation(\n                resolved_data,\n                \"resolve\",\n                optimized_config,\n                resolve_cost + optimizer_cost,\n            )\n\n        # Configure reduction\n        reduce_config = {\n            \"type\": \"reduce\",\n            \"name\": f\"semantic_reduce_{len(self._history)}\",\n            \"reduce_key\": reduce_keys,\n            \"prompt\": reduce_prompt,\n            \"output\": output,\n            **reduce_kwargs,\n        }\n\n        # Execute reduction\n        reduce_op = ReduceOperation(\n            runner=self.runner,\n            config=reduce_config,\n            default_model=self.runner.config[\"default_model\"],\n            max_threads=self.runner.max_threads,\n            console=self.runner.console,\n            status=self.runner.status,\n        )\n        results, reduce_cost = reduce_op.execute(resolved_data)\n\n        return self._record_operation(results, \"reduce\", reduce_config, reduce_cost)\n</code></pre> <p>Example usage: <pre><code># Simple aggregation\ndf.semantic.agg(\n    reduce_prompt=\"Summarize these items: {{input.text}}\",\n    output={\"schema\": {\"summary\": \"str\"}}\n)\n\n# Fuzzy matching with custom resolution\ndf.semantic.agg(\n    reduce_prompt=\"Combine these items: {{input.text}}\",\n    output={\"schema\": {\"combined\": \"str\"}},\n    fuzzy=True,\n    comparison_prompt=\"Are these items similar: {{input1.text}} vs {{input2.text}}\",\n    resolution_prompt=\"Resolve conflicts between: {{items}}\",\n    resolution_output={\"schema\": {\"resolved\": \"str\"}}\n)\n</code></pre></p>"},{"location":"pandas/operations/#split-operation","title":"Split Operation","text":"<pre><code>    Split DataFrame rows into multiple chunks based on content.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/split/\n\n    Args:\n        split_key: The column containing content to split\n        method: Splitting method, either \"token_count\" or \"delimiter\"\n        method_kwargs: Dictionary containing method-specific parameters:\n            - For \"token_count\": {\"num_tokens\": int, \"model\": str (optional)}\n            - For \"delimiter\": {\"delimiter\": str, \"num_splits_to_group\": int (optional)}\n        **kwargs: Additional configuration options:\n            - model: LLM model to use for tokenization (default: from config)\n\n    Returns:\n        pd.DataFrame: DataFrame with split content, including:\n            - {split_key}_chunk: The content of each chunk\n            - {operation_name}_id: Unique identifier for the original document\n            - {operation_name}_chunk_num: Sequential chunk number within the document\n\n    Examples:\n        &gt;&gt;&gt; # Split by token count\n        &gt;&gt;&gt; df.semantic.split(\n        ...     split_key=\"content\",\n        ...     method=\"token_count\",\n        ...     method_kwargs={\"num_tokens\": 100}\n        ... )\n\n        &gt;&gt;&gt; # Split by delimiter\n        &gt;&gt;&gt; df.semantic.split(\n        ...     split_key=\"text\",\n        ...     method=\"delimiter\",\n        ...     method_kwargs={\"delimiter\": \"\n</code></pre> <p>\", \"num_splits_to_group\": 2}             ... )</p> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>def split(\n    self, split_key: str, method: str, method_kwargs: dict[str, Any], **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Split DataFrame rows into multiple chunks based on content.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/split/\n\n    Args:\n        split_key: The column containing content to split\n        method: Splitting method, either \"token_count\" or \"delimiter\"\n        method_kwargs: Dictionary containing method-specific parameters:\n            - For \"token_count\": {\"num_tokens\": int, \"model\": str (optional)}\n            - For \"delimiter\": {\"delimiter\": str, \"num_splits_to_group\": int (optional)}\n        **kwargs: Additional configuration options:\n            - model: LLM model to use for tokenization (default: from config)\n\n    Returns:\n        pd.DataFrame: DataFrame with split content, including:\n            - {split_key}_chunk: The content of each chunk\n            - {operation_name}_id: Unique identifier for the original document\n            - {operation_name}_chunk_num: Sequential chunk number within the document\n\n    Examples:\n        &gt;&gt;&gt; # Split by token count\n        &gt;&gt;&gt; df.semantic.split(\n        ...     split_key=\"content\",\n        ...     method=\"token_count\",\n        ...     method_kwargs={\"num_tokens\": 100}\n        ... )\n\n        &gt;&gt;&gt; # Split by delimiter\n        &gt;&gt;&gt; df.semantic.split(\n        ...     split_key=\"text\",\n        ...     method=\"delimiter\",\n        ...     method_kwargs={\"delimiter\": \"\\n\\n\", \"num_splits_to_group\": 2}\n        ... )\n    \"\"\"\n    # Convert DataFrame to list of dicts\n    input_data = self._df.to_dict(\"records\")\n\n    # Create split operation config\n    split_config = {\n        \"type\": \"split\",\n        \"name\": f\"semantic_split_{len(self._history)}\",\n        \"split_key\": split_key,\n        \"method\": method,\n        \"method_kwargs\": method_kwargs,\n        **kwargs,\n    }\n\n    # Create and execute split operation\n    split_op = SplitOperation(\n        runner=self.runner,\n        config=split_config,\n        default_model=self.runner.config[\"default_model\"],\n        max_threads=self.runner.max_threads,\n        console=self.runner.console,\n        status=self.runner.status,\n    )\n    results, cost = split_op.execute(input_data)\n\n    return self._record_operation(results, \"split\", split_config, cost)\n</code></pre> <p>Example usage: <pre><code># Split by token count\ndf.semantic.split(\n    split_key=\"content\",\n    method=\"token_count\",\n    method_kwargs={\"num_tokens\": 100}\n)\n\n# Split by delimiter\ndf.semantic.split(\n    split_key=\"text\",\n    method=\"delimiter\",\n    method_kwargs={\"delimiter\": \"\\n\\n\", \"num_splits_to_group\": 2}\n)\n</code></pre></p>"},{"location":"pandas/operations/#gather-operation","title":"Gather Operation","text":"<p>Gather contextual information from surrounding chunks to enhance each chunk.</p> <p>Documentation: https://ucbepic.github.io/docetl/operators/gather/</p> <p>Parameters:</p> Name Type Description Default <code>content_key</code> <code>str</code> <p>The column containing the main content to be enhanced</p> required <code>doc_id_key</code> <code>str</code> <p>The column containing document identifiers to group chunks</p> required <code>order_key</code> <code>str</code> <p>The column containing chunk order numbers within documents</p> required <code>peripheral_chunks</code> <code>dict[str, Any] | None</code> <p>Configuration for surrounding context: - previous: {\"head\": {\"count\": int}, \"tail\": {\"count\": int}, \"middle\": {}} - next: {\"head\": {\"count\": int}, \"tail\": {\"count\": int}, \"middle\": {}}</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration options: - main_chunk_start: Start marker for main chunk (default: \"--- Begin Main Chunk ---\") - main_chunk_end: End marker for main chunk (default: \"--- End Main Chunk ---\") - doc_header_key: Column containing document headers (optional)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with enhanced content including: - {content_key}_rendered: The main content with surrounding context</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic gathering with surrounding context\n&gt;&gt;&gt; df.semantic.gather(\n...     content_key=\"chunk_content\",\n...     doc_id_key=\"document_id\",\n...     order_key=\"chunk_number\",\n...     peripheral_chunks={\n...         \"previous\": {\"head\": {\"count\": 2}, \"tail\": {\"count\": 1}},\n...         \"next\": {\"head\": {\"count\": 1}, \"tail\": {\"count\": 2}}\n...     }\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Simple gathering without peripheral chunks\n&gt;&gt;&gt; df.semantic.gather(\n...     content_key=\"content\",\n...     doc_id_key=\"doc_id\",\n...     order_key=\"order\"\n... )\n</code></pre> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>def gather(\n    self,\n    content_key: str,\n    doc_id_key: str,\n    order_key: str,\n    peripheral_chunks: dict[str, Any] | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Gather contextual information from surrounding chunks to enhance each chunk.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/gather/\n\n    Args:\n        content_key: The column containing the main content to be enhanced\n        doc_id_key: The column containing document identifiers to group chunks\n        order_key: The column containing chunk order numbers within documents\n        peripheral_chunks: Configuration for surrounding context:\n            - previous: {\"head\": {\"count\": int}, \"tail\": {\"count\": int}, \"middle\": {}}\n            - next: {\"head\": {\"count\": int}, \"tail\": {\"count\": int}, \"middle\": {}}\n        **kwargs: Additional configuration options:\n            - main_chunk_start: Start marker for main chunk (default: \"--- Begin Main Chunk ---\")\n            - main_chunk_end: End marker for main chunk (default: \"--- End Main Chunk ---\")\n            - doc_header_key: Column containing document headers (optional)\n\n    Returns:\n        pd.DataFrame: DataFrame with enhanced content including:\n            - {content_key}_rendered: The main content with surrounding context\n\n    Examples:\n        &gt;&gt;&gt; # Basic gathering with surrounding context\n        &gt;&gt;&gt; df.semantic.gather(\n        ...     content_key=\"chunk_content\",\n        ...     doc_id_key=\"document_id\",\n        ...     order_key=\"chunk_number\",\n        ...     peripheral_chunks={\n        ...         \"previous\": {\"head\": {\"count\": 2}, \"tail\": {\"count\": 1}},\n        ...         \"next\": {\"head\": {\"count\": 1}, \"tail\": {\"count\": 2}}\n        ...     }\n        ... )\n\n        &gt;&gt;&gt; # Simple gathering without peripheral chunks\n        &gt;&gt;&gt; df.semantic.gather(\n        ...     content_key=\"content\",\n        ...     doc_id_key=\"doc_id\",\n        ...     order_key=\"order\"\n        ... )\n    \"\"\"\n    # Convert DataFrame to list of dicts\n    input_data = self._df.to_dict(\"records\")\n\n    # Create gather operation config\n    gather_config = {\n        \"type\": \"gather\",\n        \"name\": f\"semantic_gather_{len(self._history)}\",\n        \"content_key\": content_key,\n        \"doc_id_key\": doc_id_key,\n        \"order_key\": order_key,\n        **kwargs,\n    }\n\n    # Add peripheral_chunks config if provided\n    if peripheral_chunks is not None:\n        gather_config[\"peripheral_chunks\"] = peripheral_chunks\n\n    # Create and execute gather operation\n    gather_op = GatherOperation(\n        runner=self.runner,\n        config=gather_config,\n        default_model=self.runner.config[\"default_model\"],\n        max_threads=self.runner.max_threads,\n        console=self.runner.console,\n        status=self.runner.status,\n    )\n    results, cost = gather_op.execute(input_data)\n\n    return self._record_operation(results, \"gather\", gather_config, cost)\n</code></pre> <p>Example usage: <pre><code># Basic gathering with surrounding context\ndf.semantic.gather(\n    content_key=\"chunk_content\",\n    doc_id_key=\"document_id\",\n    order_key=\"chunk_number\",\n    peripheral_chunks={\n        \"previous\": {\"head\": {\"count\": 2}, \"tail\": {\"count\": 1}},\n        \"next\": {\"head\": {\"count\": 1}, \"tail\": {\"count\": 2}}\n    }\n)\n\n# Simple gathering without peripheral chunks\ndf.semantic.gather(\n    content_key=\"content\",\n    doc_id_key=\"doc_id\",\n    order_key=\"order\"\n)\n</code></pre></p>"},{"location":"pandas/operations/#unnest-operation","title":"Unnest Operation","text":"<p>Unnest list-like or dictionary values into multiple rows.</p> <p>Documentation: https://ucbepic.github.io/docetl/operators/unnest/</p> <p>Parameters:</p> Name Type Description Default <code>unnest_key</code> <code>str</code> <p>The column containing list-like or dictionary values to unnest</p> required <code>keep_empty</code> <code>bool</code> <p>Whether to keep rows with empty/null values (default: False)</p> <code>False</code> <code>expand_fields</code> <code>list[str] | None</code> <p>For dictionary values, which fields to expand (default: all)</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Whether to recursively unnest nested structures (default: False)</p> <code>False</code> <code>depth</code> <code>int | None</code> <p>Maximum depth for recursive unnesting (default: 1, or unlimited if recursive=True)</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration options</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with unnested values, where: - For lists: Each list element becomes a separate row - For dicts: Specified fields are expanded into the parent row</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Unnest a list column\n&gt;&gt;&gt; df.semantic.unnest(\n...     unnest_key=\"tags\"\n... )\n# Input:  [{\"id\": 1, \"tags\": [\"a\", \"b\"]}]\n# Output: [{\"id\": 1, \"tags\": \"a\"}, {\"id\": 1, \"tags\": \"b\"}]\n</code></pre> <pre><code>&gt;&gt;&gt; # Unnest a dictionary column with specific fields\n&gt;&gt;&gt; df.semantic.unnest(\n...     unnest_key=\"user_info\",\n...     expand_fields=[\"name\", \"age\"]\n... )\n# Input:  [{\"id\": 1, \"user_info\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}}]\n# Output: [{\"id\": 1, \"user_info\": {...}, \"name\": \"Alice\", \"age\": 30}]\n</code></pre> <pre><code>&gt;&gt;&gt; # Recursive unnesting\n&gt;&gt;&gt; df.semantic.unnest(\n...     unnest_key=\"nested_lists\",\n...     recursive=True,\n...     depth=2\n... )\n</code></pre> Source code in <code>docetl/apis/pd_accessors.py</code> <pre><code>def unnest(\n    self,\n    unnest_key: str,\n    keep_empty: bool = False,\n    expand_fields: list[str] | None = None,\n    recursive: bool = False,\n    depth: int | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Unnest list-like or dictionary values into multiple rows.\n\n    Documentation: https://ucbepic.github.io/docetl/operators/unnest/\n\n    Args:\n        unnest_key: The column containing list-like or dictionary values to unnest\n        keep_empty: Whether to keep rows with empty/null values (default: False)\n        expand_fields: For dictionary values, which fields to expand (default: all)\n        recursive: Whether to recursively unnest nested structures (default: False)\n        depth: Maximum depth for recursive unnesting (default: 1, or unlimited if recursive=True)\n        **kwargs: Additional configuration options\n\n    Returns:\n        pd.DataFrame: DataFrame with unnested values, where:\n            - For lists: Each list element becomes a separate row\n            - For dicts: Specified fields are expanded into the parent row\n\n    Examples:\n        &gt;&gt;&gt; # Unnest a list column\n        &gt;&gt;&gt; df.semantic.unnest(\n        ...     unnest_key=\"tags\"\n        ... )\n        # Input:  [{\"id\": 1, \"tags\": [\"a\", \"b\"]}]\n        # Output: [{\"id\": 1, \"tags\": \"a\"}, {\"id\": 1, \"tags\": \"b\"}]\n\n        &gt;&gt;&gt; # Unnest a dictionary column with specific fields\n        &gt;&gt;&gt; df.semantic.unnest(\n        ...     unnest_key=\"user_info\",\n        ...     expand_fields=[\"name\", \"age\"]\n        ... )\n        # Input:  [{\"id\": 1, \"user_info\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}}]\n        # Output: [{\"id\": 1, \"user_info\": {...}, \"name\": \"Alice\", \"age\": 30}]\n\n        &gt;&gt;&gt; # Recursive unnesting\n        &gt;&gt;&gt; df.semantic.unnest(\n        ...     unnest_key=\"nested_lists\",\n        ...     recursive=True,\n        ...     depth=2\n        ... )\n    \"\"\"\n    # Convert DataFrame to list of dicts\n    input_data = self._df.to_dict(\"records\")\n\n    # Create unnest operation config\n    unnest_config = {\n        \"type\": \"unnest\",\n        \"name\": f\"semantic_unnest_{len(self._history)}\",\n        \"unnest_key\": unnest_key,\n        \"keep_empty\": keep_empty,\n        \"recursive\": recursive,\n        **kwargs,\n    }\n\n    # Add optional parameters if provided\n    if expand_fields is not None:\n        unnest_config[\"expand_fields\"] = expand_fields\n    if depth is not None:\n        unnest_config[\"depth\"] = depth\n\n    # Create and execute unnest operation\n    unnest_op = UnnestOperation(\n        runner=self.runner,\n        config=unnest_config,\n        default_model=self.runner.config[\"default_model\"],\n        max_threads=self.runner.max_threads,\n        console=self.runner.console,\n        status=self.runner.status,\n    )\n    results, cost = unnest_op.execute(input_data)\n\n    return self._record_operation(results, \"unnest\", unnest_config, cost)\n</code></pre> <p>Example usage: <pre><code># Unnest a list column\ndf.semantic.unnest(unnest_key=\"tags\")\n\n# Unnest a dictionary column with specific fields\ndf.semantic.unnest(\n    unnest_key=\"user_info\",\n    expand_fields=[\"name\", \"age\"]\n)\n\n# Recursive unnesting with depth control\ndf.semantic.unnest(\n    unnest_key=\"nested_lists\",\n    recursive=True,\n    depth=2\n)\n</code></pre></p>"},{"location":"pandas/operations/#common-features","title":"Common Features","text":"<p>All operations support:</p> <ol> <li> <p>Cost Tracking <pre><code># After any operation\nprint(f\"Operation cost: ${df.semantic.total_cost}\")\n</code></pre></p> </li> <li> <p>Operation History <pre><code># View operation history\nfor op in df.semantic.history:\n    print(f\"{op.op_type}: {op.output_columns}\")\n</code></pre></p> </li> <li> <p>Validation Rules <pre><code># Add validation rules to any  map or filter operation\nvalidate=[\"len(output['tags']) &lt;= 5\", \"output['score'] &gt;= 0\"]\n</code></pre></p> </li> </ol> <p>For more details on configuration options and best practices, refer to: - DocETL Best Practices - Pipeline Configuration - Output Schemas </p>"},{"location":"playground/","title":"Playground","text":"<p>The DocETL Playground is an integrated development environment (IDE) for building and testing document processing pipelines. Built with Next.js and TypeScript, it provides a real-time interface to develop, test and refine your pipelines through a FastAPI backend.</p>"},{"location":"playground/#why-a-playground","title":"Why a Playground? \ud83e\udd14","text":"<p>This interactive playground streamlines development from prototype to production! Our (in-progress) user studies show 100% of developers found building pipelines significantly faster and easier with our playground vs traditional approaches.</p> <p>Building complex LLM pipelines for your data often requires experimentation and iteration. The IDE lets you:</p> <ul> <li>\ud83d\ude80 Test prompts and see results instantly</li> <li>\u2728 Refine operations based on sample outputs  </li> <li>\ud83d\udd04 Build complex pipelines step-by-step</li> </ul>"},{"location":"playground/#public-playground","title":"Public Playground","text":"<p>You can access our hosted playground at docetl.org/playground. You'll need to provide your own LLM API keys to use the service. The chatbot and prompt engineering assistants are powered by OpenAI models, so you'll need to provide an OpenAI API key.</p> <p>Data Storage Notice</p> <p>As this is a research project, we cache results and store data on our servers to improve the system. While we will never sell or release your data, if you have privacy concerns, we recommend running the playground locally using the installation instructions below.</p>"},{"location":"playground/#installation","title":"Installation","text":"<p>There are two ways to run the playground:</p>"},{"location":"playground/#1-using-docker-recommended-for-quick-start","title":"1. Using Docker (Recommended for Quick Start)","text":"<p>The easiest way to get started is using Docker:</p>"},{"location":"playground/#a-create-the-required-environment-files","title":"a) Create the required environment files:","text":"<p>Create <code>.env</code> in the root directory (for the FastAPI backend): <pre><code># Required: API key for your preferred LLM provider (OpenAI, Anthropic, etc)\n# The key format will depend on your chosen provider (sk-..., anthro-...)\nOPENAI_API_KEY=your_api_key_here \nBACKEND_ALLOW_ORIGINS=http://localhost:3000,http://127.0.0.1:3000\nBACKEND_HOST=localhost\nBACKEND_PORT=8000\nBACKEND_RELOAD=True\nFRONTEND_HOST=localhost\nFRONTEND_PORT=3000\n</code></pre></p> <p>Create <code>.env.local</code> in the <code>website</code> directory (for the frontend) note that this must be in the <code>website</code> directory: <pre><code># Optional: These are only needed if you want to use the AI assistant chatbot \n# and prompt engineering tools. Must be OpenAI API keys specifically.\nOPENAI_API_KEY=sk-xxx\nOPENAI_API_BASE=https://api.openai.com/v1\nMODEL_NAME=gpt-4o-mini\n\nNEXT_PUBLIC_BACKEND_HOST=localhost\nNEXT_PUBLIC_BACKEND_PORT=8000\n</code></pre></p>"},{"location":"playground/#b-run-docker","title":"b) Run Docker:","text":"<pre><code>make docker\n</code></pre> <p>This will:</p> <ul> <li>Create a Docker volume for persistent data</li> <li>Build the DocETL image</li> <li>Run the container with the UI accessible at http://localhost:3000 and API at http://localhost:8000</li> </ul> <p>To clean up Docker resources (note that this will delete the Docker volume):</p> <pre><code>make docker-clean\n</code></pre>"},{"location":"playground/#2-running-locally-development-setup","title":"2. Running Locally (Development Setup)","text":"<p>For development or if you want to run the UI locally:</p> <ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/ucbepic/docetl.git\ncd docetl\n</code></pre></p> </li> <li> <p>Set up environment variables in <code>.env</code> in the root directory: <pre><code>LLM_API_KEY=your_api_key_here\nBACKEND_ALLOW_ORIGINS=http://localhost:3000,http://127.0.0.1:3000\nBACKEND_HOST=localhost\nBACKEND_PORT=8000\nBACKEND_RELOAD=True\nFRONTEND_HOST=0.0.0.0\nFRONTEND_PORT=3000\n</code></pre></p> </li> </ol> <p>Create <code>.env.local</code> in the <code>website</code> directory: <pre><code>OPENAI_API_KEY=sk-xxx\nOPENAI_API_BASE=https://api.openai.com/v1\nMODEL_NAME=gpt-4o-mini\n\nNEXT_PUBLIC_BACKEND_HOST=localhost\nNEXT_PUBLIC_BACKEND_PORT=8000\n</code></pre></p> <p>Note</p> <p>Note that the OpenAI API key, base, and model name in the <code>.env.local</code> file are only for the UI assistant functionality, not the DocETL pipeline execution engine.</p> <ol> <li> <p>Install dependencies: <pre><code>make install      # Install Python package\nmake install-ui   # Install UI dependencies\n</code></pre></p> </li> <li> <p>Start the development server: <pre><code>make run-ui-dev\n</code></pre></p> </li> <li> <p>Navigate to http://localhost:3000/playground to access the playground.</p> </li> </ol>"},{"location":"playground/#setting-up-the-ai-assistant","title":"Setting up the AI Assistant","text":"<p>The UI offers an optional chat-based assistant that can help you iteratively develop your pipeline. It is currently very experimental. It can't write to your pipeline, but you can bounce ideas off of it and get it to help you iteratively develop your pipeline.</p> <p>To use the assistant, you need to set your OpenAI API key in the <code>.env.local</code> file in the website directory. You can get an API key here. The API key should be in the following format: <code>sk-proj-...</code>. We only support the openai models for the assistant.</p> <p>Self-hosting with UI API key management</p> <p>If you want to host your own version of DocETL for your organization while allowing users to set their API keys through the UI, you'll need to set up encryption. Add the following to both <code>.env</code> and <code>website/.env.local</code>: <pre><code>DOCETL_ENCRYPTION_KEY=your_secret_key_here\n</code></pre> This shared encryption key allows API keys to be securely encrypted when sent to your server. Make sure to use the same value in both files.</p>"},{"location":"playground/#complex-tutorial","title":"Complex Tutorial","text":"<p>See this YouTube video for a more in depth tutorial on how to use the playground.</p>"},{"location":"playground/features/","title":"Features","text":"<p>The DocETL playground provides an interactive environment for building and testing document processing pipelines. Here are the key features:</p>"},{"location":"playground/features/#current-features","title":"Current Features","text":""},{"location":"playground/features/#hybrid-interface","title":"Hybrid Interface","text":"<p>The playground offers a unique hybrid between a notebook and spreadsheet interface, allowing you to: - Iteratively develop and test pipeline operations - Inspect operation outputs in a tabular format - Seamlessly switch between code and data views</p>"},{"location":"playground/features/#performance-optimizations","title":"Performance Optimizations","text":"<p>To ensure responsive interaction: - Smart sampling of large datasets for quick iteration - Automatic caching of operation results - Efficient handling of LLM API calls</p>"},{"location":"playground/features/#output-management","title":"Output Management","text":"<ul> <li>Add notes and highlights to important outputs</li> <li>Save and organize findings during pipeline development</li> <li>Track key insights and results</li> </ul>"},{"location":"playground/features/#export-capabilities","title":"Export Capabilities","text":"<ul> <li>Export results from any operation to CSV</li> <li>Preserve intermediate results for further analysis</li> <li>Share outputs with team members</li> </ul>"},{"location":"playground/features/#upcoming-features","title":"Upcoming Features","text":"<p>We're actively working on several exciting ideas:</p>"},{"location":"playground/features/#natural-language-pipeline-assistant","title":"Natural Language Pipeline Assistant","text":"<ul> <li>Generate and indirectly modify pipelines using natural language</li> <li>Interactive help for pipeline development</li> </ul>"},{"location":"playground/features/#enhanced-validation-ui","title":"Enhanced Validation UI","text":"<ul> <li>Per-document retry capabilities for failed operations</li> <li>UI support for gleaning validation outside of extra kwargs</li> <li>Visual feedback for validation results</li> </ul>"},{"location":"playground/features/#pipeline-optimization-interface","title":"Pipeline Optimization Interface","text":"<ul> <li>Interactive tools for optimizing operation performance</li> <li>Visual pipeline analysis and bottleneck identification</li> <li>Suggestions for pipeline efficiency improvements</li> </ul> <p>Join the Development</p> <p>Interested in these upcoming features? Join our Discord community to provide feedback and help shape the development of these features!</p>"},{"location":"playground/tutorial/","title":"Simple Tutorial: Extracting Funny Quotes from Presidential Debates","text":"<p>In this tutorial, we'll walk through using the DocETL playground to extract funny or memorable quotes from presidential debate transcripts. We'll see how to:</p> <ol> <li>Upload and explore data</li> <li>Run a basic pipeline with sampling</li> <li>Examine outputs</li> <li>Iterate on prompts to improve results</li> </ol> <p>Using LLMs to Help Write Pipelines</p> <p>Want to use an LLM like ChatGPT or Claude to help you write your pipeline? See docetl.org/llms.txt for a big prompt you can copy paste into ChatGPT or Claude, before describing your task.</p>"},{"location":"playground/tutorial/#step-1-upload-the-data","title":"Step 1: Upload the Data","text":"<p>First, download the presidential debates dataset from here.</p> <p>Once downloaded, use the left sidebar's upload button to load the data into the playground. The data contains transcripts from various presidential debates.</p> <p>You can view the data by clicking the \"toggle dataset view\" button in the top right corner of the screen:</p> <p></p>"},{"location":"playground/tutorial/#step-2-add-a-map-operation","title":"Step 2: Add a Map Operation","text":"<p>The pipeline is set to run on a sample of 5 documents, as indicated by the sample icon next to the pipeline name. This sampling helps us quickly test and iterate on our prompts without processing the entire dataset. This can be changed.</p> <p>We'll add a map operation that processes each debate transcript to extract logical fallacies. Click the \"Add Operation\" button in the top right corner of the screen, and select \"Map\" under the LLM section. You can set the operation name to \"extract_fallacies\", and write a prompt and output schema. </p> <p></p>"},{"location":"playground/tutorial/#step-3-run-the-pipeline-and-check-outputs","title":"Step 3: Run the Pipeline and Check Outputs","text":"<p>Click the \"Run\" button to execute the pipeline. The outputs panel will show two important tabs:</p> <ul> <li>Console: Displays progress information and any potential errors</li> <li>Table: Shows the extracted funny quotes from each document in a table, as well as the other key/value pairs in the document. Here's what the table looks like after running the pipeline:</li> </ul> <p></p> <p>You can resize the rows and columns in the table by clicking and dragging the edges of the table cells, as in the image above. You can also rezise the outputs panel by clicking and dragging the top edge of the panel.</p>"},{"location":"playground/tutorial/#step-4-iterate-on-the-prompt","title":"Step 4: Iterate on the Prompt","text":"<p>After reviewing the initial outputs, let's inspect them row by row to identify areas for improvement. Click the expand icon next to any column to see the full content.</p> <p>We can modify the prompt to request additional context based on what we observe in the data. Here are some general patterns that could use improvement:</p> <ul> <li>Some quotes lack sufficient background context</li> <li>Speaker information could be more detailed</li> <li>The reasoning behind why quotes are memorable isn't always clear</li> <li>Time and setting details are often missing</li> <li>Impact and reactions to quotes aren't consistently captured</li> </ul> <p>Here's an example of what giving notes might look like:</p> <p></p> <p>Once you've added enough notes (3-5 or more), you can click on the \"Improve Prompt\" button in the top right corner of the operation card. This will invoke the DocWrangler Prompt Improvement Assistant, which will suggest edits to your prompt:</p> <p></p> <p>Once you're satisfied with the new prompt, click \"Save\" to update the operation, and then you can rerun the pipeline to see the new outputs.</p> <p></p> <p>Caching Behavior</p> <p>DocETL automatically caches the outputs of each operation. This means that if you run the pipeline multiple times without changing the operations, it will use the cached results instead of reprocessing the documents. This is especially helpful when:</p> <ul> <li>Iterating on downstream operations in a multi-step pipeline</li> <li>Running the pipeline on the full dataset after testing on samples</li> <li>Sharing results with teammates (cached outputs persist across sessions)</li> </ul> <p>The cache is invalidated only when you modify an operation's configuration (e.g., changing the prompt or schema). This ensures you always see fresh results when making changes while avoiding unnecessary recomputation.</p> <p>If you want to bypass the cache and force recomputation, you can click the \"Clear and Run\" button instead of the regular \"Run\" button.</p>"},{"location":"playground/tutorial/#step-5-run-the-pipeline-on-the-entire-dataset","title":"Step 5: Run the Pipeline on the Entire Dataset","text":"<p>Once you're satisfied with your prompt, you can run the pipeline on the entire dataset. First, clear the sample size by clicking on the settings or gear icon next to the pipeline name. </p> <p>Then, click the \"Run\" button again. This will process all documents and update the outputs panel with the results. </p> <p>You can export the results to a CSV file by clicking the \"download\" button in the top right corner of the outputs panel, near where it says \"Selectivity: 1.00x\". The selectivity is the ratio of the number of documents in the output to the number of documents in the input for that operation. In this case, since we ran the pipeline on the full dataset, the selectivity is 1.0x.</p> <p>Model Note</p> <p>In this tutorial, we used <code>azure/gpt-4o</code> instead of <code>gpt-4o-mini</code> since content filters were triggered by <code>gpt-4o-mini</code> when processing political debate content. If you encounter similar content filter issues with <code>gpt-4o-mini</code>, consider using <code>azure/gpt-4o</code> or another model with less restrictive filters.</p>"},{"location":"python/","title":"Python API","text":"<p>The DocETL Python API provides a programmatic way to define, optimize, and run document processing pipelines. This approach offers an alternative to the YAML configuration method, allowing for more dynamic and flexible pipeline construction.</p>"},{"location":"python/#overview","title":"Overview","text":"<p>The Python API consists of several classes:</p> <ul> <li>Dataset: Represents a dataset with a type and path.</li> <li>Various operation classes (e.g., MapOp, ReduceOp, FilterOp) for different types of data processing steps.</li> <li>PipelineStep: Represents a step in the pipeline with input and operations.</li> <li>Pipeline: The main class for defining and running a complete document processing pipeline.</li> <li>PipelineOutput: Defines the output configuration for the pipeline.</li> </ul>"},{"location":"python/#example-usage","title":"Example Usage","text":"<p>Here's an example of how to use the Python API to create and run a simple document processing pipeline:</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define datasets\ndatasets = {\n    \"my_dataset\": Dataset(type=\"file\", path=\"input.json\", parsing=[{\"input_key\": \"file_path\", \"function\": \"txt_to_string\", \"output_key\": \"content\"}]),\n}\n\n# Note that the parsing is applied to the `file_path` key in each item of the dataset,\n# and the result is stored in the `content` key.\n\n# Define operations\noperations = [\n    MapOp(\n        name=\"process\",\n        type=\"map\",\n        prompt=\"Determine what type of document this is: {{ input.content }}\",\n        output={\"schema\": {\"document_type\": \"string\"}}\n    ),\n    ReduceOp(\n        name=\"summarize\",\n        type=\"reduce\",\n        reduce_key=\"document_type\",\n        prompt=\"Summarize the processed contents: {% for item in inputs %}{{ item.content }} {% endfor %}\",\n        output={\"schema\": {\"summary\": \"string\"}}\n    )\n]\n\n# Define pipeline steps\nsteps = [\n    PipelineStep(name=\"process_step\", input=\"my_dataset\", operations=[\"process\"]),\n    PipelineStep(name=\"summarize_step\", input=\"process_step\", operations=[\"summarize\"])\n]\n\n# Define pipeline output\noutput = PipelineOutput(type=\"file\", path=\"output.json\")\n\n# Create the pipeline\npipeline = Pipeline(\n    name=\"example_pipeline\",\n    datasets=datasets,\n    operations=operations,\n    steps=steps,\n    output=output,\n    default_model=\"gpt-4o-mini\"\n)\n\n# Optimize the pipeline\noptimized_pipeline = pipeline.optimize()\n\n# Run the optimized pipeline\nresult = optimized_pipeline.run() # Saves the result to the output path\n\nprint(f\"Pipeline execution completed. Total cost: ${result:.2f}\")\n</code></pre> <p>This example demonstrates how to create a simple pipeline that processes input documents and then summarizes the processed content. The pipeline is optimized before execution to improve performance.</p>"},{"location":"python/#api-reference","title":"API Reference","text":"<p>For a complete reference of all available classes and their methods, please refer to the Python API Reference.</p> <p>The API Reference provides detailed information about each class, including:</p> <ul> <li>Available parameters</li> <li>Method signatures</li> <li>Return types</li> <li>Usage examples</li> </ul>"},{"location":"python/examples/","title":"Python API Examples","text":"<p>This document provides two examples of how to use DocETL's Python API. Each example demonstrates a single pipeline step with multiple operations.</p>"},{"location":"python/examples/#example-1-extract-and-summarize-product-review-themes","title":"Example 1: Extract and Summarize Product Review Themes","text":"<p>This example extracts themes and quotes from product reviews, then summarizes the top themes across ALL documents using the special <code>_all</code> reduce key:</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define dataset - A CSV of product reviews\ndataset = Dataset(\n    type=\"file\",\n    path=\"product_reviews.csv\"  # Contains columns: review_id, product_id, rating, review_text\n)\n\n# Define operations\noperations = [\n    # Extract themes and quotes from each review\n    MapOp(\n        name=\"extract_themes\",\n        type=\"map\",\n        prompt=\"\"\"\n        Analyze this product review and extract the key themes and representative quotes:\n\n        Review: {{ input.review_text }}\n        Rating: {{ input.rating }}\n\n        Identify 2-3 major themes (e.g., usability, quality, value) and extract direct quotes that best represent each theme.\n        \"\"\",\n        output={\n            \"schema\": {\n                \"themes\": \"list[string]\",\n                \"quotes\": \"list[string]\",\n                \"sentiment\": \"string\"\n            }\n        }\n    ),\n    # Summarize all themes across reviews using _all key\n    ReduceOp(\n        name=\"summarize_themes\",\n        type=\"reduce\",\n        reduce_key=\"_all\",  # Special key to reduce across all items\n        prompt=\"\"\"\n        Analyze and synthesize the themes and quotes from these product reviews:\n\n        {% for item in inputs %}\n        Review ID: {{ item.review_id }}\n        Product: {{ item.product_id }}\n        Rating: {{ item.rating }}\n        Themes: {{ item.themes | join(\", \") }}\n        Quotes: \n        {% for quote in item.quotes %}\n        - \"{{ quote }}\"\n        {% endfor %}\n        Sentiment: {{ item.sentiment }}\n\n        {% endfor %}\n\n        Summarize the most frequent themes, the most representative quotes for each theme, and the overall sentiment.\n        \"\"\",\n        output={\n            \"schema\": {\n                \"summary\": \"string\"\n            }\n        }\n    )\n]\n\n# Define pipeline step (can consist of multiple operations)\nstep = PipelineStep(\n    name=\"review_analysis\",\n    input=\"product_reviews\",\n    operations=[\"extract_themes\", \"summarize_themes\"]\n)\n\n# Define output\noutput = PipelineOutput(type=\"file\", path=\"review_analysis_summary.json\")\n\n# Create and run pipeline\npipeline = Pipeline(\n    name=\"review_analysis_pipeline\",\n    datasets={\"product_reviews\": dataset},\n    operations=operations,\n    steps=[step],\n    output=output,\n    default_model=\"gpt-4o-mini\"\n)\n\n# Run the pipeline\ncost = pipeline.run()\nprint(f\"Pipeline execution completed. Total cost: ${cost:.2f}\")\n</code></pre>"},{"location":"python/examples/#example-2-map-unnest-resolve-reduce-on-theme-keys","title":"Example 2: Map-Unnest-Resolve-Reduce on Theme Keys","text":"<p>This example extracts theme-quote pairs from reviews, unnests them, resolves similar themes, and then reduces on the theme key. Here we will also optimize the pipeline.</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, UnnestOp, ResolveOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define dataset - A JSON file with product reviews\ndataset = Dataset(\n    type=\"file\",\n    path=\"product_reviews.csv\" # Same csv as previously\n)\n\n# Define operations\noperations = [\n    # Extract theme-quote pairs from each review\n    MapOp(\n        name=\"extract_theme_quotes\",\n        type=\"map\",\n        prompt=\"\"\"\n        Extract theme and quote pairs from this product review:\n\n        Review: {{ input.review_text }}\n        Product: {{ input.product_name }}\n        Rating: {{ input.rating }}\n\n        For each distinct theme in the review, extract a direct quote that best represents that theme.\n        Return each theme and its representative quote as a separate object in the \"theme_quotes\" array.\n        \"\"\",\n        output={\n            \"schema\": {\n                \"theme_quotes\": \"array\"  # Array of objects with theme and quote properties\n            }\n        }\n    ),\n    # Unnest to create separate items for each theme-quote pair\n    UnnestOp(\n        name=\"unnest_theme_quotes\",\n        type=\"unnest\",\n        array_path=\"theme_quotes\"\n    ),\n    # Resolve similar themes using fuzzy matching\n    ResolveOp(\n        name=\"resolve_themes\",\n        type=\"resolve\",\n        comparison_prompt=\"\"\"\n        Determine if these two themes are the same or closely related:\n\n        Theme 1: {{ input1.theme }}\n        Theme 2: {{ input2.theme }}\n\n        Consider semantic similarity, synonyms, and conceptual overlap.\n        \"\"\",\n        resolution_prompt=\"\"\"\n        Given the following list of similar themes, determine a canonical name that best represents all of them:\n\n        {% for item in inputs %}\n        Theme: {{ item.theme }}\n        {% endfor %}\n\n        Choose a clear, concise name that accurately captures the core concept shared across all these related themes.\n        \"\"\"\n    ),\n    # Reduce by theme to aggregate quotes and insights\n    ReduceOp(\n        name=\"aggregate_by_theme\",\n        type=\"reduce\",\n        reduce_key=\"theme\",\n        prompt=\"\"\"\n        Analyze all quotes related to the theme \"{{ reduce_key }}\":\n\n        {% for item in inputs %}\n        Product: {{ item.product_name }}\n        Rating: {{ item.rating }}\n        Quote: \"{{ item.quote }}\"\n\n        {% endfor %}\n\n        Summarize the key insights about this theme across all products and ratings.\n        \"\"\",\n        output={\n            \"schema\": {\n                \"summary\": \"string\"\n            }\n        }\n    )\n]\n\n# Define pipeline with a single step\nstep = PipelineStep(\n    name=\"theme_analysis\",\n    input=\"product_reviews\",\n    operations=[\"extract_theme_quotes\", \"unnest_theme_quotes\", \"resolve_themes\", \"aggregate_by_theme\"]\n)\n\n# Define output\noutput = PipelineOutput(type=\"file\", path=\"theme_analysis_results.json\")\n\n# Create the pipeline\npipeline = Pipeline(\n    name=\"theme_analysis_pipeline\",\n    datasets={\"product_reviews\": dataset},\n    operations=operations,\n    steps=[step],\n    output=output,\n    default_model=\"gpt-4o\"\n)\n\n# Optimize the pipeline before running\noptimized_pipeline = pipeline.optimize()\n\n# Run the optimized pipeline\ncost = optimized_pipeline.run()\nprint(f\"Pipeline execution completed. Total cost: ${cost:.2f}\")\n</code></pre> <p>Note that datasets can be json or CSV.</p>"}]}